{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "423e7341",
   "metadata": {},
   "source": [
    "# Configure and instantiate SydsGPT (345M-style)\n",
    "\n",
    " \n",
    "\n",
    "This cell prepares a ready-to-use SydsGPT model with a GPT-2–345M–like configuration.\n",
    "\n",
    "It imports PyTorch and the model class, defines a configuration dictionary, sets a random seed for reproducibility, instantiates the model, and switches it to evaluation mode.\n",
    "\n",
    " \n",
    "\n",
    "## What the code does\n",
    "\n",
    "- `import torch`: Loads PyTorch for tensors, RNG control, and device utilities.\n",
    "\n",
    "- `from model.SydsGPT import SydsGPT`: Imports the model class from the local `model` package.\n",
    "\n",
    "- `SYDSGPT_CONFIG_345M = {...}`: Defines key hyperparameters:\n",
    "\n",
    "  - `vocab_size` (int): Size of the tokenizer vocabulary the model predicts over.\n",
    "\n",
    "  - `context_length` (int): Maximum sequence length (number of time steps) the model attends to.\n",
    "\n",
    "  - `embedding_dim` (int): Hidden size / channel dimension of token and position embeddings and transformer layers.\n",
    "\n",
    "  - `num_heads` (int): Number of attention heads per transformer block. Should divide `embedding_dim` evenly.\n",
    "\n",
    "  - `num_layers` (int): Number of stacked transformer blocks (depth).\n",
    "\n",
    "  - `dropout` (float): Dropout probability used in training; disabled automatically in eval mode.\n",
    "\n",
    "  - `qkv_bias` (bool): Whether to include bias terms in Q/K/V projection layers.\n",
    "\n",
    "- `torch.manual_seed(246)`: Sets the RNG seed for reproducibility (affects random init, sampling, etc.).\n",
    "\n",
    "- `model = SydsGPT(SYDSGPT_CONFIG_345M)`: Builds the model using the provided config.\n",
    "\n",
    "- `model.eval()`: Puts the model in inference mode (disables dropout; batch-norm-like training behavior is not used in this architecture).\n",
    "\n",
    " \n",
    "\n",
    "## Expected inputs and outputs\n",
    "\n",
    "- Input: integer token IDs shaped `(batch_size, seq_len)` with `0 <= token_id < vocab_size`.\n",
    "\n",
    "- Output: logits shaped `(batch_size, seq_len, vocab_size)` ready for softmax or sampling.\n",
    "\n",
    "## Device and performance tips\n",
    "\n",
    "- To use GPU if available:\n",
    "\n",
    "  ```python\n",
    "\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  model = model.to(device)\n",
    "\n",
    "  x = x.to(device)\n",
    "\n",
    "  ```\n",
    "\n",
    "- For memory-sensitive scenarios, consider bfloat16/float16 inference (on supported hardware):\n",
    "\n",
    "  ```python\n",
    "\n",
    "  model = model.to(device).to(dtype=torch.bfloat16)\n",
    "\n",
    "  x = x.to(device)\n",
    "\n",
    "  with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "\n",
    "      logits = model(x)\n",
    "\n",
    "  ```\n",
    "\n",
    " \n",
    "## Common pitfalls\n",
    "\n",
    "- `ModuleNotFoundError`: Ensure you run the notebook from the repo root (where this notebook lives) and that `model/` and `modules/` contain `__init__.py`.\n",
    "\n",
    "- `num_heads` must evenly divide `embedding_dim`.\n",
    "\n",
    "- Using `model.train()` during inference will enable dropout and change outputs; keep `model.eval()` for deterministic behavior (given a fixed seed and inputs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98746fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SydsGPT(\n",
       "  (token_embedding): Embedding(50257, 1024)\n",
       "  (position_embedding): Embedding(512, 1024)\n",
       "  (drop_embedding): Dropout(p=0.1, inplace=False)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): LayerNorm()\n",
       "  (output_projection): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from model.SydsGPT import SydsGPT\n",
    "\n",
    "SYDSGPT_CONFIG_345M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_length\" : 512,\n",
    "    \"embedding_dim\" : 1024,\n",
    "    \"num_heads\" : 16,\n",
    "    \"num_layers\" : 24,\n",
    "    \"dropout\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}\n",
    "\n",
    "torch.manual_seed(246)\n",
    "model = SydsGPT(SYDSGPT_CONFIG_345M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ccab65",
   "metadata": {},
   "source": [
    "# Simple text generation with tiktoken\n",
    "\n",
    " \n",
    "\n",
    "This cell demonstrates a minimal text-generation loop using the model and GPT‑2 BPE tokenization.\n",
    "\n",
    "It converts input text to token IDs, generates a few new tokens with `generate_simple`, then decodes the tokens back to text.\n",
    "\n",
    " \n",
    "\n",
    "## What the code does\n",
    "\n",
    "- `from modules.GenerateSimple import generate_simple`: Imports a helper that appends new tokens to the context.\n",
    "\n",
    "- `import tiktoken`: Loads the GPT‑2 tokenizer implementation used for encoding/decoding.\n",
    "\n",
    "- `text_to_tokens(text, tokenizer)`: Encodes a string into token IDs and wraps them into a batch of size 1 with shape `(1, seq_len)`.\n",
    "\n",
    "- `tokens_to_text(tokens, tokenizer)`: Decodes a tensor of token IDs back to a string.\n",
    "\n",
    "- `tokenizer = tiktoken.get_encoding(\"gpt2\")`: Uses the GPT‑2 BPE vocabulary (vocab size 50257).\n",
    "\n",
    "- `input_text = \"Once upon a time\"`: Seed prompt for generation.\n",
    "\n",
    "- `input_tokens = text_to_tokens(...)`: Converts the prompt to `(1, seq_len)` tensor.\n",
    "\n",
    "- `output_tokens = generate_simple(model, input_tokens, 10, context_length)`: Generates 10 new tokens (total length increases by up to 10, subject to context length).\n",
    "\n",
    "- `print(...)`: Shows the decoded text after generation.\n",
    "\n",
    " \n",
    "\n",
    "## Inputs and outputs\n",
    "\n",
    "- Input text: a Python string prompt.\n",
    "\n",
    "- Encoded input: `input_tokens` with dtype `torch.long` and shape `(1, T)` where `0 <= token_id < vocab_size`.\n",
    "\n",
    "- Output tokens: a tensor typically shaped `(1, T + N)` where `N` is `max_new_tokens` (here 10), but may be shorter if the function stops early (e.g., on end‑of‑text).\n",
    "\n",
    "- Decoded output: a Python string built from `output_tokens`.\n",
    "\n",
    " \n",
    "## Tokenization notes\n",
    "\n",
    "- This example allows the special token `<|endoftext|>` during encoding:\n",
    "\n",
    "  ```python\n",
    "\n",
    "  tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "  ```\n",
    "\n",
    "- Decoding uses `tokenizer.decode(...)` to reconstruct readable text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec824c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.GenerateSimple import generate_simple\n",
    "import tiktoken\n",
    "\n",
    "def text_to_tokens(text, tokenizer):\n",
    "    tokens = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
    "    return tokens\n",
    "\n",
    "def tokens_to_text(tokens, tokenizer):\n",
    "    text = tokenizer.decode(tokens.squeeze(0).tolist())\n",
    "    return text\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bff3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Text: Once upon a time,\n",
      "                                                                                                  \n"
     ]
    }
   ],
   "source": [
    "input_text = \"Once upon a time\"\n",
    "input_tokens = text_to_tokens(input_text, tokenizer)\n",
    "output_tokens = generate_simple(model, input_tokens, 100, SYDSGPT_CONFIG_345M['context_length'])\n",
    "output_text = tokens_to_text(output_tokens, tokenizer)\n",
    "\n",
    "print(f\"Output Text: {output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c519394e",
   "metadata": {},
   "source": [
    "# Batch inference and greedy token selection (baseline)\n",
    "\n",
    " \n",
    "\n",
    "This cell runs a batched forward pass on two prompts, converts logits to probabilities, and selects the most likely token at each position via greedy argmax. It then decodes these per‑position predictions to text for a quick qualitative check.\n",
    "\n",
    " \n",
    "\n",
    "## What the code does\n",
    "\n",
    "- Defines two short input prompts and two short \"target\" snippets for comparison.\n",
    "\n",
    "- Encodes strings to token IDs with `text_to_tokens` and batches them using `torch.cat` to form tensors of shape `(batch_size=2, seq_len)`.\n",
    "\n",
    "- Computes model logits with `model(batch_input_tokens)` → shape `(B, T, V)` where `V = vocab_size`.\n",
    "\n",
    "- Converts logits to probabilities with `softmax` and picks the most likely token id at each position using `argmax` over the vocab dimension.\n",
    "\n",
    "- Decodes the resulting token IDs back to text with `tokens_to_text` for a quick sanity check.\n",
    "\n",
    " \n",
    "\n",
    "## Shapes and dtypes\n",
    "\n",
    "- `batch_input_tokens`: `(B, T)` of dtype `torch.long`.\n",
    "\n",
    "- `logits`: `(B, T, V)` of dtype `torch.float32` (or your model’s dtype).\n",
    "\n",
    "- `probs`: `(B, T, V)` after softmax.\n",
    "\n",
    "- `generated_tokens`: `(B, T, 1)` after `argmax(..., keepdim=True)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07569318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Input Tokens Shape: torch.Size([2, 4])\n",
      "Batch Input Tokens: tensor([[  464,  2068,  7586, 21831],\n",
      "        [  818,   257, 16161,  1290]])\n",
      "Batch Target Tokens Shape: torch.Size([2, 4])\n",
      "Batch Target Tokens: tensor([[ 2068,  7586, 21831, 18045],\n",
      "        [  257, 16161,  1290,  1497]])\n",
      "Generated Tokens: \n",
      "tensor([[[34912],\n",
      "         [11918],\n",
      "         [50106],\n",
      "         [22915]],\n",
      "\n",
      "        [[47903],\n",
      "         [31311],\n",
      "         [27997],\n",
      "         [ 8618]]])\n",
      "Target text for example 1:  quick brown fox jumps\n",
      "Generated text for example 1:  Trace chaoscatchingoutput\n",
      "Target text for example 2:  a galaxy far away\n",
      "Generated text for example 2: ּ Debbie fats gained\n"
     ]
    }
   ],
   "source": [
    "example_input_text_1 = \"The quick brown fox\"\n",
    "example_target_text_1 = \" quick brown fox jumps\"\n",
    "\n",
    "example_input_text_2 = \"In a galaxy far\"\n",
    "example_target_text_2 = \" a galaxy far away\"\n",
    "\n",
    "input_tokens_1 = text_to_tokens(example_input_text_1, tokenizer)\n",
    "input_tokens_2 = text_to_tokens(example_input_text_2, tokenizer)\n",
    "\n",
    "target_tokens_1 = text_to_tokens(example_target_text_1, tokenizer)\n",
    "target_tokens_2 = text_to_tokens(example_target_text_2, tokenizer)\n",
    "\n",
    "batch_input_tokens = torch.cat([input_tokens_1, input_tokens_2], dim=0)\n",
    "batch_target_tokens = torch.cat([target_tokens_1, target_tokens_2], dim=0)\n",
    "\n",
    "print(f\"Batch Input Tokens Shape: {batch_input_tokens.shape}\")\n",
    "print(f\"Batch Input Tokens: {batch_input_tokens}\")\n",
    "print(f\"Batch Target Tokens Shape: {batch_target_tokens.shape}\")\n",
    "print(f\"Batch Target Tokens: {batch_target_tokens}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(batch_input_tokens)\n",
    "probs = torch.softmax(logits, dim = -1)\n",
    "\n",
    "generated_tokens = torch.argmax(probs, dim = -1, keepdim = True)\n",
    "print(f\"Generated Tokens: \\n{generated_tokens}\")\n",
    "\n",
    "print(f\"Target text for example 1: {example_target_text_1}\")\n",
    "print(f\"Generated text for example 1: {tokens_to_text(generated_tokens[0].flatten(), tokenizer)}\")\n",
    "print(f\"Target text for example 2: {example_target_text_2}\")\n",
    "print(f\"Generated text for example 2: {tokens_to_text(generated_tokens[1].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de6d540",
   "metadata": {},
   "source": [
    "# Selecting target probabilities and estimating a simple loss\n",
    "\n",
    " \n",
    "\n",
    "This cell extracts probabilities assigned to the target tokens, converts them to log‑probs, and computes an averaged negative log‑probability (a proxy for loss). It demonstrates indexing into a `(batch, time, vocab)` probability tensor and aggregates results across two examples.\n",
    "\n",
    " \n",
    "\n",
    "## Tensors and shapes\n",
    "\n",
    "- `probs`: shape `(B, T, V)` — probabilities over the vocabulary at every time step for each example.\n",
    "\n",
    "- `batch_target_tokens`: shape `(B, T)` — integer token IDs for the target sequence per example.\n",
    "\n",
    "- `batch_index`: scalar `int` selecting which example (0 or 1 here).\n",
    "\n",
    " \n",
    "\n",
    "## What each line does\n",
    "\n",
    "1. `batch_index = 0` (and later `1`): chooses which example in the batch to analyze.\n",
    "\n",
    "2. `target_probs_1 = probs[batch_index, [0,1,2,3], batch_target_tokens[batch_index]]`:\n",
    "\n",
    "   - Narrows to the chosen example → `probs[batch_index]` has shape `(T, V)`.\n",
    "\n",
    "   - Uses advanced indexing with two indexers:\n",
    "\n",
    "     - Time indices `[0,1,2,3]` select 4 positions from the time dimension.\n",
    "\n",
    "     - `batch_target_tokens[batch_index]` is a length‑`T` vector of token IDs and indexes the vocab dimension.\n",
    "\n",
    "   - Because these two indexers have different shapes `(4,)` and `(T,)`, PyTorch broadcasts them, producing a matrix of shape `(4, T)`. Each row corresponds to a chosen time step; each column corresponds to a target token at some position in the sequence. This yields a grid of probabilities, not a 1‑to‑1 per‑step alignment.\n",
    "\n",
    "3. Repeat for `batch_index = 1` to get `target_probs_2` with shape `(4, T)` as well.\n",
    "\n",
    "4. `log_probs = torch.log(torch.cat((target_probs_1, target_probs_2)))`: concatenates along the first dimension → shape `(8, T)` and applies natural log to get log‑probs.\n",
    "\n",
    "5. `mean_log_probs = torch.mean(log_probs)`: averages all selected log‑probs.\n",
    "\n",
    "6. `negative_mean_log_probs = mean_log_probs * -1`: converts to a positive quantity akin to an average negative log‑probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96d78d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target probabilities for example 1: tensor([3.4469e-05, 1.3686e-05, 6.9494e-06, 1.3889e-05])\n",
      "Target probabilities for example 2: tensor([4.0864e-05, 1.1410e-05, 9.1020e-06, 8.7344e-06])\n",
      "Log probabilities: tensor([-10.2754, -11.1992, -11.8768, -11.1844, -10.1053, -11.3810, -11.6070,\n",
      "        -11.6482])\n",
      "Mean log probability: -11.15966510772705\n",
      "Negative mean log probability (loss): 11.15966510772705\n"
     ]
    }
   ],
   "source": [
    "batch_index = 0\n",
    "target_probs_1 = probs[batch_index, [0,1,2,3], batch_target_tokens[batch_index]]\n",
    "print(f\"Target probabilities for example 1: {target_probs_1}\")\n",
    "\n",
    "batch_index = 1\n",
    "target_probs_2 = probs[batch_index, [0,1,2,3], batch_target_tokens[batch_index]]\n",
    "print(f\"Target probabilities for example 2: {target_probs_2}\")\n",
    "\n",
    "log_probs = torch.log(torch.cat((target_probs_1, target_probs_2)))\n",
    "print(f\"Log probabilities: {log_probs}\")\n",
    "\n",
    "mean_log_probs = torch.mean(log_probs)\n",
    "print(f\"Mean log probability: {mean_log_probs}\")\n",
    "\n",
    "negative_mean_log_probs = mean_log_probs * -1\n",
    "print(f\"Negative mean log probability (loss): {negative_mean_log_probs}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af2ff63",
   "metadata": {},
   "source": [
    "# Inspecting logits/targets and computing cross‑entropy loss\n",
    "\n",
    " \n",
    "\n",
    "This cell prints tensor shapes/values, flattens logits and targets to a 2‑D/1‑D form, and computes a categorical cross‑entropy loss. It’s a quick way to sanity‑check model outputs against targets and to illustrate how classification loss is applied to sequence models.\n",
    "\n",
    " \n",
    "\n",
    "## What the code does\n",
    "\n",
    "- Prints `logits.shape` and `batch_target_tokens.shape` to verify expected dimensions:\n",
    "\n",
    "  - `logits`: `(B, T, V)` — unnormalized scores over the vocabulary for each batch/time position.\n",
    "\n",
    "  - `batch_target_tokens`: `(B, T)` — integer token IDs (class labels) for each batch/time position.\n",
    "\n",
    "- Flattens tensors for loss computation:\n",
    "\n",
    "  - `flat_logits = logits.flatten(0, 1)` → shape `((B*T), V)`.\n",
    "\n",
    "  - `flat_targets = batch_target_tokens.flatten()` → shape `((B*T),)`.\n",
    "\n",
    "- Computes cross‑entropy using `torch.nn.functional.cross_entropy` on raw logits and integer class targets.\n",
    "\n",
    " \n",
    "\n",
    "## Why flatten?\n",
    "\n",
    "Most loss functions in PyTorch expect 2‑D logits `(N, C)` and 1‑D targets `(N,)`. Flattening the batch and time dimensions treats every time step in the batch as an independent classification example, which is equivalent to computing the mean loss over all `(b, t)` positions.\n",
    "\n",
    " \n",
    "## Dtype, device, and numerical stability\n",
    "\n",
    "- `logits` should be floating point (e.g., `float32`), and `targets` must be integer type (`torch.long`).\n",
    "\n",
    "- Ensure `logits` and `targets` reside on the same device (CPU/GPU) to avoid runtime errors.\n",
    "\n",
    "- `cross_entropy` is numerically stable; you rarely need to clamp or add epsilons yourself.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "- We get the same Cross Entropy loss as our manual calculation in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32cd3ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 4, 50257])\n",
      "Logits: tensor([[[-0.0807,  0.1419, -0.0128,  ...,  0.0488, -1.1006, -0.5177],\n",
      "         [ 0.2431,  0.1199,  0.4347,  ..., -1.4129, -0.4291, -0.3951],\n",
      "         [-0.4904, -0.1851, -0.0027,  ..., -1.0833,  1.1487, -0.5754],\n",
      "         [-0.0147,  0.2563,  1.2010,  ..., -0.4941, -0.5542, -1.3598]],\n",
      "\n",
      "        [[ 0.0052,  0.1310,  0.0080,  ..., -0.1653, -0.8215, -0.9171],\n",
      "         [-0.2092, -0.1521,  0.5149,  ..., -0.5578,  0.0754, -1.4415],\n",
      "         [-0.2539, -0.2681,  0.4995,  ..., -0.6771,  0.0557, -0.8401],\n",
      "         [-0.3334, -0.0690,  1.2449,  ...,  0.1689, -0.3848, -0.5397]]])\n",
      "Targets shape: torch.Size([2, 4])\n",
      "Targets: tensor([[ 2068,  7586, 21831, 18045],\n",
      "        [  257, 16161,  1290,  1497]])\n",
      "Flattened Logits shape: torch.Size([8, 50257])\n",
      "Flattened Targets shape: torch.Size([8])\n",
      "Cross-entropy loss: 11.15966510772705\n"
     ]
    }
   ],
   "source": [
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Logits: {logits}\")\n",
    "\n",
    "print(f\"Targets shape: {batch_target_tokens.shape}\")\n",
    "print(f\"Targets: {batch_target_tokens}\")\n",
    "\n",
    "flat_logits = logits.flatten(0, 1)\n",
    "flat_targets = batch_target_tokens.flatten()\n",
    "\n",
    "print(f\"Flattened Logits shape: {flat_logits.shape}\")\n",
    "print(f\"Flattened Targets shape: {flat_targets.shape}\")\n",
    "\n",
    "loss_fn = torch.nn.functional.cross_entropy\n",
    "loss = loss_fn(flat_logits, flat_targets)\n",
    "print(f\"Cross-entropy loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97fe673",
   "metadata": {},
   "source": [
    "# Load raw corpus and estimate token count\n",
    "\n",
    " \n",
    "\n",
    "This cell reads a text corpus from disk and reports:\n",
    "\n",
    "- Total number of characters in the file (useful for sanity checks and throughput planning).\n",
    "\n",
    "- Total number of tokens after GPT‑2 BPE encoding using `tiktoken` (this approximates the training token budget).\n",
    "\n",
    " \n",
    "\n",
    "## What the code does\n",
    "\n",
    "- Opens `data/all_books.txt` with UTF‑8 encoding and loads it into memory as a single string.\n",
    "\n",
    "- Uses the previously created `tokenizer = tiktoken.get_encoding(\"gpt2\")` to encode the entire string to token IDs.\n",
    "\n",
    "- Prints character and token counts.\n",
    "\n",
    " \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- The file `data/all_books.txt` should exist relative to this notebook’s working directory (the repository root).\n",
    "\n",
    "- A `tokenizer` must be available in the notebook scope (earlier cells set `tokenizer = tiktoken.get_encoding(\"gpt2\")`).\n",
    "\n",
    " \n",
    "\n",
    "## Notes on GPT‑2 BPE tokenization\n",
    "\n",
    "- GPT‑2 uses a byte‑level BPE with a vocabulary size of 50,257; token count ≠ word count.\n",
    "\n",
    "- Token counts depend on punctuation, whitespace, and casing; the same characters can yield different tokenizations if the text changes slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50477f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters: 19849702\n",
      "Total Tokens after encoding: 5611150\n",
      "Total Tokens after encoding: 5611150\n"
     ]
    }
   ],
   "source": [
    "data_file_path = \"data/all_books.txt\"\n",
    "with open(data_file_path, 'r', encoding = 'utf-8') as books:\n",
    "    text_data = books.read()\n",
    "\n",
    "print(f\"Total Characters: {len(text_data)}\")\n",
    "print(f\"Total Tokens after encoding: {len(tokenizer.encode(text_data))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a90c512",
   "metadata": {},
   "source": [
    "# Create training/validation DataLoaders from raw text\n",
    "\n",
    " \n",
    "\n",
    "This cell splits the raw corpus into train/validation subsets and builds batched `(input, target)` sequences for next‑token prediction using a fixed context window.\n",
    "\n",
    " \n",
    "\n",
    "## What the code does\n",
    "\n",
    "- Splits `text_data` with `training_ratio = 0.9`:\n",
    "\n",
    "  - `training_dataset` = first 90% of characters\n",
    "\n",
    "  - `validation_dataset` = remaining 10%\n",
    "\n",
    "- Uses `modules.DataLoader.create_dataloader(...)` to produce iterable loaders that yield `(x, y)` batches.\n",
    "\n",
    "- Reports the number of batches and prints the shape of one batch from each loader for a quick sanity check.\n",
    "\n",
    " \n",
    "\n",
    "## Key parameters\n",
    "\n",
    "- `max_length = SYDSGPT_CONFIG_345M['context_length']`:\n",
    "\n",
    "  - Sequence length per sample (context window size).\n",
    "\n",
    "- `step_size = SYDSGPT_CONFIG_345M['context_length'] // 2`:\n",
    "\n",
    "  - Half‑overlapping windows (stride is half the window). This increases dataset size vs non‑overlapping windows and improves sample diversity at the cost of more compute.\n",
    "\n",
    "- `batch_size = 64`:\n",
    "\n",
    "  - Number of sequences per batch. Tune based on GPU/CPU memory; larger batches increase throughput but require more memory.\n",
    "\n",
    "- `shuffle = True`:\n",
    "\n",
    "  - Randomizes sample order to improve training stability.\n",
    "\n",
    "- `drop_last = True`:\n",
    "\n",
    "  - Drops incomplete final batch to keep shapes consistent.\n",
    "\n",
    "- `num_workers = 0`:\n",
    "\n",
    "  - Data loading in the main process. Increase (e.g., 2–8) to parallelize preprocessing if your implementation supports it.\n",
    "\n",
    " \n",
    "\n",
    "## Expected batch shapes and semantics\n",
    "\n",
    "- `x`: LongTensor `(batch_size, max_length)` — input token IDs.\n",
    "\n",
    "- `y`: LongTensor `(batch_size, max_length)` — target token IDs.\n",
    "\n",
    "- Language modeling convention: logits at time `t` are trained to predict `y` at time `t+1` (a one‑token shift). Your `create_dataloader` should either prepare `y` accordingly or you can apply the shift when computing loss.\n",
    "\n",
    " \n",
    "\n",
    "## Assumptions (API contract)\n",
    "\n",
    "- `create_dataloader(text, max_length, step_size, ...)` tokenizes `text` (using the same tokenizer as earlier), slices into windows of length `max_length` with stride `step_size`, and returns batches of `(x, y)` suitable for a next‑token objective.\n",
    "\n",
    "- If your implementation expects pre‑tokenized data or returns different shapes, adjust parameters or downstream code accordingly.\n",
    "\n",
    " \n",
    "\n",
    "## Tips and troubleshooting\n",
    "\n",
    "- Throughput vs redundancy:\n",
    "\n",
    "  - Smaller `step_size` → more overlapping windows → more training samples but higher redundancy. Start with `max_length//2` and tune.\n",
    "\n",
    "- Memory pressure:\n",
    "\n",
    "  - Reduce `batch_size` if you hit OOM; alternatively, keep `batch_size` and reduce `max_length`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb84b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 1249\n",
      "Training loader:\n",
      "torch.Size([8, 512]) torch.Size([8, 512])\n",
      "Number of validation batches: 120\n",
      "Validation loader:\n",
      "torch.Size([8, 512]) torch.Size([8, 512])\n"
     ]
    }
   ],
   "source": [
    "training_ratio = 0.9\n",
    "training_size = int(training_ratio * len(text_data))\n",
    "training_dataset = text_data[:training_size]\n",
    "validation_dataset = text_data[training_size:]\n",
    "\n",
    "from modules.DataLoader import create_dataloader\n",
    "\n",
    "training_dataloader = create_dataloader(\n",
    "    training_dataset,\n",
    "    max_length = SYDSGPT_CONFIG_345M['context_length'],\n",
    "    step_size = SYDSGPT_CONFIG_345M['context_length'],\n",
    "    batch_size = 8,\n",
    "    shuffle = True,\n",
    "    drop_last = True,\n",
    "    num_workers = 0,\n",
    ")\n",
    "\n",
    "validation_dataloader = create_dataloader(\n",
    "    validation_dataset,\n",
    "    max_length = SYDSGPT_CONFIG_345M['context_length'],\n",
    "    step_size = SYDSGPT_CONFIG_345M['context_length'],\n",
    "    batch_size = 8,\n",
    "    shuffle = True,\n",
    "    drop_last = True,\n",
    "    num_workers = 0,\n",
    ")\n",
    "\n",
    "print(f\"Number of training batches: {len(training_dataloader)}\")\n",
    "\n",
    "print(\"Training loader:\")\n",
    "for x, y in training_dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "    break\n",
    "\n",
    "print(f\"Number of validation batches: {len(validation_dataloader)}\")\n",
    "\n",
    "print(\"Validation loader:\")\n",
    "for x, y in validation_dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18bff9e",
   "metadata": {},
   "source": [
    "# Mini utility: per-batch loss computation\n",
    "\n",
    " \n",
    "\n",
    "This helper computes the cross‑entropy loss for a single batch. It moves tensors to the desired device, runs the model forward pass, flattens logits/targets into the expected shapes, and returns a scalar loss suitable for `backward()`.\n",
    "\n",
    " \n",
    "\n",
    "## Function signature\n",
    "\n",
    "- `calc_batch_loss(input_batch, target_batch, model, device) -> torch.Tensor`\n",
    "\n",
    "  - Returns a scalar 0‑D tensor (the mean cross‑entropy over all tokens in the batch).\n",
    "\n",
    " \n",
    "\n",
    "## Inputs and shapes\n",
    "\n",
    "- `input_batch`: LongTensor `(B, T)` — token IDs fed into the model.\n",
    "\n",
    "- `target_batch`: LongTensor `(B, T)` — token IDs used as labels.\n",
    "\n",
    "- `model`: a `nn.Module` with `forward(input_batch) -> logits` of shape `(B, T, V)`.\n",
    "\n",
    "- `device`: target device (e.g., `torch.device(\"cuda\")` or `\"cpu\"`).\n",
    "\n",
    " \n",
    "\n",
    "## What happens inside\n",
    "\n",
    "1. Moves `input_batch` and `target_batch` to `device` for consistency.\n",
    "\n",
    "2. Computes `logits = model(input_batch)` with shape `(B, T, V)`.\n",
    "\n",
    "3. Flattens for loss: `logits.flatten(0, 1)` → `((B*T), V)` and `target_batch.flatten()` → `((B*T),)`.\n",
    "\n",
    "4. Applies `torch.nn.functional.cross_entropy` to raw logits and integer targets (no softmax needed).\n",
    "\n",
    "5. Returns the mean loss across all `(B*T)` token positions as a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d77f4337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_batch_loss(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0,1), target_batch.flatten())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8c1b89",
   "metadata": {},
   "source": [
    "# Compute average loss over a DataLoader (validation helper)\n",
    "\n",
    " \n",
    "\n",
    "This utility iterates over a DataLoader, computes the per‑batch cross‑entropy via `calc_batch_loss`, and returns the mean of batch losses. It’s intended for quick validation/evaluation, not training.\n",
    "\n",
    " \n",
    "\n",
    "## Function signature\n",
    "\n",
    "- `calc_loader_loss(data_loader, model, device, num_batches=None) -> float`\n",
    "\n",
    "  - Returns a Python float: the average of `num_batches` batch losses.\n",
    "\n",
    " \n",
    "\n",
    "## Parameters\n",
    "\n",
    "- `data_loader`: Iterable yielding `(input_batch, target_batch)` tensors, typically shaped `(B, T)` each.\n",
    "\n",
    "- `model`: The language model (`nn.Module`). For evaluation, call `model.eval()` beforehand.\n",
    "\n",
    "- `device`: `torch.device` or string identifying the device (e.g., `\"cuda\"`, `\"cpu\"`).\n",
    "\n",
    "- `num_batches` (optional): If provided, caps the number of batches processed (useful for fast estimates). Defaults to the full length of the loader when available.\n",
    "\n",
    " \n",
    "\n",
    "## What the code does\n",
    "\n",
    "1. Initializes `total_loss = 0`.\n",
    "\n",
    "2. Handles an empty loader by returning `nan` early.\n",
    "\n",
    "3. Determines how many batches to process:\n",
    "\n",
    "   - If `num_batches is None`, uses `len(data_loader)`.\n",
    "\n",
    "   - Else takes `min(num_batches, len(data_loader))`.\n",
    "\n",
    "4. Iterates over the loader and calls `calc_batch_loss(...)` for each batch until the cap is reached.\n",
    "\n",
    "5. Sums `batch_loss.item()` and returns `total_loss / num_batches`.\n",
    "\n",
    " \n",
    "\n",
    "## Averaging semantics and weighting\n",
    "\n",
    "- `calc_batch_loss` uses `cross_entropy(..., reduction='mean')` on flattened `(B*T)` elements, so each batch loss is the mean per token within that batch.\n",
    "\n",
    "- This helper averages those batch means equally across batches. If all batches are the same size (common when `drop_last=True`), this equals the dataset mean.\n",
    " \n",
    "\n",
    "## Performance tips\n",
    "\n",
    "- Wrap calls in `torch.no_grad()` or `torch.inference_mode()` to disable autograd and save memory/time during validation.\n",
    "\n",
    "- Increase `num_workers` in your dataloader (if supported) to accelerate data prep.\n",
    "\n",
    "- Ensure batches and model are on the same device to avoid implicit transfers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dba22f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loader_loss(data_loader, model, device, num_batches = None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float('nan')\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for batch_index, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if batch_index >= num_batches:\n",
    "            break\n",
    "        else:\n",
    "            batch_loss = calc_batch_loss(input_batch, target_batch, model, device)\n",
    "            total_loss += batch_loss.item()\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6f1b05",
   "metadata": {},
   "source": [
    "# Evaluate initial training/validation loss (no grad)\n",
    "\n",
    "This cell runs a quick, read-only evaluation to get baseline losses on the training and validation loaders. It’s useful for sanity checking your pipeline before training (e.g., confirming shapes/devices, dataloader yield, and a reasonable initial loss).\n",
    "\n",
    "## What the code does\n",
    "- Picks a device: `cuda` if available, else `cpu`.\n",
    "- Prints the chosen device for visibility.\n",
    "- Moves the model to that device: `model.to(device)`.\n",
    "- Disables autograd with `torch.no_grad()` and computes:\n",
    "  - `training_loss = calc_loader_loss(training_dataloader, model, device)`\n",
    "  - `validation_loss = calc_loader_loss(validation_dataloader, model, device)`\n",
    "- Prints both losses.\n",
    "\n",
    "## Interpreting the numbers\n",
    "- The loss is the mean cross‑entropy over all tokens seen by the helper.\n",
    "- With random initialization and a vocab of 50,257, a loss near ln(50257) ≈ 10.82 is expected. Lower is better; after training, you should see this decrease.\n",
    "\n",
    "Example options:\n",
    "- Fast estimate: `calc_loader_loss(training_dataloader, model, device, num_batches=50)`\n",
    "- Exact: `calc_loader_loss(training_dataloader, model, device, num_batches=num_train_batches)` where `num_train_batches = count_batches_streaming(...)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "042c89ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initial Training Loss: 10.957578182220459\n",
      "Initial Validation Loss: 10.96420328957694\n",
      "Initial Training Loss: 10.957578182220459\n",
      "Initial Validation Loss: 10.96420328957694\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    training_loss = calc_loader_loss(training_dataloader, model, device)\n",
    "    validation_loss = calc_loader_loss(validation_dataloader, model, device)\n",
    "\n",
    "print(f\"Initial Training Loss: {training_loss}\")\n",
    "print(f\"Initial Validation Loss: {validation_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829bcb2b",
   "metadata": {},
   "source": [
    "# Helper: `generate_sample_text` (quick qualitative checkpoint)\n",
    "\n",
    "This helper function produces a short sample completion from the current model state after (or during) training. It lets you visually inspect language quality and drift without exporting or writing a separate script.\n",
    "\n",
    "---\n",
    "## Function Signature\n",
    "```python\n",
    "def generate_sample_text(model, tokenizer, device, start_context):\n",
    "    ...\n",
    "```\n",
    "\n",
    "### Parameters\n",
    "- **model** (`nn.Module`): Your SydsGPT instance (already moved to `device`). Should be in training or eval mode depending on outer loop.\n",
    "- **tokenizer** (`tiktoken.Encoding`): GPT‑2 BPE tokenizer used for encoding the prompt and decoding output tokens.\n",
    "- **device** (`torch.device`): Target compute device (e.g., `cuda` or `cpu`). Ensures input tokens match model placement.\n",
    "- **start_context** (`str`): The initial prompt (seed text) to condition generation. Can be a sentence fragment, instruction, or domain phrase.\n",
    "\n",
    "### Returns\n",
    "- No explicit return; prints a one‑line generated string (newlines collapsed to spaces). You could refactor to return the string if needed.\n",
    "\n",
    "---\n",
    "## Step‑by‑Step Workflow\n",
    "1. Switches the model to `eval()` to:\n",
    "   - Disable dropout.\n",
    "   - Avoid gradient tracking (paired with an explicit `torch.no_grad()` block).\n",
    "2. Determines `context_size` from the model config (`SYDSGPT_CONFIG_345M['context_length']`). This sets the max total token length (prompt + generated) the function will attempt.\n",
    "3. Encodes `start_context` into token IDs and wraps into a batch of size 1; moves tensor to `device`.\n",
    "4. Calls `generate_simple(model, input_tokens, 100, context_size)`:\n",
    "   - Greedy extension up to `max_new_tokens=100` or until context limit reached.\n",
    "   - `generate_simple` internally ensures input tokens are on the same device as model (device‑safety fix applied earlier).\n",
    "5. Decodes resulting tokens back to text via the tokenizer.\n",
    "6. Prints the generated text with newlines replaced by spaces for cleaner logging.\n",
    "7. Restores `model.train()` so the outer training loop can continue collecting gradients.\n",
    "\n",
    "---\n",
    "## Usage Patterns\n",
    "Typical placement:\n",
    "- End of each epoch (qualitative progress checkpoint).\n",
    "- After major learning rate changes.\n",
    "- Before and after loading a checkpoint to verify restoration.\n",
    "\n",
    "Example invocation inside a loop:\n",
    "```python\n",
    "generate_sample_text(model, tokenizer, device, start_context=\"Once upon a time\")\n",
    "```\n",
    "\n",
    "---\n",
    "## Design Considerations\n",
    "- **Non‑Determinism**: If you include dropout or sample stochastically (e.g., top‑k, temperature), results change across calls. Here, greedy decoding + `eval()` yields deterministic output given fixed weights.\n",
    "- **Prompt Length vs. Context**: If `start_context` is already near `context_length`, generation will be short or zero. Consider truncating long prompts.\n",
    "- **Performance**: 100 tokens is modest; for faster feedback on large models reduce to 32 or 64.\n",
    "- **Mode Switching**: Ensures the model returns to training mode automatically—prevents accidental dropout disablement mid‑training.\n",
    "\n",
    "## Summary\n",
    "`generate_sample_text` is a lightweight, deterministic snapshot tool: it briefly transitions the model to evaluation, performs greedy decoding for a fixed number of tokens, prints the result, and returns the model to training mode—giving you rapid human feedback on training progress without disrupting optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50401be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_text(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = SYDSGPT_CONFIG_345M['context_length']\n",
    "    input_tokens = text_to_tokens(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = generate_simple(model, input_tokens, 100, context_size)\n",
    "    generated_text = tokens_to_text(generated_tokens, tokenizer)\n",
    "    print(f\"Generated Text: {generated_text}\".replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777cbd22",
   "metadata": {},
   "source": [
    "# Training Loop: `train_model_v1` (core optimization engine)\n",
    "\n",
    "This section documents the training function responsible for iterating over data, computing gradients, updating model weights, periodically evaluating progress, checkpointing state, and logging token throughput.\n",
    "\n",
    "---\n",
    "## Function Signature\n",
    "```python\n",
    "def train_model_v1(model,\n",
    "                   training_dataloader,\n",
    "                   validation_dataloader,\n",
    "                   optimizer,\n",
    "                   device,\n",
    "                   num_epochs,\n",
    "                   evaluation_frequency,\n",
    "                   evaluation_iterations,\n",
    "                   start_context,\n",
    "                   tokenizer,\n",
    "                   checkpoint_interval=500):\n",
    "    ...\n",
    "```\n",
    "\n",
    "### Arguments\n",
    "| Name | Type | Description |\n",
    "|------|------|-------------|\n",
    "| `model` | `nn.Module` | SydsGPT model to optimize. Must already reside on `device`. |\n",
    "| `training_dataloader` | iterable of `(input_batch, target_batch)` | Yields training batches (LongTensors of shape `(B, T)`). |\n",
    "| `validation_dataloader` | iterable of `(input_batch, target_batch)` | Used for periodic evaluation (subset or full depending on `evaluation_iterations`). |\n",
    "| `optimizer` | `torch.optim.Optimizer` | Optimizer instance (e.g., AdamW) over model parameters. |\n",
    "| `device` | `torch.device` / `str` | Compute target (GPU preferred if available). |\n",
    "| `num_epochs` | `int` | Number of full passes over `training_dataloader`. |\n",
    "| `evaluation_frequency` | `int` | Evaluate every N optimization steps (NOT epochs). |\n",
    "| `evaluation_iterations` | `int` | Number of batches to sample from each loader during evaluation (fast approximation). |\n",
    "| `start_context` | `str` | Prompt for qualitative sample via `generate_sample_text` after each epoch. |\n",
    "| `tokenizer` | tokenizer object | Used only for the sampling helper (not directly in training loop math). |\n",
    "| `checkpoint_interval` | `int` (default 500) | Save a checkpoint every N steps (model + optimizer state). |\n",
    "\n",
    "### Returns\n",
    "A tuple `(training_losses, validation_losses, total_tokens_processed)`:\n",
    "- `training_losses`: List of sampled training loss values (one per evaluation event).\n",
    "- `validation_losses`: List of sampled validation loss values aligned with `training_losses` indices.\n",
    "- `total_tokens_processed`: Cumulative token counts (one entry per training step logged) enabling plotting loss vs. tokens.\n",
    "\n",
    "---\n",
    "## Internal State Variables\n",
    "- `tokens_processed`: Running count of all token positions used in gradient steps across steps and epochs. Incremented by `input_batch.numel()` (which = `batch_size * sequence_length`).\n",
    "- `global_step`: Counts optimization steps across epochs (starts at `-1`, incremented at each batch). Used to trigger evaluations and checkpoints. Starting at -1 ensures after first increment the first batch is step 0 (aligns with modulo logic for early evaluation/checkpoint if desired).\n",
    "\n",
    "---\n",
    "## Step-by-Step Flow\n",
    "1. **Epoch Loop**: Repeats `num_epochs` times.\n",
    "2. **Set `model.train()`**: Ensures dropout (if any) and gradient accumulation are active.\n",
    "3. **Batch Loop (Training)**:\n",
    "   - Zero gradients: `optimizer.zero_grad()` (baseline approach; could switch to gradient accumulation strategy if needed).\n",
    "   - Forward + Loss: Uses `calc_batch_loss` which (a) moves tensors to device, (b) computes logits, (c) applies cross-entropy across flattened `(B*T)` tokens.\n",
    "   - Backprop: `loss.backward()` computes gradients.\n",
    "   - Optimizer Step: `optimizer.step()` applies parameter updates.\n",
    "   - Update token counter: `tokens_processed += input_batch.numel()`.\n",
    "   - Increment global step.\n",
    "   - Append the new cumulative token count to `total_tokens_processed`.\n",
    "   - Progress print: provides epoch, step, and cumulative tokens.\n",
    "4. **Conditional Evaluation** (`if global_step % evaluation_frequency == 0`):\n",
    "   - Switch to `model.eval()`.\n",
    "   - Wrap in `torch.no_grad()` to disable gradient tracking.\n",
    "   - Compute approximate training loss: `calc_loader_loss(training_dataloader, ..., num_batches=evaluation_iterations)`.\n",
    "   - Compute approximate validation loss similarly.\n",
    "   - Append both losses to respective lists and print a concise summary.\n",
    "   - Return to `model.train()`.\n",
    "5. **Conditional Checkpoint** (`if global_step % checkpoint_interval == 0`):\n",
    "   - Save a dictionary with both model and optimizer state dicts to `autosave_sydsgpt_345m_trained_model_optimizer.pth`.\n",
    "6. **End of Epoch Qualitative Sample**: Calls `generate_sample_text` using `start_context` to qualitatively track improvements.\n",
    "7. **Loop Continue or Exit**: Repeats until all epochs complete; returns logged metrics.\n",
    "\n",
    "---\n",
    "## Evaluation Strategy Rationale\n",
    "- Using a small `evaluation_iterations` drastically reduces overhead, enabling frequent snapshots (e.g., every 100 steps) without stalling training.\n",
    "- For a precise validation curve later, run a dedicated full evaluation pass using the batch counting helpers from the streaming dataloader (if integrated) or iterate fully with `drop_last=False`.\n",
    "\n",
    "---\n",
    "## Checkpointing Notes\n",
    "- Each autosave overwrites the same filename, minimizing disk usage.\n",
    "- For resilience, consider timestamped or step‑indexed filenames, e.g.: `autosave_step_{global_step}.pth`.\n",
    "- Include `tokens_processed` and `global_step` in future checkpoint metadata to support exact resumption.\n",
    "\n",
    "---\n",
    "## Logging & Monitoring\n",
    "- Current approach: simple `print` statements.\n",
    "- Optional enhancements:\n",
    "  - Use `tqdm` progress bars (wrap dataloader) for ETA visibility.\n",
    "  - Log scalars (loss, tokens) to TensorBoard or a tracking service.\n",
    "  - Store (step, training_loss, validation_loss, tokens_processed) as rows in a CSV for later analysis.\n",
    "\n",
    "---\n",
    "## Performance Tips\n",
    "- Prefer larger batch sizes if GPU memory permits—improves arithmetic intensity.\n",
    "- If dataloader becomes bottleneck: increase `num_workers`, enable `pin_memory=True`, pre‑tokenize (if not already streaming efficiently).\n",
    "- Track `tokens_processed` vs. wall clock time to estimate throughput (tokens/sec) for capacity planning.\n",
    "\n",
    "---\n",
    "## Usage Example\n",
    "```python\n",
    "training_losses, validation_losses, token_counts = train_model_v1(\n",
    "    model, training_dataloader, validation_dataloader, optimizer, device,\n",
    "    num_epochs=5, evaluation_frequency=100, evaluation_iterations=2,\n",
    "    start_context=\"Once upon a time\", tokenizer=tokenizer, checkpoint_interval=500)\n",
    "```\n",
    "\n",
    "---\n",
    "## Summary\n",
    "`train_model_v1` provides a clear, modular training scaffold: it tracks tokens, performs scheduled lightweight evaluations, produces qualitative samples, and saves recoverable checkpoints. This makes it a solid foundation for iterative experimentation and scaling with minimal friction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c69a73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_v1(model, training_dataloader, validation_dataloader, optimizer, device, num_epochs, evaluation_frequency, evaluation_iterations, start_context, tokenizer, checkpoint_interval = 500):\n",
    "    training_losses, validation_losses, total_tokens_processed = [], [], []\n",
    "    tokens_processed = 0\n",
    "    global_step = -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in training_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_batch_loss(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_processed += input_batch.numel()\n",
    "            global_step += 1\n",
    "            total_tokens_processed.append(tokens_processed)\n",
    "            print(f\"Epoch {epoch+1}, Step {global_step}: Tokens Processed = {tokens_processed}\")\n",
    "\n",
    "            if global_step % evaluation_frequency == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    training_loss = calc_loader_loss(training_dataloader, model, device, evaluation_iterations)\n",
    "                    validation_loss = calc_loader_loss(validation_dataloader, model, device, evaluation_iterations)\n",
    "                    training_losses.append(training_loss)\n",
    "                    validation_losses.append(validation_loss)                    \n",
    "                    print(f\"Epoch {epoch+1}, Step {global_step}: Training Loss = {training_loss}, Validation Loss = {validation_loss}, Tokens Processed = {tokens_processed}\")\n",
    "                model.train()\n",
    "\n",
    "            if global_step % checkpoint_interval == 0:\n",
    "                torch.save({\"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict()}, \"autosave_sydsgpt_345m_trained_model_optimizer.pth\")\n",
    "\n",
    "        \n",
    "        generate_sample_text(model, tokenizer, device, start_context)\n",
    "\n",
    "    return training_losses, validation_losses, total_tokens_processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea50f3c",
   "metadata": {},
   "source": [
    "# Initial training run (from scratch)\n",
    "\n",
    "This cell wires everything together to kick off a multi‑epoch training run using the previously defined `train_model_v1` loop. It selects a device, seeds RNG for reproducibility, instantiates a fresh model, configures an optimizer, and starts training with periodic evaluations and qualitative sampling.\n",
    "\n",
    "---\n",
    "## What the code does\n",
    "1. Chooses a compute device (`cuda` if available, else `cpu`) and prints it.\n",
    "2. Sets a fixed torch seed (`246`) so initialization and early steps are repeatable.\n",
    "3. Creates a new `SydsGPT` model with the 345M‑style config and moves it to the selected device.\n",
    "4. Constructs an `AdamW` optimizer over model parameters with:\n",
    "   - Learning rate: `2e-4`\n",
    "   - Weight decay: `0.05`\n",
    "5. Sets `num_epochs = 5` and calls `train_model_v1(...)` with:\n",
    "   - `evaluation_frequency=100`: run a quick loss snapshot every 100 optimization steps.\n",
    "   - `evaluation_iterations=2`: evaluate only 2 mini‑batches per snapshot to keep it fast.\n",
    "   - `start_context=\"Once upon a time\"`: prompt for qualitative samples at epoch ends.\n",
    "6. Captures outputs:\n",
    "   - `training_losses`: sampled training losses (one per evaluation event)\n",
    "   - `validation_losses`: sampled validation losses aligned with training snapshots\n",
    "   - `total_tokens_processed`: cumulative token count across steps (for plotting)\n",
    "\n",
    "---\n",
    "## Inputs and assumptions\n",
    "- `training_dataloader` and `validation_dataloader` yield `(input_batch, target_batch)` with dtype `torch.long` and shape `(B, T)` each.\n",
    "- The model’s vocab size and `context_length` match the tokenizer/loader.\n",
    "- The training loop handles device placement for batches and uses `cross_entropy` on flattened logits/targets.\n",
    "\n",
    "---\n",
    "## Tuning knobs (quick guidance)\n",
    "- Batch size vs. context length: If you see CUDA OOM, reduce batch size first; then consider lowering `context_length`.\n",
    "- Learning rate: `2e-4` is a reasonable starting point for AdamW; try 1e‑4 to 3e‑4 depending on stability.\n",
    "- Weight decay: `0.05` encourages generalization; adjust if you notice under/over‑regularization.\n",
    "- Evaluation cadence: Increase `evaluation_iterations` once runs are stable to get more precise estimates (at a compute cost).\n",
    "\n",
    "---\n",
    "## Expected outputs\n",
    "- Console logs for steps, tokens processed, and periodic training/validation losses.\n",
    "- A short generated sample at the end of each epoch to qualitatively gauge progress.\n",
    "- On completion, you can optionally save a final checkpoint in the next cell (separate from autosaves inside the loop).\n",
    "\n",
    "---\n",
    "## Troubleshooting\n",
    "- Device mismatch: Ensure `model.to(device)` ran before training; the loop moves batches to `device` internally.\n",
    "- Empty/short dataloader: If your dataset is tiny, `evaluation_frequency` may fire too often; increase it or reduce epochs.\n",
    "- Slow evaluations: Lower `evaluation_iterations` or run snapshots less frequently.\n",
    "\n",
    "---\n",
    "With this cell, you should see both quantitative (loss) and qualitative (sample text) signals to verify your pipeline is training correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9158e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(246)\n",
    "model = SydsGPT(SYDSGPT_CONFIG_345M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.0002, weight_decay = 0.05)\n",
    "num_epochs = 5\n",
    "training_losses, validation_losses, total_tokens_processed = train_model_v1(\n",
    "    model,\n",
    "    training_dataloader,\n",
    "    validation_dataloader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    evaluation_frequency = 100,\n",
    "    evaluation_iterations = 2,\n",
    "    start_context = \"Once upon a time\",\n",
    "    tokenizer = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79de3a5e",
   "metadata": {},
   "source": [
    "# Final checkpoint save (explicit milestone)\n",
    "\n",
    "This cell performs an explicit, manual save of the model and optimizer state at the current training milestone. Although the training loop has already produced periodic autosaves, this dedicated checkpoint is useful for tagging a known-good completion of an initial training phase (e.g., after all planned epochs).\n",
    "\n",
    "---\n",
    "## What gets saved\n",
    "The file `sydsgpt_345m_trained_model_optimizer.pth` contains a Python dictionary with two keys:\n",
    "```python\n",
    "{\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict()\n",
    "}\n",
    "```\n",
    "- `model_state_dict`: Parameter tensors (weights, biases, embeddings, layer norms, attention projections, etc.).\n",
    "- `optimizer_state_dict`: Internal optimizer buffers (e.g., AdamW moment estimates `exp_avg`, `exp_avg_sq`, learning rate state, step counters).\n",
    "\n",
    "Including the optimizer state allows seamless continuation of training without losing momentum statistics or adaptive learning rate context. This preserves optimization dynamics and avoids a transient spike in loss that can occur when restarting with a freshly initialized optimizer.\n",
    "\n",
    "---\n",
    "## Why take a manual checkpoint here\n",
    "- Marks the end of a planned training segment (e.g., initial 5 epochs) distinctly from autosaves that may overwrite.\n",
    "- Ensures you have a stable artifact before experimenting with new hyperparameters, architectures, or data.\n",
    "- Provides a rollback point if subsequent fine‑tuning or continued training degrades performance.\n",
    "\n",
    "---\n",
    "## Reloading later\n",
    "To resume training or run inference:\n",
    "```python\n",
    "checkpoint = torch.load(\"sydsgpt_345m_trained_model_optimizer.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "# Move optimizer state tensors to device (see later reload cell pattern)\n",
    "```\n",
    "After loading, move any optimizer tensors to the target `device` so continued training doesn’t trigger device mismatch errors.\n",
    "\n",
    "---\n",
    "## Differences vs. autosave\n",
    "| Aspect | Autosave file | Manual final checkpoint |\n",
    "|--------|---------------|-------------------------|\n",
    "| Trigger | Time/step interval (`checkpoint_interval`) | Explicit user intent (end of phase) |\n",
    "| Overwrite behavior | Typically overwritten each interval | Usually kept / versioned |\n",
    "| Semantic meaning | Latest state at periodic step | Milestone and reference artifact |\n",
    "\n",
    "Consider versioning milestone checkpoints (e.g., append `_epoch5.pth`, `_tokens{count}.pth`) if you plan multiple phases.\n",
    "\n",
    "---\n",
    "## Best practices\n",
    "- Keep at least two historical milestone checkpoints in case of accidental corruption.\n",
    "- Record metadata externally (JSON/YAML): total tokens processed, epoch count, validation loss at save time.\n",
    "- For inference-only deployment, you can discard `optimizer_state_dict` to reduce file size (just save `model_state_dict`).\n",
    "- Compress large checkpoints when archiving (`torch.save` already uses pickle; for storage, consider `zip` or `tar.gz`).\n",
    "\n",
    "---\n",
    "## Common pitfalls\n",
    "| Pitfall | Symptom | Mitigation |\n",
    "|---------|---------|------------|\n",
    "| Forgetting optimizer state | Restarted training shows learning instability | Always save `optimizer_state_dict` during training checkpoints |\n",
    "| Device mismatch on reload | Runtime error about CPU vs CUDA tensors | After `optimizer.load_state_dict`, move state tensors to `device` |\n",
    "| Overwriting milestone unintentionally | Loss of earlier good snapshot | Use distinct filenames per milestone |\n",
    "| Large disk usage | Many multi‑GB files | Prune intermediate autosaves; retain only curated milestones |\n",
    "\n",
    "---\n",
    "## Minimal variant for inference\n",
    "If you only need the model weights:\n",
    "```python\n",
    "torch.save({\"model_state_dict\": model.state_dict()}, \"sydsgpt_345m_model_only.pth\")\n",
    "```\n",
    "This is smaller and faster to load for pure generation tasks.\n",
    "\n",
    "---\n",
    "## Summary\n",
    "This cell captures a durable, milestone checkpoint bundling both model parameters and optimizer state—essential for faithful resumption and reproducible experimentation beyond the initial training phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "044a6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict()}, \"sydsgpt_345m_trained_model_optimizer.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a09996a",
   "metadata": {},
   "source": [
    "# Reloading a saved checkpoint (model + optimizer)\n",
    "\n",
    "This markdown explains the checkpoint **reload** code directly below. Its purpose is to restore both model weights and optimizer state so you can seamlessly continue training or run inference after a previous session ended.\n",
    "\n",
    "---\n",
    "## What the code does (line by line)\n",
    "1. Picks a device: `cuda` if available else `cpu`.\n",
    "2. Prints the chosen device for visibility.\n",
    "3. Instantiates a fresh `SydsGPT` model with the same config used during training.\n",
    "4. `torch.load(..., map_location=device)` loads the serialized checkpoint dict from disk and ensures all tensors are mapped onto the selected device (or CPU fallback if no GPU).\n",
    "5. `model.load_state_dict(checkpoint['model_state_dict'])` populates the newly created model with the trained parameters.\n",
    "6. Creates a new `AdamW` optimizer with identical hyperparameters (LR, weight decay) used earlier.\n",
    "7. `optimizer.load_state_dict(checkpoint['optimizer_state_dict'])` restores optimizer internal buffers (moment estimates, step counters) for continuity.\n",
    "8. Iterates through every tensor in the optimizer state dict and forces it onto `device` (sometimes `map_location` + `load_state_dict` leave optimizer state tensors on CPU while the model is on GPU—this avoids device mismatch errors during the next `optimizer.step()`).\n",
    "9. Moves the model itself to device (`model.to(device)`).\n",
    "\n",
    "---\n",
    "## Why instantiate a new model before loading\n",
    "You need an object with the correct architecture to receive parameters. The `state_dict` only contains raw tensors keyed by module names; without constructing the model first, there’s nowhere to load them.\n",
    "\n",
    "---\n",
    "## Optimizer state importance\n",
    "Restoring optimizer buffers keeps training dynamics smooth:\n",
    "- AdamW uses first (`exp_avg`) and second (`exp_avg_sq`) moment estimates to adapt per‑parameter learning rates.\n",
    "- Omitting them causes a brief instability phase while moments re‑warm.\n",
    "- Preserving the `step` counter ensures scheduler logic (if later added) resumes correctly.\n",
    "\n",
    "---\n",
    "## Device handling details\n",
    "- `map_location=device` ensures checkpoint tensors don’t try to allocate on an unavailable GPU.\n",
    "- Always move optimizer state tensors explicitly after `load_state_dict`; some PyTorch versions leave them on the original device regardless of `map_location`.\n",
    "- If later using multiple GPUs or `DistributedDataParallel`, load on CPU first, then wrap/replicate.\n",
    "\n",
    "---\n",
    "## Minimal inference-only variant\n",
    "If you only need text generation (no further training):\n",
    "```python\n",
    "model = SydsGPT(SYDSGPT_CONFIG_345M)\n",
    "checkpoint = torch.load(\"sydsgpt_345m_trained_model_optimizer.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# Skip optimizer entirely\n",
    "```\n",
    "This loads faster and uses less memory.\n",
    "\n",
    "---\n",
    "## Summary\n",
    "This reload cell reconstructs the training state (model + optimizer) on the chosen device, enabling seamless continuation or evaluation. Proper device mapping and optimizer state preservation prevent subtle training regressions and runtime errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47686bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SydsGPT(\n",
       "  (token_embedding): Embedding(50257, 1024)\n",
       "  (position_embedding): Embedding(512, 1024)\n",
       "  (drop_embedding): Dropout(p=0.1, inplace=False)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (weight_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (weight_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): LayerNorm()\n",
       "  (output_projection): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SydsGPT(SYDSGPT_CONFIG_345M)\n",
    "checkpoint = torch.load(\"sydsgpt_345m_trained_model_optimizer.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.0002, weight_decay = 0.05)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f15772d",
   "metadata": {},
   "source": [
    "# Post‑reload qualitative sanity check (`generate_sample_text` call)\n",
    "\n",
    "This cell performs a **quick qualitative verification** immediately after restoring the model and optimizer from a checkpoint. It doesn’t train or modify weights; it simply generates a short continuation from a prompt to confirm the load succeeded and the model produces coherent text.\n",
    "\n",
    "---\n",
    "## Purpose\n",
    "- Validate that `model.load_state_dict(...)` correctly restored parameters.\n",
    "- Confirm the model is on the intended `device` (no silent CPU/GPU mismatch).\n",
    "- Provide a human‑readable signal (sample text) before investing time in continued training.\n",
    "\n",
    "---\n",
    "## What happens\n",
    "1. Calls `generate_sample_text(model, tokenizer, device, \"once upon a time\")`.\n",
    "2. Inside the helper:\n",
    "   - Switches to `eval()` (disables dropout) and wraps generation in `no_grad()`.\n",
    "   - Encodes the prompt, performs greedy extension (up to configured max new tokens in the helper), decodes output.\n",
    "   - Prints the generated string (newlines flattened) and returns the model to `train()` mode.\n",
    "\n",
    "---\n",
    "## Why a qualitative sample here\n",
    "- Faster than computing a full validation loss (no dataloader iteration).\n",
    "- Immediately surfaces obvious load issues (garbled or purely random tokens vs. plausible language).\n",
    "- Lets you compare style with samples produced **before** saving the checkpoint.\n",
    "\n",
    "---\n",
    "## Interpreting output\n",
    "| Observation | Likely Meaning | Action |\n",
    "|-------------|----------------|--------|\n",
    "| Fluent continuation resembling earlier runs | Successful checkpoint restore | Proceed to continued training / evaluation |\n",
    "| Completely random / high entropy gibberish | Wrong weights file or failed load | Recheck filename, config mismatch, or `state_dict` keys |\n",
    "| Runtime error about device mismatch | Some tensors still on CPU | Ensure optimizer state + model moved with `.to(device)` earlier |\n",
    "| Identical text every run (expected here) | Deterministic greedy decoding | Introduce temperature/top‑k if diversity needed |\n",
    "\n",
    "---\n",
    "## Customizing\n",
    "- Change the prompt: `\"In a distant future\"`, `\"Chapter 1:\"`, domain‑specific phrase.\n",
    "- Shorter test: modify helper to use fewer new tokens (e.g., 40) for speed.\n",
    "- Add timing: wrap call with `time.perf_counter()` to gauge generation latency.\n",
    "\n",
    "---\n",
    "## Summary\n",
    "A lightweight, single‑call checkpoint verification: generate a deterministic sample to ensure the restored model behaves plausibly before resuming expensive training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2bda2ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgenerate_sample_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43monce upon a time\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mgenerate_sample_text\u001b[39m\u001b[34m(model, tokenizer, device, start_context)\u001b[39m\n\u001b[32m      4\u001b[39m input_tokens = text_to_tokens(start_context, tokenizer).to(device)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     generated_tokens = \u001b[43mgenerate_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m generated_text = tokens_to_text(generated_tokens, tokenizer)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerated Text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m.replace(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Code\\SydsGPT-Pretraining\\modules\\GenerateSimple.py:7\u001b[39m, in \u001b[36mgenerate_simple\u001b[39m\u001b[34m(model, input_ids, max_length, context_size)\u001b[39m\n\u001b[32m      5\u001b[39m input_ids_crop = input_ids[:, -context_size:]\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     logits  = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids_crop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m next_token_logits = logits[:, -\u001b[32m1\u001b[39m, :]\n\u001b[32m      9\u001b[39m next_token_probs = torch.softmax(next_token_logits, dim = -\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Code\\SydsGPT-Pretraining\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Code\\SydsGPT-Pretraining\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Code\\SydsGPT-Pretraining\\model\\SydsGPT.py:18\u001b[39m, in \u001b[36mSydsGPT.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m     17\u001b[39m     batch_size, seq_length = \u001b[38;5;28minput\u001b[39m.shape\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     token_embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     position_embeddings = \u001b[38;5;28mself\u001b[39m.position_embedding(torch.arange(seq_length, device=\u001b[38;5;28minput\u001b[39m.device))\n\u001b[32m     20\u001b[39m     x = token_embeddings + position_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Code\\SydsGPT-Pretraining\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Code\\SydsGPT-Pretraining\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Code\\SydsGPT-Pretraining\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Code\\SydsGPT-Pretraining\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "generate_sample_text(model, tokenizer, device, \"once upon a time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc70e45",
   "metadata": {},
   "source": [
    "# Continue training from a saved checkpoint (resume run)\n",
    "\n",
    "This cell resumes optimization from a previously saved checkpoint. It reconstructs the model and optimizer states on the chosen device, then runs a shorter training segment with periodic evaluation and autosaves.\n",
    "\n",
    "---\n",
    "## What the code does\n",
    "1. Selects a device (`cuda` if available, else `cpu`) and prints it.\n",
    "2. Instantiates a fresh `SydsGPT` with the same configuration used earlier.\n",
    "3. Loads the checkpoint dictionary from disk with `map_location=device` so tensors land on the intended device even without a GPU.\n",
    "4. Restores weights: `model.load_state_dict(checkpoint[\"model_state_dict\"])`.\n",
    "5. Creates an `AdamW` optimizer and restores its state with `optimizer.load_state_dict(...)` (includes moment buffers and step counters).\n",
    "6. Ensures all optimizer state tensors live on `device` (prevents CPU/GPU mismatch during `optimizer.step()`).\n",
    "7. Moves the `model` to `device` and kicks off training for `num_epochs = 2` via `train_model_v1` with quick evaluations.\n",
    "\n",
    "---\n",
    "## Resume semantics and counters\n",
    "- Optimizer continuity: AdamW internal buffers (`exp_avg`, `exp_avg_sq`) and `state['step']` are restored, preserving learning dynamics.\n",
    "- Training loop counters: `train_model_v1` initializes a fresh `global_step = -1` and `tokens_processed = 0` each time you call it. That means:\n",
    "  - Evaluation/checkpoint cadence restarts from step 0 for this resumed segment.\n",
    "  - If you need a global step across sessions, extend the function to accept and persist `global_step`/`tokens_processed`.\n",
    "\n",
    "---\n",
    "## Evaluation and checkpoint cadence\n",
    "- `evaluation_frequency = 100`: samples losses every 100 updates using only `evaluation_iterations = 2` mini‑batches for speed.\n",
    "- Autosave: every `checkpoint_interval` steps (default set inside `train_model_v1`), an autosave writes to `autosave_sydsgpt_345m_trained_model_optimizer.pth`.\n",
    "  - Note: This filename may overwrite a prior autosave. Use timestamped or step‑indexed names if you want a trail of snapshots.\n",
    "\n",
    "---\n",
    "## Tuning knobs\n",
    "- `num_epochs`: increase for longer continuation runs.\n",
    "- `lr` and `weight_decay`: keep consistent with the original run for stability; adjust cautiously if loss plateaus.\n",
    "- `evaluation_iterations`: raise to improve estimate accuracy (at the cost of extra compute).\n",
    "- Consider enabling mixed precision (AMP) in `train_model_v1` for speed on supported GPUs.\n",
    "\n",
    "---\n",
    "## Assumptions and safety checks\n",
    "- Model config matches the checkpoint (same `embedding_dim`, `num_layers`, `num_heads`, `vocab_size`, `context_length`).\n",
    "- Checkpoint path `sydsgpt_345m_trained_model_optimizer.pth` exists in the working directory.\n",
    "- Tokenizer and dataloaders are unchanged from the original training setup.\n",
    "\n",
    "---\n",
    "## Quick verification\n",
    "- Watch early training log lines for decreasing validation loss compared to just‑restored values.\n",
    "- Generate a short sample at each epoch end (already handled via `generate_sample_text` inside `train_model_v1`).\n",
    "\n",
    "---\n",
    "## Outputs\n",
    "- `training_losses`, `validation_losses`: sampled losses at the chosen evaluation cadence.\n",
    "- `total_tokens_processed`: cumulative tokens (within this resumed segment) for plotting progress.\n",
    "\n",
    "---\n",
    "## Summary\n",
    "This cell cleanly restores model and optimizer state, then continues training with lightweight periodic evaluations and autosaves—ideal for incremental improvements without losing prior optimization momentum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca15bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1, Step 0: Tokens Processed = 4096\n",
      "Epoch 1, Step 0: Tokens Processed = 4096\n",
      "Epoch 1, Step 0: Training Loss = 2.4921298027038574, Validation Loss = 5.017168760299683, Tokens Processed = 4096\n",
      "Epoch 1, Step 0: Training Loss = 2.4921298027038574, Validation Loss = 5.017168760299683, Tokens Processed = 4096\n",
      "Epoch 1, Step 1: Tokens Processed = 8192\n",
      "Epoch 1, Step 1: Tokens Processed = 8192\n",
      "Epoch 1, Step 2: Tokens Processed = 12288\n",
      "Epoch 1, Step 2: Tokens Processed = 12288\n",
      "Epoch 1, Step 3: Tokens Processed = 16384\n",
      "Epoch 1, Step 3: Tokens Processed = 16384\n",
      "Epoch 1, Step 4: Tokens Processed = 20480\n",
      "Epoch 1, Step 4: Tokens Processed = 20480\n",
      "Epoch 1, Step 5: Tokens Processed = 24576\n",
      "Epoch 1, Step 5: Tokens Processed = 24576\n",
      "Epoch 1, Step 6: Tokens Processed = 28672\n",
      "Epoch 1, Step 6: Tokens Processed = 28672\n",
      "Epoch 1, Step 7: Tokens Processed = 32768\n",
      "Epoch 1, Step 7: Tokens Processed = 32768\n",
      "Epoch 1, Step 8: Tokens Processed = 36864\n",
      "Epoch 1, Step 8: Tokens Processed = 36864\n",
      "Epoch 1, Step 9: Tokens Processed = 40960\n",
      "Epoch 1, Step 9: Tokens Processed = 40960\n",
      "Epoch 1, Step 10: Tokens Processed = 45056\n",
      "Epoch 1, Step 10: Tokens Processed = 45056\n",
      "Epoch 1, Step 11: Tokens Processed = 49152\n",
      "Epoch 1, Step 11: Tokens Processed = 49152\n",
      "Epoch 1, Step 12: Tokens Processed = 53248\n",
      "Epoch 1, Step 12: Tokens Processed = 53248\n",
      "Epoch 1, Step 13: Tokens Processed = 57344\n",
      "Epoch 1, Step 13: Tokens Processed = 57344\n",
      "Epoch 1, Step 14: Tokens Processed = 61440\n",
      "Epoch 1, Step 14: Tokens Processed = 61440\n",
      "Epoch 1, Step 15: Tokens Processed = 65536\n",
      "Epoch 1, Step 15: Tokens Processed = 65536\n",
      "Epoch 1, Step 16: Tokens Processed = 69632\n",
      "Epoch 1, Step 16: Tokens Processed = 69632\n",
      "Epoch 1, Step 17: Tokens Processed = 73728\n",
      "Epoch 1, Step 17: Tokens Processed = 73728\n",
      "Epoch 1, Step 18: Tokens Processed = 77824\n",
      "Epoch 1, Step 18: Tokens Processed = 77824\n",
      "Epoch 1, Step 19: Tokens Processed = 81920\n",
      "Epoch 1, Step 19: Tokens Processed = 81920\n",
      "Epoch 1, Step 20: Tokens Processed = 86016\n",
      "Epoch 1, Step 20: Tokens Processed = 86016\n",
      "Epoch 1, Step 21: Tokens Processed = 90112\n",
      "Epoch 1, Step 21: Tokens Processed = 90112\n",
      "Epoch 1, Step 22: Tokens Processed = 94208\n",
      "Epoch 1, Step 22: Tokens Processed = 94208\n",
      "Epoch 1, Step 23: Tokens Processed = 98304\n",
      "Epoch 1, Step 23: Tokens Processed = 98304\n",
      "Epoch 1, Step 24: Tokens Processed = 102400\n",
      "Epoch 1, Step 24: Tokens Processed = 102400\n",
      "Epoch 1, Step 25: Tokens Processed = 106496\n",
      "Epoch 1, Step 25: Tokens Processed = 106496\n",
      "Epoch 1, Step 26: Tokens Processed = 110592\n",
      "Epoch 1, Step 26: Tokens Processed = 110592\n",
      "Epoch 1, Step 27: Tokens Processed = 114688\n",
      "Epoch 1, Step 27: Tokens Processed = 114688\n",
      "Epoch 1, Step 28: Tokens Processed = 118784\n",
      "Epoch 1, Step 28: Tokens Processed = 118784\n",
      "Epoch 1, Step 29: Tokens Processed = 122880\n",
      "Epoch 1, Step 29: Tokens Processed = 122880\n",
      "Epoch 1, Step 30: Tokens Processed = 126976\n",
      "Epoch 1, Step 30: Tokens Processed = 126976\n",
      "Epoch 1, Step 31: Tokens Processed = 131072\n",
      "Epoch 1, Step 31: Tokens Processed = 131072\n",
      "Epoch 1, Step 32: Tokens Processed = 135168\n",
      "Epoch 1, Step 32: Tokens Processed = 135168\n",
      "Epoch 1, Step 33: Tokens Processed = 139264\n",
      "Epoch 1, Step 33: Tokens Processed = 139264\n",
      "Epoch 1, Step 34: Tokens Processed = 143360\n",
      "Epoch 1, Step 34: Tokens Processed = 143360\n",
      "Epoch 1, Step 35: Tokens Processed = 147456\n",
      "Epoch 1, Step 35: Tokens Processed = 147456\n",
      "Epoch 1, Step 36: Tokens Processed = 151552\n",
      "Epoch 1, Step 36: Tokens Processed = 151552\n",
      "Epoch 1, Step 37: Tokens Processed = 155648\n",
      "Epoch 1, Step 37: Tokens Processed = 155648\n",
      "Epoch 1, Step 38: Tokens Processed = 159744\n",
      "Epoch 1, Step 38: Tokens Processed = 159744\n",
      "Epoch 1, Step 39: Tokens Processed = 163840\n",
      "Epoch 1, Step 39: Tokens Processed = 163840\n",
      "Epoch 1, Step 40: Tokens Processed = 167936\n",
      "Epoch 1, Step 40: Tokens Processed = 167936\n",
      "Epoch 1, Step 41: Tokens Processed = 172032\n",
      "Epoch 1, Step 41: Tokens Processed = 172032\n",
      "Epoch 1, Step 42: Tokens Processed = 176128\n",
      "Epoch 1, Step 42: Tokens Processed = 176128\n",
      "Epoch 1, Step 43: Tokens Processed = 180224\n",
      "Epoch 1, Step 43: Tokens Processed = 180224\n",
      "Epoch 1, Step 44: Tokens Processed = 184320\n",
      "Epoch 1, Step 44: Tokens Processed = 184320\n",
      "Epoch 1, Step 45: Tokens Processed = 188416\n",
      "Epoch 1, Step 45: Tokens Processed = 188416\n",
      "Epoch 1, Step 46: Tokens Processed = 192512\n",
      "Epoch 1, Step 46: Tokens Processed = 192512\n",
      "Epoch 1, Step 47: Tokens Processed = 196608\n",
      "Epoch 1, Step 47: Tokens Processed = 196608\n",
      "Epoch 1, Step 48: Tokens Processed = 200704\n",
      "Epoch 1, Step 48: Tokens Processed = 200704\n",
      "Epoch 1, Step 49: Tokens Processed = 204800\n",
      "Epoch 1, Step 49: Tokens Processed = 204800\n",
      "Epoch 1, Step 50: Tokens Processed = 208896\n",
      "Epoch 1, Step 50: Tokens Processed = 208896\n",
      "Epoch 1, Step 51: Tokens Processed = 212992\n",
      "Epoch 1, Step 51: Tokens Processed = 212992\n",
      "Epoch 1, Step 52: Tokens Processed = 217088\n",
      "Epoch 1, Step 52: Tokens Processed = 217088\n",
      "Epoch 1, Step 53: Tokens Processed = 221184\n",
      "Epoch 1, Step 53: Tokens Processed = 221184\n",
      "Epoch 1, Step 54: Tokens Processed = 225280\n",
      "Epoch 1, Step 54: Tokens Processed = 225280\n",
      "Epoch 1, Step 55: Tokens Processed = 229376\n",
      "Epoch 1, Step 55: Tokens Processed = 229376\n",
      "Epoch 1, Step 56: Tokens Processed = 233472\n",
      "Epoch 1, Step 56: Tokens Processed = 233472\n",
      "Epoch 1, Step 57: Tokens Processed = 237568\n",
      "Epoch 1, Step 57: Tokens Processed = 237568\n",
      "Epoch 1, Step 58: Tokens Processed = 241664\n",
      "Epoch 1, Step 58: Tokens Processed = 241664\n",
      "Epoch 1, Step 59: Tokens Processed = 245760\n",
      "Epoch 1, Step 59: Tokens Processed = 245760\n",
      "Epoch 1, Step 60: Tokens Processed = 249856\n",
      "Epoch 1, Step 60: Tokens Processed = 249856\n",
      "Epoch 1, Step 61: Tokens Processed = 253952\n",
      "Epoch 1, Step 61: Tokens Processed = 253952\n",
      "Epoch 1, Step 62: Tokens Processed = 258048\n",
      "Epoch 1, Step 62: Tokens Processed = 258048\n",
      "Epoch 1, Step 63: Tokens Processed = 262144\n",
      "Epoch 1, Step 63: Tokens Processed = 262144\n",
      "Epoch 1, Step 64: Tokens Processed = 266240\n",
      "Epoch 1, Step 64: Tokens Processed = 266240\n",
      "Epoch 1, Step 65: Tokens Processed = 270336\n",
      "Epoch 1, Step 65: Tokens Processed = 270336\n",
      "Epoch 1, Step 66: Tokens Processed = 274432\n",
      "Epoch 1, Step 66: Tokens Processed = 274432\n",
      "Epoch 1, Step 67: Tokens Processed = 278528\n",
      "Epoch 1, Step 67: Tokens Processed = 278528\n",
      "Epoch 1, Step 68: Tokens Processed = 282624\n",
      "Epoch 1, Step 68: Tokens Processed = 282624\n",
      "Epoch 1, Step 69: Tokens Processed = 286720\n",
      "Epoch 1, Step 69: Tokens Processed = 286720\n",
      "Epoch 1, Step 70: Tokens Processed = 290816\n",
      "Epoch 1, Step 70: Tokens Processed = 290816\n",
      "Epoch 1, Step 71: Tokens Processed = 294912\n",
      "Epoch 1, Step 71: Tokens Processed = 294912\n",
      "Epoch 1, Step 72: Tokens Processed = 299008\n",
      "Epoch 1, Step 72: Tokens Processed = 299008\n",
      "Epoch 1, Step 73: Tokens Processed = 303104\n",
      "Epoch 1, Step 73: Tokens Processed = 303104\n",
      "Epoch 1, Step 74: Tokens Processed = 307200\n",
      "Epoch 1, Step 74: Tokens Processed = 307200\n",
      "Epoch 1, Step 75: Tokens Processed = 311296\n",
      "Epoch 1, Step 75: Tokens Processed = 311296\n",
      "Epoch 1, Step 76: Tokens Processed = 315392\n",
      "Epoch 1, Step 76: Tokens Processed = 315392\n",
      "Epoch 1, Step 77: Tokens Processed = 319488\n",
      "Epoch 1, Step 77: Tokens Processed = 319488\n",
      "Epoch 1, Step 78: Tokens Processed = 323584\n",
      "Epoch 1, Step 78: Tokens Processed = 323584\n",
      "Epoch 1, Step 79: Tokens Processed = 327680\n",
      "Epoch 1, Step 79: Tokens Processed = 327680\n",
      "Epoch 1, Step 80: Tokens Processed = 331776\n",
      "Epoch 1, Step 80: Tokens Processed = 331776\n",
      "Epoch 1, Step 81: Tokens Processed = 335872\n",
      "Epoch 1, Step 81: Tokens Processed = 335872\n",
      "Epoch 1, Step 82: Tokens Processed = 339968\n",
      "Epoch 1, Step 82: Tokens Processed = 339968\n",
      "Epoch 1, Step 83: Tokens Processed = 344064\n",
      "Epoch 1, Step 83: Tokens Processed = 344064\n",
      "Epoch 1, Step 84: Tokens Processed = 348160\n",
      "Epoch 1, Step 84: Tokens Processed = 348160\n",
      "Epoch 1, Step 85: Tokens Processed = 352256\n",
      "Epoch 1, Step 85: Tokens Processed = 352256\n",
      "Epoch 1, Step 86: Tokens Processed = 356352\n",
      "Epoch 1, Step 86: Tokens Processed = 356352\n",
      "Epoch 1, Step 87: Tokens Processed = 360448\n",
      "Epoch 1, Step 87: Tokens Processed = 360448\n",
      "Epoch 1, Step 88: Tokens Processed = 364544\n",
      "Epoch 1, Step 88: Tokens Processed = 364544\n",
      "Epoch 1, Step 89: Tokens Processed = 368640\n",
      "Epoch 1, Step 89: Tokens Processed = 368640\n",
      "Epoch 1, Step 90: Tokens Processed = 372736\n",
      "Epoch 1, Step 90: Tokens Processed = 372736\n",
      "Epoch 1, Step 91: Tokens Processed = 376832\n",
      "Epoch 1, Step 91: Tokens Processed = 376832\n",
      "Epoch 1, Step 92: Tokens Processed = 380928\n",
      "Epoch 1, Step 92: Tokens Processed = 380928\n",
      "Epoch 1, Step 93: Tokens Processed = 385024\n",
      "Epoch 1, Step 93: Tokens Processed = 385024\n",
      "Epoch 1, Step 94: Tokens Processed = 389120\n",
      "Epoch 1, Step 94: Tokens Processed = 389120\n",
      "Epoch 1, Step 95: Tokens Processed = 393216\n",
      "Epoch 1, Step 95: Tokens Processed = 393216\n",
      "Epoch 1, Step 96: Tokens Processed = 397312\n",
      "Epoch 1, Step 96: Tokens Processed = 397312\n",
      "Epoch 1, Step 97: Tokens Processed = 401408\n",
      "Epoch 1, Step 97: Tokens Processed = 401408\n",
      "Epoch 1, Step 98: Tokens Processed = 405504\n",
      "Epoch 1, Step 98: Tokens Processed = 405504\n",
      "Epoch 1, Step 99: Tokens Processed = 409600\n",
      "Epoch 1, Step 99: Tokens Processed = 409600\n",
      "Epoch 1, Step 100: Tokens Processed = 413696\n",
      "Epoch 1, Step 100: Tokens Processed = 413696\n",
      "Epoch 1, Step 100: Training Loss = 2.1851747035980225, Validation Loss = 5.291724920272827, Tokens Processed = 413696\n",
      "Epoch 1, Step 100: Training Loss = 2.1851747035980225, Validation Loss = 5.291724920272827, Tokens Processed = 413696\n",
      "Epoch 1, Step 101: Tokens Processed = 417792\n",
      "Epoch 1, Step 101: Tokens Processed = 417792\n",
      "Epoch 1, Step 102: Tokens Processed = 421888\n",
      "Epoch 1, Step 102: Tokens Processed = 421888\n",
      "Epoch 1, Step 103: Tokens Processed = 425984\n",
      "Epoch 1, Step 103: Tokens Processed = 425984\n",
      "Epoch 1, Step 104: Tokens Processed = 430080\n",
      "Epoch 1, Step 104: Tokens Processed = 430080\n",
      "Epoch 1, Step 105: Tokens Processed = 434176\n",
      "Epoch 1, Step 105: Tokens Processed = 434176\n",
      "Epoch 1, Step 106: Tokens Processed = 438272\n",
      "Epoch 1, Step 106: Tokens Processed = 438272\n",
      "Epoch 1, Step 107: Tokens Processed = 442368\n",
      "Epoch 1, Step 107: Tokens Processed = 442368\n",
      "Epoch 1, Step 108: Tokens Processed = 446464\n",
      "Epoch 1, Step 108: Tokens Processed = 446464\n",
      "Epoch 1, Step 109: Tokens Processed = 450560\n",
      "Epoch 1, Step 109: Tokens Processed = 450560\n",
      "Epoch 1, Step 110: Tokens Processed = 454656\n",
      "Epoch 1, Step 110: Tokens Processed = 454656\n",
      "Epoch 1, Step 111: Tokens Processed = 458752\n",
      "Epoch 1, Step 111: Tokens Processed = 458752\n",
      "Epoch 1, Step 112: Tokens Processed = 462848\n",
      "Epoch 1, Step 112: Tokens Processed = 462848\n",
      "Epoch 1, Step 113: Tokens Processed = 466944\n",
      "Epoch 1, Step 113: Tokens Processed = 466944\n",
      "Epoch 1, Step 114: Tokens Processed = 471040\n",
      "Epoch 1, Step 114: Tokens Processed = 471040\n",
      "Epoch 1, Step 115: Tokens Processed = 475136\n",
      "Epoch 1, Step 115: Tokens Processed = 475136\n",
      "Epoch 1, Step 116: Tokens Processed = 479232\n",
      "Epoch 1, Step 116: Tokens Processed = 479232\n",
      "Epoch 1, Step 117: Tokens Processed = 483328\n",
      "Epoch 1, Step 117: Tokens Processed = 483328\n",
      "Epoch 1, Step 118: Tokens Processed = 487424\n",
      "Epoch 1, Step 118: Tokens Processed = 487424\n",
      "Epoch 1, Step 119: Tokens Processed = 491520\n",
      "Epoch 1, Step 119: Tokens Processed = 491520\n",
      "Epoch 1, Step 120: Tokens Processed = 495616\n",
      "Epoch 1, Step 120: Tokens Processed = 495616\n",
      "Epoch 1, Step 121: Tokens Processed = 499712\n",
      "Epoch 1, Step 121: Tokens Processed = 499712\n",
      "Epoch 1, Step 122: Tokens Processed = 503808\n",
      "Epoch 1, Step 122: Tokens Processed = 503808\n",
      "Epoch 1, Step 123: Tokens Processed = 507904\n",
      "Epoch 1, Step 123: Tokens Processed = 507904\n",
      "Epoch 1, Step 124: Tokens Processed = 512000\n",
      "Epoch 1, Step 124: Tokens Processed = 512000\n",
      "Epoch 1, Step 125: Tokens Processed = 516096\n",
      "Epoch 1, Step 125: Tokens Processed = 516096\n",
      "Epoch 1, Step 126: Tokens Processed = 520192\n",
      "Epoch 1, Step 126: Tokens Processed = 520192\n",
      "Epoch 1, Step 127: Tokens Processed = 524288\n",
      "Epoch 1, Step 127: Tokens Processed = 524288\n",
      "Epoch 1, Step 128: Tokens Processed = 528384\n",
      "Epoch 1, Step 128: Tokens Processed = 528384\n",
      "Epoch 1, Step 129: Tokens Processed = 532480\n",
      "Epoch 1, Step 129: Tokens Processed = 532480\n",
      "Epoch 1, Step 130: Tokens Processed = 536576\n",
      "Epoch 1, Step 130: Tokens Processed = 536576\n",
      "Epoch 1, Step 131: Tokens Processed = 540672\n",
      "Epoch 1, Step 131: Tokens Processed = 540672\n",
      "Epoch 1, Step 132: Tokens Processed = 544768\n",
      "Epoch 1, Step 132: Tokens Processed = 544768\n",
      "Epoch 1, Step 133: Tokens Processed = 548864\n",
      "Epoch 1, Step 133: Tokens Processed = 548864\n",
      "Epoch 1, Step 134: Tokens Processed = 552960\n",
      "Epoch 1, Step 134: Tokens Processed = 552960\n",
      "Epoch 1, Step 135: Tokens Processed = 557056\n",
      "Epoch 1, Step 135: Tokens Processed = 557056\n",
      "Epoch 1, Step 136: Tokens Processed = 561152\n",
      "Epoch 1, Step 136: Tokens Processed = 561152\n",
      "Epoch 1, Step 137: Tokens Processed = 565248\n",
      "Epoch 1, Step 137: Tokens Processed = 565248\n",
      "Epoch 1, Step 138: Tokens Processed = 569344\n",
      "Epoch 1, Step 138: Tokens Processed = 569344\n",
      "Epoch 1, Step 139: Tokens Processed = 573440\n",
      "Epoch 1, Step 139: Tokens Processed = 573440\n",
      "Epoch 1, Step 140: Tokens Processed = 577536\n",
      "Epoch 1, Step 140: Tokens Processed = 577536\n",
      "Epoch 1, Step 141: Tokens Processed = 581632\n",
      "Epoch 1, Step 141: Tokens Processed = 581632\n",
      "Epoch 1, Step 142: Tokens Processed = 585728\n",
      "Epoch 1, Step 142: Tokens Processed = 585728\n",
      "Epoch 1, Step 143: Tokens Processed = 589824\n",
      "Epoch 1, Step 143: Tokens Processed = 589824\n",
      "Epoch 1, Step 144: Tokens Processed = 593920\n",
      "Epoch 1, Step 144: Tokens Processed = 593920\n",
      "Epoch 1, Step 145: Tokens Processed = 598016\n",
      "Epoch 1, Step 145: Tokens Processed = 598016\n",
      "Epoch 1, Step 146: Tokens Processed = 602112\n",
      "Epoch 1, Step 146: Tokens Processed = 602112\n",
      "Epoch 1, Step 147: Tokens Processed = 606208\n",
      "Epoch 1, Step 147: Tokens Processed = 606208\n",
      "Epoch 1, Step 148: Tokens Processed = 610304\n",
      "Epoch 1, Step 148: Tokens Processed = 610304\n",
      "Epoch 1, Step 149: Tokens Processed = 614400\n",
      "Epoch 1, Step 149: Tokens Processed = 614400\n",
      "Epoch 1, Step 150: Tokens Processed = 618496\n",
      "Epoch 1, Step 150: Tokens Processed = 618496\n",
      "Epoch 1, Step 151: Tokens Processed = 622592\n",
      "Epoch 1, Step 151: Tokens Processed = 622592\n",
      "Epoch 1, Step 152: Tokens Processed = 626688\n",
      "Epoch 1, Step 152: Tokens Processed = 626688\n",
      "Epoch 1, Step 153: Tokens Processed = 630784\n",
      "Epoch 1, Step 153: Tokens Processed = 630784\n",
      "Epoch 1, Step 154: Tokens Processed = 634880\n",
      "Epoch 1, Step 154: Tokens Processed = 634880\n",
      "Epoch 1, Step 155: Tokens Processed = 638976\n",
      "Epoch 1, Step 155: Tokens Processed = 638976\n",
      "Epoch 1, Step 156: Tokens Processed = 643072\n",
      "Epoch 1, Step 156: Tokens Processed = 643072\n",
      "Epoch 1, Step 157: Tokens Processed = 647168\n",
      "Epoch 1, Step 157: Tokens Processed = 647168\n",
      "Epoch 1, Step 158: Tokens Processed = 651264\n",
      "Epoch 1, Step 158: Tokens Processed = 651264\n",
      "Epoch 1, Step 159: Tokens Processed = 655360\n",
      "Epoch 1, Step 159: Tokens Processed = 655360\n",
      "Epoch 1, Step 160: Tokens Processed = 659456\n",
      "Epoch 1, Step 160: Tokens Processed = 659456\n",
      "Epoch 1, Step 161: Tokens Processed = 663552\n",
      "Epoch 1, Step 161: Tokens Processed = 663552\n",
      "Epoch 1, Step 162: Tokens Processed = 667648\n",
      "Epoch 1, Step 162: Tokens Processed = 667648\n",
      "Epoch 1, Step 163: Tokens Processed = 671744\n",
      "Epoch 1, Step 163: Tokens Processed = 671744\n",
      "Epoch 1, Step 164: Tokens Processed = 675840\n",
      "Epoch 1, Step 164: Tokens Processed = 675840\n",
      "Epoch 1, Step 165: Tokens Processed = 679936\n",
      "Epoch 1, Step 165: Tokens Processed = 679936\n",
      "Epoch 1, Step 166: Tokens Processed = 684032\n",
      "Epoch 1, Step 166: Tokens Processed = 684032\n",
      "Epoch 1, Step 167: Tokens Processed = 688128\n",
      "Epoch 1, Step 167: Tokens Processed = 688128\n",
      "Epoch 1, Step 168: Tokens Processed = 692224\n",
      "Epoch 1, Step 168: Tokens Processed = 692224\n",
      "Epoch 1, Step 169: Tokens Processed = 696320\n",
      "Epoch 1, Step 169: Tokens Processed = 696320\n",
      "Epoch 1, Step 170: Tokens Processed = 700416\n",
      "Epoch 1, Step 170: Tokens Processed = 700416\n",
      "Epoch 1, Step 171: Tokens Processed = 704512\n",
      "Epoch 1, Step 171: Tokens Processed = 704512\n",
      "Epoch 1, Step 172: Tokens Processed = 708608\n",
      "Epoch 1, Step 172: Tokens Processed = 708608\n",
      "Epoch 1, Step 173: Tokens Processed = 712704\n",
      "Epoch 1, Step 173: Tokens Processed = 712704\n",
      "Epoch 1, Step 174: Tokens Processed = 716800\n",
      "Epoch 1, Step 174: Tokens Processed = 716800\n",
      "Epoch 1, Step 175: Tokens Processed = 720896\n",
      "Epoch 1, Step 175: Tokens Processed = 720896\n",
      "Epoch 1, Step 176: Tokens Processed = 724992\n",
      "Epoch 1, Step 176: Tokens Processed = 724992\n",
      "Epoch 1, Step 177: Tokens Processed = 729088\n",
      "Epoch 1, Step 177: Tokens Processed = 729088\n",
      "Epoch 1, Step 178: Tokens Processed = 733184\n",
      "Epoch 1, Step 178: Tokens Processed = 733184\n",
      "Epoch 1, Step 179: Tokens Processed = 737280\n",
      "Epoch 1, Step 179: Tokens Processed = 737280\n",
      "Epoch 1, Step 180: Tokens Processed = 741376\n",
      "Epoch 1, Step 180: Tokens Processed = 741376\n",
      "Epoch 1, Step 181: Tokens Processed = 745472\n",
      "Epoch 1, Step 181: Tokens Processed = 745472\n",
      "Epoch 1, Step 182: Tokens Processed = 749568\n",
      "Epoch 1, Step 182: Tokens Processed = 749568\n",
      "Epoch 1, Step 183: Tokens Processed = 753664\n",
      "Epoch 1, Step 183: Tokens Processed = 753664\n",
      "Epoch 1, Step 184: Tokens Processed = 757760\n",
      "Epoch 1, Step 184: Tokens Processed = 757760\n",
      "Epoch 1, Step 185: Tokens Processed = 761856\n",
      "Epoch 1, Step 185: Tokens Processed = 761856\n",
      "Epoch 1, Step 186: Tokens Processed = 765952\n",
      "Epoch 1, Step 186: Tokens Processed = 765952\n",
      "Epoch 1, Step 187: Tokens Processed = 770048\n",
      "Epoch 1, Step 187: Tokens Processed = 770048\n",
      "Epoch 1, Step 188: Tokens Processed = 774144\n",
      "Epoch 1, Step 188: Tokens Processed = 774144\n",
      "Epoch 1, Step 189: Tokens Processed = 778240\n",
      "Epoch 1, Step 189: Tokens Processed = 778240\n",
      "Epoch 1, Step 190: Tokens Processed = 782336\n",
      "Epoch 1, Step 190: Tokens Processed = 782336\n",
      "Epoch 1, Step 191: Tokens Processed = 786432\n",
      "Epoch 1, Step 191: Tokens Processed = 786432\n",
      "Epoch 1, Step 192: Tokens Processed = 790528\n",
      "Epoch 1, Step 192: Tokens Processed = 790528\n",
      "Epoch 1, Step 193: Tokens Processed = 794624\n",
      "Epoch 1, Step 193: Tokens Processed = 794624\n",
      "Epoch 1, Step 194: Tokens Processed = 798720\n",
      "Epoch 1, Step 194: Tokens Processed = 798720\n",
      "Epoch 1, Step 195: Tokens Processed = 802816\n",
      "Epoch 1, Step 195: Tokens Processed = 802816\n",
      "Epoch 1, Step 196: Tokens Processed = 806912\n",
      "Epoch 1, Step 196: Tokens Processed = 806912\n",
      "Epoch 1, Step 197: Tokens Processed = 811008\n",
      "Epoch 1, Step 197: Tokens Processed = 811008\n",
      "Epoch 1, Step 198: Tokens Processed = 815104\n",
      "Epoch 1, Step 198: Tokens Processed = 815104\n",
      "Epoch 1, Step 199: Tokens Processed = 819200\n",
      "Epoch 1, Step 199: Tokens Processed = 819200\n",
      "Epoch 1, Step 200: Tokens Processed = 823296\n",
      "Epoch 1, Step 200: Tokens Processed = 823296\n",
      "Epoch 1, Step 200: Training Loss = 2.196773052215576, Validation Loss = 4.590626001358032, Tokens Processed = 823296\n",
      "Epoch 1, Step 200: Training Loss = 2.196773052215576, Validation Loss = 4.590626001358032, Tokens Processed = 823296\n",
      "Epoch 1, Step 201: Tokens Processed = 827392\n",
      "Epoch 1, Step 201: Tokens Processed = 827392\n",
      "Epoch 1, Step 202: Tokens Processed = 831488\n",
      "Epoch 1, Step 202: Tokens Processed = 831488\n",
      "Epoch 1, Step 203: Tokens Processed = 835584\n",
      "Epoch 1, Step 203: Tokens Processed = 835584\n",
      "Epoch 1, Step 204: Tokens Processed = 839680\n",
      "Epoch 1, Step 204: Tokens Processed = 839680\n",
      "Epoch 1, Step 205: Tokens Processed = 843776\n",
      "Epoch 1, Step 205: Tokens Processed = 843776\n",
      "Epoch 1, Step 206: Tokens Processed = 847872\n",
      "Epoch 1, Step 206: Tokens Processed = 847872\n",
      "Epoch 1, Step 207: Tokens Processed = 851968\n",
      "Epoch 1, Step 207: Tokens Processed = 851968\n",
      "Epoch 1, Step 208: Tokens Processed = 856064\n",
      "Epoch 1, Step 208: Tokens Processed = 856064\n",
      "Epoch 1, Step 209: Tokens Processed = 860160\n",
      "Epoch 1, Step 209: Tokens Processed = 860160\n",
      "Epoch 1, Step 210: Tokens Processed = 864256\n",
      "Epoch 1, Step 210: Tokens Processed = 864256\n",
      "Epoch 1, Step 211: Tokens Processed = 868352\n",
      "Epoch 1, Step 211: Tokens Processed = 868352\n",
      "Epoch 1, Step 212: Tokens Processed = 872448\n",
      "Epoch 1, Step 212: Tokens Processed = 872448\n",
      "Epoch 1, Step 213: Tokens Processed = 876544\n",
      "Epoch 1, Step 213: Tokens Processed = 876544\n",
      "Epoch 1, Step 214: Tokens Processed = 880640\n",
      "Epoch 1, Step 214: Tokens Processed = 880640\n",
      "Epoch 1, Step 215: Tokens Processed = 884736\n",
      "Epoch 1, Step 215: Tokens Processed = 884736\n",
      "Epoch 1, Step 216: Tokens Processed = 888832\n",
      "Epoch 1, Step 216: Tokens Processed = 888832\n",
      "Epoch 1, Step 217: Tokens Processed = 892928\n",
      "Epoch 1, Step 217: Tokens Processed = 892928\n",
      "Epoch 1, Step 218: Tokens Processed = 897024\n",
      "Epoch 1, Step 218: Tokens Processed = 897024\n",
      "Epoch 1, Step 219: Tokens Processed = 901120\n",
      "Epoch 1, Step 219: Tokens Processed = 901120\n",
      "Epoch 1, Step 220: Tokens Processed = 905216\n",
      "Epoch 1, Step 220: Tokens Processed = 905216\n",
      "Epoch 1, Step 221: Tokens Processed = 909312\n",
      "Epoch 1, Step 221: Tokens Processed = 909312\n",
      "Epoch 1, Step 222: Tokens Processed = 913408\n",
      "Epoch 1, Step 222: Tokens Processed = 913408\n",
      "Epoch 1, Step 223: Tokens Processed = 917504\n",
      "Epoch 1, Step 223: Tokens Processed = 917504\n",
      "Epoch 1, Step 224: Tokens Processed = 921600\n",
      "Epoch 1, Step 224: Tokens Processed = 921600\n",
      "Epoch 1, Step 225: Tokens Processed = 925696\n",
      "Epoch 1, Step 225: Tokens Processed = 925696\n",
      "Epoch 1, Step 226: Tokens Processed = 929792\n",
      "Epoch 1, Step 226: Tokens Processed = 929792\n",
      "Epoch 1, Step 227: Tokens Processed = 933888\n",
      "Epoch 1, Step 227: Tokens Processed = 933888\n",
      "Epoch 1, Step 228: Tokens Processed = 937984\n",
      "Epoch 1, Step 228: Tokens Processed = 937984\n",
      "Epoch 1, Step 229: Tokens Processed = 942080\n",
      "Epoch 1, Step 229: Tokens Processed = 942080\n",
      "Epoch 1, Step 230: Tokens Processed = 946176\n",
      "Epoch 1, Step 230: Tokens Processed = 946176\n",
      "Epoch 1, Step 231: Tokens Processed = 950272\n",
      "Epoch 1, Step 231: Tokens Processed = 950272\n",
      "Epoch 1, Step 232: Tokens Processed = 954368\n",
      "Epoch 1, Step 232: Tokens Processed = 954368\n",
      "Epoch 1, Step 233: Tokens Processed = 958464\n",
      "Epoch 1, Step 233: Tokens Processed = 958464\n",
      "Epoch 1, Step 234: Tokens Processed = 962560\n",
      "Epoch 1, Step 234: Tokens Processed = 962560\n",
      "Epoch 1, Step 235: Tokens Processed = 966656\n",
      "Epoch 1, Step 235: Tokens Processed = 966656\n",
      "Epoch 1, Step 236: Tokens Processed = 970752\n",
      "Epoch 1, Step 236: Tokens Processed = 970752\n",
      "Epoch 1, Step 237: Tokens Processed = 974848\n",
      "Epoch 1, Step 237: Tokens Processed = 974848\n",
      "Epoch 1, Step 238: Tokens Processed = 978944\n",
      "Epoch 1, Step 238: Tokens Processed = 978944\n",
      "Epoch 1, Step 239: Tokens Processed = 983040\n",
      "Epoch 1, Step 239: Tokens Processed = 983040\n",
      "Epoch 1, Step 240: Tokens Processed = 987136\n",
      "Epoch 1, Step 240: Tokens Processed = 987136\n",
      "Epoch 1, Step 241: Tokens Processed = 991232\n",
      "Epoch 1, Step 241: Tokens Processed = 991232\n",
      "Epoch 1, Step 242: Tokens Processed = 995328\n",
      "Epoch 1, Step 242: Tokens Processed = 995328\n",
      "Epoch 1, Step 243: Tokens Processed = 999424\n",
      "Epoch 1, Step 243: Tokens Processed = 999424\n",
      "Epoch 1, Step 244: Tokens Processed = 1003520\n",
      "Epoch 1, Step 244: Tokens Processed = 1003520\n",
      "Epoch 1, Step 245: Tokens Processed = 1007616\n",
      "Epoch 1, Step 245: Tokens Processed = 1007616\n",
      "Epoch 1, Step 246: Tokens Processed = 1011712\n",
      "Epoch 1, Step 246: Tokens Processed = 1011712\n",
      "Epoch 1, Step 247: Tokens Processed = 1015808\n",
      "Epoch 1, Step 247: Tokens Processed = 1015808\n",
      "Epoch 1, Step 248: Tokens Processed = 1019904\n",
      "Epoch 1, Step 248: Tokens Processed = 1019904\n",
      "Epoch 1, Step 249: Tokens Processed = 1024000\n",
      "Epoch 1, Step 249: Tokens Processed = 1024000\n",
      "Epoch 1, Step 250: Tokens Processed = 1028096\n",
      "Epoch 1, Step 250: Tokens Processed = 1028096\n",
      "Epoch 1, Step 251: Tokens Processed = 1032192\n",
      "Epoch 1, Step 251: Tokens Processed = 1032192\n",
      "Epoch 1, Step 252: Tokens Processed = 1036288\n",
      "Epoch 1, Step 252: Tokens Processed = 1036288\n",
      "Epoch 1, Step 253: Tokens Processed = 1040384\n",
      "Epoch 1, Step 253: Tokens Processed = 1040384\n",
      "Epoch 1, Step 254: Tokens Processed = 1044480\n",
      "Epoch 1, Step 254: Tokens Processed = 1044480\n",
      "Epoch 1, Step 255: Tokens Processed = 1048576\n",
      "Epoch 1, Step 255: Tokens Processed = 1048576\n",
      "Epoch 1, Step 256: Tokens Processed = 1052672\n",
      "Epoch 1, Step 256: Tokens Processed = 1052672\n",
      "Epoch 1, Step 257: Tokens Processed = 1056768\n",
      "Epoch 1, Step 257: Tokens Processed = 1056768\n",
      "Epoch 1, Step 258: Tokens Processed = 1060864\n",
      "Epoch 1, Step 258: Tokens Processed = 1060864\n",
      "Epoch 1, Step 259: Tokens Processed = 1064960\n",
      "Epoch 1, Step 259: Tokens Processed = 1064960\n",
      "Epoch 1, Step 260: Tokens Processed = 1069056\n",
      "Epoch 1, Step 260: Tokens Processed = 1069056\n",
      "Epoch 1, Step 261: Tokens Processed = 1073152\n",
      "Epoch 1, Step 261: Tokens Processed = 1073152\n",
      "Epoch 1, Step 262: Tokens Processed = 1077248\n",
      "Epoch 1, Step 262: Tokens Processed = 1077248\n",
      "Epoch 1, Step 263: Tokens Processed = 1081344\n",
      "Epoch 1, Step 263: Tokens Processed = 1081344\n",
      "Epoch 1, Step 264: Tokens Processed = 1085440\n",
      "Epoch 1, Step 264: Tokens Processed = 1085440\n",
      "Epoch 1, Step 265: Tokens Processed = 1089536\n",
      "Epoch 1, Step 265: Tokens Processed = 1089536\n",
      "Epoch 1, Step 266: Tokens Processed = 1093632\n",
      "Epoch 1, Step 266: Tokens Processed = 1093632\n",
      "Epoch 1, Step 267: Tokens Processed = 1097728\n",
      "Epoch 1, Step 267: Tokens Processed = 1097728\n",
      "Epoch 1, Step 268: Tokens Processed = 1101824\n",
      "Epoch 1, Step 268: Tokens Processed = 1101824\n",
      "Epoch 1, Step 269: Tokens Processed = 1105920\n",
      "Epoch 1, Step 269: Tokens Processed = 1105920\n",
      "Epoch 1, Step 270: Tokens Processed = 1110016\n",
      "Epoch 1, Step 270: Tokens Processed = 1110016\n",
      "Epoch 1, Step 271: Tokens Processed = 1114112\n",
      "Epoch 1, Step 271: Tokens Processed = 1114112\n",
      "Epoch 1, Step 272: Tokens Processed = 1118208\n",
      "Epoch 1, Step 272: Tokens Processed = 1118208\n",
      "Epoch 1, Step 273: Tokens Processed = 1122304\n",
      "Epoch 1, Step 273: Tokens Processed = 1122304\n",
      "Epoch 1, Step 274: Tokens Processed = 1126400\n",
      "Epoch 1, Step 274: Tokens Processed = 1126400\n",
      "Epoch 1, Step 275: Tokens Processed = 1130496\n",
      "Epoch 1, Step 275: Tokens Processed = 1130496\n",
      "Epoch 1, Step 276: Tokens Processed = 1134592\n",
      "Epoch 1, Step 276: Tokens Processed = 1134592\n",
      "Epoch 1, Step 277: Tokens Processed = 1138688\n",
      "Epoch 1, Step 277: Tokens Processed = 1138688\n",
      "Epoch 1, Step 278: Tokens Processed = 1142784\n",
      "Epoch 1, Step 278: Tokens Processed = 1142784\n",
      "Epoch 1, Step 279: Tokens Processed = 1146880\n",
      "Epoch 1, Step 279: Tokens Processed = 1146880\n",
      "Epoch 1, Step 280: Tokens Processed = 1150976\n",
      "Epoch 1, Step 280: Tokens Processed = 1150976\n",
      "Epoch 1, Step 281: Tokens Processed = 1155072\n",
      "Epoch 1, Step 281: Tokens Processed = 1155072\n",
      "Epoch 1, Step 282: Tokens Processed = 1159168\n",
      "Epoch 1, Step 282: Tokens Processed = 1159168\n",
      "Epoch 1, Step 283: Tokens Processed = 1163264\n",
      "Epoch 1, Step 283: Tokens Processed = 1163264\n",
      "Epoch 1, Step 284: Tokens Processed = 1167360\n",
      "Epoch 1, Step 284: Tokens Processed = 1167360\n",
      "Epoch 1, Step 285: Tokens Processed = 1171456\n",
      "Epoch 1, Step 285: Tokens Processed = 1171456\n",
      "Epoch 1, Step 286: Tokens Processed = 1175552\n",
      "Epoch 1, Step 286: Tokens Processed = 1175552\n",
      "Epoch 1, Step 287: Tokens Processed = 1179648\n",
      "Epoch 1, Step 287: Tokens Processed = 1179648\n",
      "Epoch 1, Step 288: Tokens Processed = 1183744\n",
      "Epoch 1, Step 288: Tokens Processed = 1183744\n",
      "Epoch 1, Step 289: Tokens Processed = 1187840\n",
      "Epoch 1, Step 289: Tokens Processed = 1187840\n",
      "Epoch 1, Step 290: Tokens Processed = 1191936\n",
      "Epoch 1, Step 290: Tokens Processed = 1191936\n",
      "Epoch 1, Step 291: Tokens Processed = 1196032\n",
      "Epoch 1, Step 291: Tokens Processed = 1196032\n",
      "Epoch 1, Step 292: Tokens Processed = 1200128\n",
      "Epoch 1, Step 292: Tokens Processed = 1200128\n",
      "Epoch 1, Step 293: Tokens Processed = 1204224\n",
      "Epoch 1, Step 293: Tokens Processed = 1204224\n",
      "Epoch 1, Step 294: Tokens Processed = 1208320\n",
      "Epoch 1, Step 294: Tokens Processed = 1208320\n",
      "Epoch 1, Step 295: Tokens Processed = 1212416\n",
      "Epoch 1, Step 295: Tokens Processed = 1212416\n",
      "Epoch 1, Step 296: Tokens Processed = 1216512\n",
      "Epoch 1, Step 296: Tokens Processed = 1216512\n",
      "Epoch 1, Step 297: Tokens Processed = 1220608\n",
      "Epoch 1, Step 297: Tokens Processed = 1220608\n",
      "Epoch 1, Step 298: Tokens Processed = 1224704\n",
      "Epoch 1, Step 298: Tokens Processed = 1224704\n",
      "Epoch 1, Step 299: Tokens Processed = 1228800\n",
      "Epoch 1, Step 299: Tokens Processed = 1228800\n",
      "Epoch 1, Step 300: Tokens Processed = 1232896\n",
      "Epoch 1, Step 300: Tokens Processed = 1232896\n",
      "Epoch 1, Step 300: Training Loss = 2.41936457157135, Validation Loss = 4.70005202293396, Tokens Processed = 1232896\n",
      "Epoch 1, Step 300: Training Loss = 2.41936457157135, Validation Loss = 4.70005202293396, Tokens Processed = 1232896\n",
      "Epoch 1, Step 301: Tokens Processed = 1236992\n",
      "Epoch 1, Step 301: Tokens Processed = 1236992\n",
      "Epoch 1, Step 302: Tokens Processed = 1241088\n",
      "Epoch 1, Step 302: Tokens Processed = 1241088\n",
      "Epoch 1, Step 303: Tokens Processed = 1245184\n",
      "Epoch 1, Step 303: Tokens Processed = 1245184\n",
      "Epoch 1, Step 304: Tokens Processed = 1249280\n",
      "Epoch 1, Step 304: Tokens Processed = 1249280\n",
      "Epoch 1, Step 305: Tokens Processed = 1253376\n",
      "Epoch 1, Step 305: Tokens Processed = 1253376\n",
      "Epoch 1, Step 306: Tokens Processed = 1257472\n",
      "Epoch 1, Step 306: Tokens Processed = 1257472\n",
      "Epoch 1, Step 307: Tokens Processed = 1261568\n",
      "Epoch 1, Step 307: Tokens Processed = 1261568\n",
      "Epoch 1, Step 308: Tokens Processed = 1265664\n",
      "Epoch 1, Step 308: Tokens Processed = 1265664\n",
      "Epoch 1, Step 309: Tokens Processed = 1269760\n",
      "Epoch 1, Step 309: Tokens Processed = 1269760\n",
      "Epoch 1, Step 310: Tokens Processed = 1273856\n",
      "Epoch 1, Step 310: Tokens Processed = 1273856\n",
      "Epoch 1, Step 311: Tokens Processed = 1277952\n",
      "Epoch 1, Step 311: Tokens Processed = 1277952\n",
      "Epoch 1, Step 312: Tokens Processed = 1282048\n",
      "Epoch 1, Step 312: Tokens Processed = 1282048\n",
      "Epoch 1, Step 313: Tokens Processed = 1286144\n",
      "Epoch 1, Step 313: Tokens Processed = 1286144\n",
      "Epoch 1, Step 314: Tokens Processed = 1290240\n",
      "Epoch 1, Step 314: Tokens Processed = 1290240\n",
      "Epoch 1, Step 315: Tokens Processed = 1294336\n",
      "Epoch 1, Step 315: Tokens Processed = 1294336\n",
      "Epoch 1, Step 316: Tokens Processed = 1298432\n",
      "Epoch 1, Step 316: Tokens Processed = 1298432\n",
      "Epoch 1, Step 317: Tokens Processed = 1302528\n",
      "Epoch 1, Step 317: Tokens Processed = 1302528\n",
      "Epoch 1, Step 318: Tokens Processed = 1306624\n",
      "Epoch 1, Step 318: Tokens Processed = 1306624\n",
      "Epoch 1, Step 319: Tokens Processed = 1310720\n",
      "Epoch 1, Step 319: Tokens Processed = 1310720\n",
      "Epoch 1, Step 320: Tokens Processed = 1314816\n",
      "Epoch 1, Step 320: Tokens Processed = 1314816\n",
      "Epoch 1, Step 321: Tokens Processed = 1318912\n",
      "Epoch 1, Step 321: Tokens Processed = 1318912\n",
      "Epoch 1, Step 322: Tokens Processed = 1323008\n",
      "Epoch 1, Step 322: Tokens Processed = 1323008\n",
      "Epoch 1, Step 323: Tokens Processed = 1327104\n",
      "Epoch 1, Step 323: Tokens Processed = 1327104\n",
      "Epoch 1, Step 324: Tokens Processed = 1331200\n",
      "Epoch 1, Step 324: Tokens Processed = 1331200\n",
      "Epoch 1, Step 325: Tokens Processed = 1335296\n",
      "Epoch 1, Step 325: Tokens Processed = 1335296\n",
      "Epoch 1, Step 326: Tokens Processed = 1339392\n",
      "Epoch 1, Step 326: Tokens Processed = 1339392\n",
      "Epoch 1, Step 327: Tokens Processed = 1343488\n",
      "Epoch 1, Step 327: Tokens Processed = 1343488\n",
      "Epoch 1, Step 328: Tokens Processed = 1347584\n",
      "Epoch 1, Step 328: Tokens Processed = 1347584\n",
      "Epoch 1, Step 329: Tokens Processed = 1351680\n",
      "Epoch 1, Step 329: Tokens Processed = 1351680\n",
      "Epoch 1, Step 330: Tokens Processed = 1355776\n",
      "Epoch 1, Step 330: Tokens Processed = 1355776\n",
      "Epoch 1, Step 331: Tokens Processed = 1359872\n",
      "Epoch 1, Step 331: Tokens Processed = 1359872\n",
      "Epoch 1, Step 332: Tokens Processed = 1363968\n",
      "Epoch 1, Step 332: Tokens Processed = 1363968\n",
      "Epoch 1, Step 333: Tokens Processed = 1368064\n",
      "Epoch 1, Step 333: Tokens Processed = 1368064\n",
      "Epoch 1, Step 334: Tokens Processed = 1372160\n",
      "Epoch 1, Step 334: Tokens Processed = 1372160\n",
      "Epoch 1, Step 335: Tokens Processed = 1376256\n",
      "Epoch 1, Step 335: Tokens Processed = 1376256\n",
      "Epoch 1, Step 336: Tokens Processed = 1380352\n",
      "Epoch 1, Step 336: Tokens Processed = 1380352\n",
      "Epoch 1, Step 337: Tokens Processed = 1384448\n",
      "Epoch 1, Step 337: Tokens Processed = 1384448\n",
      "Epoch 1, Step 338: Tokens Processed = 1388544\n",
      "Epoch 1, Step 338: Tokens Processed = 1388544\n",
      "Epoch 1, Step 339: Tokens Processed = 1392640\n",
      "Epoch 1, Step 339: Tokens Processed = 1392640\n",
      "Epoch 1, Step 340: Tokens Processed = 1396736\n",
      "Epoch 1, Step 340: Tokens Processed = 1396736\n",
      "Epoch 1, Step 341: Tokens Processed = 1400832\n",
      "Epoch 1, Step 341: Tokens Processed = 1400832\n",
      "Epoch 1, Step 342: Tokens Processed = 1404928\n",
      "Epoch 1, Step 342: Tokens Processed = 1404928\n",
      "Epoch 1, Step 343: Tokens Processed = 1409024\n",
      "Epoch 1, Step 343: Tokens Processed = 1409024\n",
      "Epoch 1, Step 344: Tokens Processed = 1413120\n",
      "Epoch 1, Step 344: Tokens Processed = 1413120\n",
      "Epoch 1, Step 345: Tokens Processed = 1417216\n",
      "Epoch 1, Step 345: Tokens Processed = 1417216\n",
      "Epoch 1, Step 346: Tokens Processed = 1421312\n",
      "Epoch 1, Step 346: Tokens Processed = 1421312\n",
      "Epoch 1, Step 347: Tokens Processed = 1425408\n",
      "Epoch 1, Step 347: Tokens Processed = 1425408\n",
      "Epoch 1, Step 348: Tokens Processed = 1429504\n",
      "Epoch 1, Step 348: Tokens Processed = 1429504\n",
      "Epoch 1, Step 349: Tokens Processed = 1433600\n",
      "Epoch 1, Step 349: Tokens Processed = 1433600\n",
      "Epoch 1, Step 350: Tokens Processed = 1437696\n",
      "Epoch 1, Step 350: Tokens Processed = 1437696\n",
      "Epoch 1, Step 351: Tokens Processed = 1441792\n",
      "Epoch 1, Step 351: Tokens Processed = 1441792\n",
      "Epoch 1, Step 352: Tokens Processed = 1445888\n",
      "Epoch 1, Step 352: Tokens Processed = 1445888\n",
      "Epoch 1, Step 353: Tokens Processed = 1449984\n",
      "Epoch 1, Step 353: Tokens Processed = 1449984\n",
      "Epoch 1, Step 354: Tokens Processed = 1454080\n",
      "Epoch 1, Step 354: Tokens Processed = 1454080\n",
      "Epoch 1, Step 355: Tokens Processed = 1458176\n",
      "Epoch 1, Step 355: Tokens Processed = 1458176\n",
      "Epoch 1, Step 356: Tokens Processed = 1462272\n",
      "Epoch 1, Step 356: Tokens Processed = 1462272\n",
      "Epoch 1, Step 357: Tokens Processed = 1466368\n",
      "Epoch 1, Step 357: Tokens Processed = 1466368\n",
      "Epoch 1, Step 358: Tokens Processed = 1470464\n",
      "Epoch 1, Step 358: Tokens Processed = 1470464\n",
      "Epoch 1, Step 359: Tokens Processed = 1474560\n",
      "Epoch 1, Step 359: Tokens Processed = 1474560\n",
      "Epoch 1, Step 360: Tokens Processed = 1478656\n",
      "Epoch 1, Step 360: Tokens Processed = 1478656\n",
      "Epoch 1, Step 361: Tokens Processed = 1482752\n",
      "Epoch 1, Step 361: Tokens Processed = 1482752\n",
      "Epoch 1, Step 362: Tokens Processed = 1486848\n",
      "Epoch 1, Step 362: Tokens Processed = 1486848\n",
      "Epoch 1, Step 363: Tokens Processed = 1490944\n",
      "Epoch 1, Step 363: Tokens Processed = 1490944\n",
      "Epoch 1, Step 364: Tokens Processed = 1495040\n",
      "Epoch 1, Step 364: Tokens Processed = 1495040\n",
      "Epoch 1, Step 365: Tokens Processed = 1499136\n",
      "Epoch 1, Step 365: Tokens Processed = 1499136\n",
      "Epoch 1, Step 366: Tokens Processed = 1503232\n",
      "Epoch 1, Step 366: Tokens Processed = 1503232\n",
      "Epoch 1, Step 367: Tokens Processed = 1507328\n",
      "Epoch 1, Step 367: Tokens Processed = 1507328\n",
      "Epoch 1, Step 368: Tokens Processed = 1511424\n",
      "Epoch 1, Step 368: Tokens Processed = 1511424\n",
      "Epoch 1, Step 369: Tokens Processed = 1515520\n",
      "Epoch 1, Step 369: Tokens Processed = 1515520\n",
      "Epoch 1, Step 370: Tokens Processed = 1519616\n",
      "Epoch 1, Step 370: Tokens Processed = 1519616\n",
      "Epoch 1, Step 371: Tokens Processed = 1523712\n",
      "Epoch 1, Step 371: Tokens Processed = 1523712\n",
      "Epoch 1, Step 372: Tokens Processed = 1527808\n",
      "Epoch 1, Step 372: Tokens Processed = 1527808\n",
      "Epoch 1, Step 373: Tokens Processed = 1531904\n",
      "Epoch 1, Step 373: Tokens Processed = 1531904\n",
      "Epoch 1, Step 374: Tokens Processed = 1536000\n",
      "Epoch 1, Step 374: Tokens Processed = 1536000\n",
      "Epoch 1, Step 375: Tokens Processed = 1540096\n",
      "Epoch 1, Step 375: Tokens Processed = 1540096\n",
      "Epoch 1, Step 376: Tokens Processed = 1544192\n",
      "Epoch 1, Step 376: Tokens Processed = 1544192\n",
      "Epoch 1, Step 377: Tokens Processed = 1548288\n",
      "Epoch 1, Step 377: Tokens Processed = 1548288\n",
      "Epoch 1, Step 378: Tokens Processed = 1552384\n",
      "Epoch 1, Step 378: Tokens Processed = 1552384\n",
      "Epoch 1, Step 379: Tokens Processed = 1556480\n",
      "Epoch 1, Step 379: Tokens Processed = 1556480\n",
      "Epoch 1, Step 380: Tokens Processed = 1560576\n",
      "Epoch 1, Step 380: Tokens Processed = 1560576\n",
      "Epoch 1, Step 381: Tokens Processed = 1564672\n",
      "Epoch 1, Step 381: Tokens Processed = 1564672\n",
      "Epoch 1, Step 382: Tokens Processed = 1568768\n",
      "Epoch 1, Step 382: Tokens Processed = 1568768\n",
      "Epoch 1, Step 383: Tokens Processed = 1572864\n",
      "Epoch 1, Step 383: Tokens Processed = 1572864\n",
      "Epoch 1, Step 384: Tokens Processed = 1576960\n",
      "Epoch 1, Step 384: Tokens Processed = 1576960\n",
      "Epoch 1, Step 385: Tokens Processed = 1581056\n",
      "Epoch 1, Step 385: Tokens Processed = 1581056\n",
      "Epoch 1, Step 386: Tokens Processed = 1585152\n",
      "Epoch 1, Step 386: Tokens Processed = 1585152\n",
      "Epoch 1, Step 387: Tokens Processed = 1589248\n",
      "Epoch 1, Step 387: Tokens Processed = 1589248\n",
      "Epoch 1, Step 388: Tokens Processed = 1593344\n",
      "Epoch 1, Step 388: Tokens Processed = 1593344\n",
      "Epoch 1, Step 389: Tokens Processed = 1597440\n",
      "Epoch 1, Step 389: Tokens Processed = 1597440\n",
      "Epoch 1, Step 390: Tokens Processed = 1601536\n",
      "Epoch 1, Step 390: Tokens Processed = 1601536\n",
      "Epoch 1, Step 391: Tokens Processed = 1605632\n",
      "Epoch 1, Step 391: Tokens Processed = 1605632\n",
      "Epoch 1, Step 392: Tokens Processed = 1609728\n",
      "Epoch 1, Step 392: Tokens Processed = 1609728\n",
      "Epoch 1, Step 393: Tokens Processed = 1613824\n",
      "Epoch 1, Step 393: Tokens Processed = 1613824\n",
      "Epoch 1, Step 394: Tokens Processed = 1617920\n",
      "Epoch 1, Step 394: Tokens Processed = 1617920\n",
      "Epoch 1, Step 395: Tokens Processed = 1622016\n",
      "Epoch 1, Step 395: Tokens Processed = 1622016\n",
      "Epoch 1, Step 396: Tokens Processed = 1626112\n",
      "Epoch 1, Step 396: Tokens Processed = 1626112\n",
      "Epoch 1, Step 397: Tokens Processed = 1630208\n",
      "Epoch 1, Step 397: Tokens Processed = 1630208\n",
      "Epoch 1, Step 398: Tokens Processed = 1634304\n",
      "Epoch 1, Step 398: Tokens Processed = 1634304\n",
      "Epoch 1, Step 399: Tokens Processed = 1638400\n",
      "Epoch 1, Step 399: Tokens Processed = 1638400\n",
      "Epoch 1, Step 400: Tokens Processed = 1642496\n",
      "Epoch 1, Step 400: Tokens Processed = 1642496\n",
      "Epoch 1, Step 400: Training Loss = 2.35014009475708, Validation Loss = 5.594507932662964, Tokens Processed = 1642496\n",
      "Epoch 1, Step 400: Training Loss = 2.35014009475708, Validation Loss = 5.594507932662964, Tokens Processed = 1642496\n",
      "Epoch 1, Step 401: Tokens Processed = 1646592\n",
      "Epoch 1, Step 401: Tokens Processed = 1646592\n",
      "Epoch 1, Step 402: Tokens Processed = 1650688\n",
      "Epoch 1, Step 402: Tokens Processed = 1650688\n",
      "Epoch 1, Step 403: Tokens Processed = 1654784\n",
      "Epoch 1, Step 403: Tokens Processed = 1654784\n",
      "Epoch 1, Step 404: Tokens Processed = 1658880\n",
      "Epoch 1, Step 404: Tokens Processed = 1658880\n",
      "Epoch 1, Step 405: Tokens Processed = 1662976\n",
      "Epoch 1, Step 405: Tokens Processed = 1662976\n",
      "Epoch 1, Step 406: Tokens Processed = 1667072\n",
      "Epoch 1, Step 406: Tokens Processed = 1667072\n",
      "Epoch 1, Step 407: Tokens Processed = 1671168\n",
      "Epoch 1, Step 407: Tokens Processed = 1671168\n",
      "Epoch 1, Step 408: Tokens Processed = 1675264\n",
      "Epoch 1, Step 408: Tokens Processed = 1675264\n",
      "Epoch 1, Step 409: Tokens Processed = 1679360\n",
      "Epoch 1, Step 409: Tokens Processed = 1679360\n",
      "Epoch 1, Step 410: Tokens Processed = 1683456\n",
      "Epoch 1, Step 410: Tokens Processed = 1683456\n",
      "Epoch 1, Step 411: Tokens Processed = 1687552\n",
      "Epoch 1, Step 411: Tokens Processed = 1687552\n",
      "Epoch 1, Step 412: Tokens Processed = 1691648\n",
      "Epoch 1, Step 412: Tokens Processed = 1691648\n",
      "Epoch 1, Step 413: Tokens Processed = 1695744\n",
      "Epoch 1, Step 413: Tokens Processed = 1695744\n",
      "Epoch 1, Step 414: Tokens Processed = 1699840\n",
      "Epoch 1, Step 414: Tokens Processed = 1699840\n",
      "Epoch 1, Step 415: Tokens Processed = 1703936\n",
      "Epoch 1, Step 415: Tokens Processed = 1703936\n",
      "Epoch 1, Step 416: Tokens Processed = 1708032\n",
      "Epoch 1, Step 416: Tokens Processed = 1708032\n",
      "Epoch 1, Step 417: Tokens Processed = 1712128\n",
      "Epoch 1, Step 417: Tokens Processed = 1712128\n",
      "Epoch 1, Step 418: Tokens Processed = 1716224\n",
      "Epoch 1, Step 418: Tokens Processed = 1716224\n",
      "Epoch 1, Step 419: Tokens Processed = 1720320\n",
      "Epoch 1, Step 419: Tokens Processed = 1720320\n",
      "Epoch 1, Step 420: Tokens Processed = 1724416\n",
      "Epoch 1, Step 420: Tokens Processed = 1724416\n",
      "Epoch 1, Step 421: Tokens Processed = 1728512\n",
      "Epoch 1, Step 421: Tokens Processed = 1728512\n",
      "Epoch 1, Step 422: Tokens Processed = 1732608\n",
      "Epoch 1, Step 422: Tokens Processed = 1732608\n",
      "Epoch 1, Step 423: Tokens Processed = 1736704\n",
      "Epoch 1, Step 423: Tokens Processed = 1736704\n",
      "Epoch 1, Step 424: Tokens Processed = 1740800\n",
      "Epoch 1, Step 424: Tokens Processed = 1740800\n",
      "Epoch 1, Step 425: Tokens Processed = 1744896\n",
      "Epoch 1, Step 425: Tokens Processed = 1744896\n",
      "Epoch 1, Step 426: Tokens Processed = 1748992\n",
      "Epoch 1, Step 426: Tokens Processed = 1748992\n",
      "Epoch 1, Step 427: Tokens Processed = 1753088\n",
      "Epoch 1, Step 427: Tokens Processed = 1753088\n",
      "Epoch 1, Step 428: Tokens Processed = 1757184\n",
      "Epoch 1, Step 428: Tokens Processed = 1757184\n",
      "Epoch 1, Step 429: Tokens Processed = 1761280\n",
      "Epoch 1, Step 429: Tokens Processed = 1761280\n",
      "Epoch 1, Step 430: Tokens Processed = 1765376\n",
      "Epoch 1, Step 430: Tokens Processed = 1765376\n",
      "Epoch 1, Step 431: Tokens Processed = 1769472\n",
      "Epoch 1, Step 431: Tokens Processed = 1769472\n",
      "Epoch 1, Step 432: Tokens Processed = 1773568\n",
      "Epoch 1, Step 432: Tokens Processed = 1773568\n",
      "Epoch 1, Step 433: Tokens Processed = 1777664\n",
      "Epoch 1, Step 433: Tokens Processed = 1777664\n",
      "Epoch 1, Step 434: Tokens Processed = 1781760\n",
      "Epoch 1, Step 434: Tokens Processed = 1781760\n",
      "Epoch 1, Step 435: Tokens Processed = 1785856\n",
      "Epoch 1, Step 435: Tokens Processed = 1785856\n",
      "Epoch 1, Step 436: Tokens Processed = 1789952\n",
      "Epoch 1, Step 436: Tokens Processed = 1789952\n",
      "Epoch 1, Step 437: Tokens Processed = 1794048\n",
      "Epoch 1, Step 437: Tokens Processed = 1794048\n",
      "Epoch 1, Step 438: Tokens Processed = 1798144\n",
      "Epoch 1, Step 438: Tokens Processed = 1798144\n",
      "Epoch 1, Step 439: Tokens Processed = 1802240\n",
      "Epoch 1, Step 439: Tokens Processed = 1802240\n",
      "Epoch 1, Step 440: Tokens Processed = 1806336\n",
      "Epoch 1, Step 440: Tokens Processed = 1806336\n",
      "Epoch 1, Step 441: Tokens Processed = 1810432\n",
      "Epoch 1, Step 441: Tokens Processed = 1810432\n",
      "Epoch 1, Step 442: Tokens Processed = 1814528\n",
      "Epoch 1, Step 442: Tokens Processed = 1814528\n",
      "Epoch 1, Step 443: Tokens Processed = 1818624\n",
      "Epoch 1, Step 443: Tokens Processed = 1818624\n",
      "Epoch 1, Step 444: Tokens Processed = 1822720\n",
      "Epoch 1, Step 444: Tokens Processed = 1822720\n",
      "Epoch 1, Step 445: Tokens Processed = 1826816\n",
      "Epoch 1, Step 445: Tokens Processed = 1826816\n",
      "Epoch 1, Step 446: Tokens Processed = 1830912\n",
      "Epoch 1, Step 446: Tokens Processed = 1830912\n",
      "Epoch 1, Step 447: Tokens Processed = 1835008\n",
      "Epoch 1, Step 447: Tokens Processed = 1835008\n",
      "Epoch 1, Step 448: Tokens Processed = 1839104\n",
      "Epoch 1, Step 448: Tokens Processed = 1839104\n",
      "Epoch 1, Step 449: Tokens Processed = 1843200\n",
      "Epoch 1, Step 449: Tokens Processed = 1843200\n",
      "Epoch 1, Step 450: Tokens Processed = 1847296\n",
      "Epoch 1, Step 450: Tokens Processed = 1847296\n",
      "Epoch 1, Step 451: Tokens Processed = 1851392\n",
      "Epoch 1, Step 451: Tokens Processed = 1851392\n",
      "Epoch 1, Step 452: Tokens Processed = 1855488\n",
      "Epoch 1, Step 452: Tokens Processed = 1855488\n",
      "Epoch 1, Step 453: Tokens Processed = 1859584\n",
      "Epoch 1, Step 453: Tokens Processed = 1859584\n",
      "Epoch 1, Step 454: Tokens Processed = 1863680\n",
      "Epoch 1, Step 454: Tokens Processed = 1863680\n",
      "Epoch 1, Step 455: Tokens Processed = 1867776\n",
      "Epoch 1, Step 455: Tokens Processed = 1867776\n",
      "Epoch 1, Step 456: Tokens Processed = 1871872\n",
      "Epoch 1, Step 456: Tokens Processed = 1871872\n",
      "Epoch 1, Step 457: Tokens Processed = 1875968\n",
      "Epoch 1, Step 457: Tokens Processed = 1875968\n",
      "Epoch 1, Step 458: Tokens Processed = 1880064\n",
      "Epoch 1, Step 458: Tokens Processed = 1880064\n",
      "Epoch 1, Step 459: Tokens Processed = 1884160\n",
      "Epoch 1, Step 459: Tokens Processed = 1884160\n",
      "Epoch 1, Step 460: Tokens Processed = 1888256\n",
      "Epoch 1, Step 460: Tokens Processed = 1888256\n",
      "Epoch 1, Step 461: Tokens Processed = 1892352\n",
      "Epoch 1, Step 461: Tokens Processed = 1892352\n",
      "Epoch 1, Step 462: Tokens Processed = 1896448\n",
      "Epoch 1, Step 462: Tokens Processed = 1896448\n",
      "Epoch 1, Step 463: Tokens Processed = 1900544\n",
      "Epoch 1, Step 463: Tokens Processed = 1900544\n",
      "Epoch 1, Step 464: Tokens Processed = 1904640\n",
      "Epoch 1, Step 464: Tokens Processed = 1904640\n",
      "Epoch 1, Step 465: Tokens Processed = 1908736\n",
      "Epoch 1, Step 465: Tokens Processed = 1908736\n",
      "Epoch 1, Step 466: Tokens Processed = 1912832\n",
      "Epoch 1, Step 466: Tokens Processed = 1912832\n",
      "Epoch 1, Step 467: Tokens Processed = 1916928\n",
      "Epoch 1, Step 467: Tokens Processed = 1916928\n",
      "Epoch 1, Step 468: Tokens Processed = 1921024\n",
      "Epoch 1, Step 468: Tokens Processed = 1921024\n",
      "Epoch 1, Step 469: Tokens Processed = 1925120\n",
      "Epoch 1, Step 469: Tokens Processed = 1925120\n",
      "Epoch 1, Step 470: Tokens Processed = 1929216\n",
      "Epoch 1, Step 470: Tokens Processed = 1929216\n",
      "Epoch 1, Step 471: Tokens Processed = 1933312\n",
      "Epoch 1, Step 471: Tokens Processed = 1933312\n",
      "Epoch 1, Step 472: Tokens Processed = 1937408\n",
      "Epoch 1, Step 472: Tokens Processed = 1937408\n",
      "Epoch 1, Step 473: Tokens Processed = 1941504\n",
      "Epoch 1, Step 473: Tokens Processed = 1941504\n",
      "Epoch 1, Step 474: Tokens Processed = 1945600\n",
      "Epoch 1, Step 474: Tokens Processed = 1945600\n",
      "Epoch 1, Step 475: Tokens Processed = 1949696\n",
      "Epoch 1, Step 475: Tokens Processed = 1949696\n",
      "Epoch 1, Step 476: Tokens Processed = 1953792\n",
      "Epoch 1, Step 476: Tokens Processed = 1953792\n",
      "Epoch 1, Step 477: Tokens Processed = 1957888\n",
      "Epoch 1, Step 477: Tokens Processed = 1957888\n",
      "Epoch 1, Step 478: Tokens Processed = 1961984\n",
      "Epoch 1, Step 478: Tokens Processed = 1961984\n",
      "Epoch 1, Step 479: Tokens Processed = 1966080\n",
      "Epoch 1, Step 479: Tokens Processed = 1966080\n",
      "Epoch 1, Step 480: Tokens Processed = 1970176\n",
      "Epoch 1, Step 480: Tokens Processed = 1970176\n",
      "Epoch 1, Step 481: Tokens Processed = 1974272\n",
      "Epoch 1, Step 481: Tokens Processed = 1974272\n",
      "Epoch 1, Step 482: Tokens Processed = 1978368\n",
      "Epoch 1, Step 482: Tokens Processed = 1978368\n",
      "Epoch 1, Step 483: Tokens Processed = 1982464\n",
      "Epoch 1, Step 483: Tokens Processed = 1982464\n",
      "Epoch 1, Step 484: Tokens Processed = 1986560\n",
      "Epoch 1, Step 484: Tokens Processed = 1986560\n",
      "Epoch 1, Step 485: Tokens Processed = 1990656\n",
      "Epoch 1, Step 485: Tokens Processed = 1990656\n",
      "Epoch 1, Step 486: Tokens Processed = 1994752\n",
      "Epoch 1, Step 486: Tokens Processed = 1994752\n",
      "Epoch 1, Step 487: Tokens Processed = 1998848\n",
      "Epoch 1, Step 487: Tokens Processed = 1998848\n",
      "Epoch 1, Step 488: Tokens Processed = 2002944\n",
      "Epoch 1, Step 488: Tokens Processed = 2002944\n",
      "Epoch 1, Step 489: Tokens Processed = 2007040\n",
      "Epoch 1, Step 489: Tokens Processed = 2007040\n",
      "Epoch 1, Step 490: Tokens Processed = 2011136\n",
      "Epoch 1, Step 490: Tokens Processed = 2011136\n",
      "Epoch 1, Step 491: Tokens Processed = 2015232\n",
      "Epoch 1, Step 491: Tokens Processed = 2015232\n",
      "Epoch 1, Step 492: Tokens Processed = 2019328\n",
      "Epoch 1, Step 492: Tokens Processed = 2019328\n",
      "Epoch 1, Step 493: Tokens Processed = 2023424\n",
      "Epoch 1, Step 493: Tokens Processed = 2023424\n",
      "Epoch 1, Step 494: Tokens Processed = 2027520\n",
      "Epoch 1, Step 494: Tokens Processed = 2027520\n",
      "Epoch 1, Step 495: Tokens Processed = 2031616\n",
      "Epoch 1, Step 495: Tokens Processed = 2031616\n",
      "Epoch 1, Step 496: Tokens Processed = 2035712\n",
      "Epoch 1, Step 496: Tokens Processed = 2035712\n",
      "Epoch 1, Step 497: Tokens Processed = 2039808\n",
      "Epoch 1, Step 497: Tokens Processed = 2039808\n",
      "Epoch 1, Step 498: Tokens Processed = 2043904\n",
      "Epoch 1, Step 498: Tokens Processed = 2043904\n",
      "Epoch 1, Step 499: Tokens Processed = 2048000\n",
      "Epoch 1, Step 499: Tokens Processed = 2048000\n",
      "Epoch 1, Step 500: Tokens Processed = 2052096\n",
      "Epoch 1, Step 500: Tokens Processed = 2052096\n",
      "Epoch 1, Step 500: Training Loss = 2.3756825923919678, Validation Loss = 4.83152437210083, Tokens Processed = 2052096\n",
      "Epoch 1, Step 500: Training Loss = 2.3756825923919678, Validation Loss = 4.83152437210083, Tokens Processed = 2052096\n",
      "Epoch 1, Step 501: Tokens Processed = 2056192\n",
      "Epoch 1, Step 501: Tokens Processed = 2056192\n",
      "Epoch 1, Step 502: Tokens Processed = 2060288\n",
      "Epoch 1, Step 502: Tokens Processed = 2060288\n",
      "Epoch 1, Step 503: Tokens Processed = 2064384\n",
      "Epoch 1, Step 503: Tokens Processed = 2064384\n",
      "Epoch 1, Step 504: Tokens Processed = 2068480\n",
      "Epoch 1, Step 504: Tokens Processed = 2068480\n",
      "Epoch 1, Step 505: Tokens Processed = 2072576\n",
      "Epoch 1, Step 505: Tokens Processed = 2072576\n",
      "Epoch 1, Step 506: Tokens Processed = 2076672\n",
      "Epoch 1, Step 506: Tokens Processed = 2076672\n",
      "Epoch 1, Step 507: Tokens Processed = 2080768\n",
      "Epoch 1, Step 507: Tokens Processed = 2080768\n",
      "Epoch 1, Step 508: Tokens Processed = 2084864\n",
      "Epoch 1, Step 508: Tokens Processed = 2084864\n",
      "Epoch 1, Step 509: Tokens Processed = 2088960\n",
      "Epoch 1, Step 509: Tokens Processed = 2088960\n",
      "Epoch 1, Step 510: Tokens Processed = 2093056\n",
      "Epoch 1, Step 510: Tokens Processed = 2093056\n",
      "Epoch 1, Step 511: Tokens Processed = 2097152\n",
      "Epoch 1, Step 511: Tokens Processed = 2097152\n",
      "Epoch 1, Step 512: Tokens Processed = 2101248\n",
      "Epoch 1, Step 512: Tokens Processed = 2101248\n",
      "Epoch 1, Step 513: Tokens Processed = 2105344\n",
      "Epoch 1, Step 513: Tokens Processed = 2105344\n",
      "Epoch 1, Step 514: Tokens Processed = 2109440\n",
      "Epoch 1, Step 514: Tokens Processed = 2109440\n",
      "Epoch 1, Step 515: Tokens Processed = 2113536\n",
      "Epoch 1, Step 515: Tokens Processed = 2113536\n",
      "Epoch 1, Step 516: Tokens Processed = 2117632\n",
      "Epoch 1, Step 516: Tokens Processed = 2117632\n",
      "Epoch 1, Step 517: Tokens Processed = 2121728\n",
      "Epoch 1, Step 517: Tokens Processed = 2121728\n",
      "Epoch 1, Step 518: Tokens Processed = 2125824\n",
      "Epoch 1, Step 518: Tokens Processed = 2125824\n",
      "Epoch 1, Step 519: Tokens Processed = 2129920\n",
      "Epoch 1, Step 519: Tokens Processed = 2129920\n",
      "Epoch 1, Step 520: Tokens Processed = 2134016\n",
      "Epoch 1, Step 520: Tokens Processed = 2134016\n",
      "Epoch 1, Step 521: Tokens Processed = 2138112\n",
      "Epoch 1, Step 521: Tokens Processed = 2138112\n",
      "Epoch 1, Step 522: Tokens Processed = 2142208\n",
      "Epoch 1, Step 522: Tokens Processed = 2142208\n",
      "Epoch 1, Step 523: Tokens Processed = 2146304\n",
      "Epoch 1, Step 523: Tokens Processed = 2146304\n",
      "Epoch 1, Step 524: Tokens Processed = 2150400\n",
      "Epoch 1, Step 524: Tokens Processed = 2150400\n",
      "Epoch 1, Step 525: Tokens Processed = 2154496\n",
      "Epoch 1, Step 525: Tokens Processed = 2154496\n",
      "Epoch 1, Step 526: Tokens Processed = 2158592\n",
      "Epoch 1, Step 526: Tokens Processed = 2158592\n",
      "Epoch 1, Step 527: Tokens Processed = 2162688\n",
      "Epoch 1, Step 527: Tokens Processed = 2162688\n",
      "Epoch 1, Step 528: Tokens Processed = 2166784\n",
      "Epoch 1, Step 528: Tokens Processed = 2166784\n",
      "Epoch 1, Step 529: Tokens Processed = 2170880\n",
      "Epoch 1, Step 529: Tokens Processed = 2170880\n",
      "Epoch 1, Step 530: Tokens Processed = 2174976\n",
      "Epoch 1, Step 530: Tokens Processed = 2174976\n",
      "Epoch 1, Step 531: Tokens Processed = 2179072\n",
      "Epoch 1, Step 531: Tokens Processed = 2179072\n",
      "Epoch 1, Step 532: Tokens Processed = 2183168\n",
      "Epoch 1, Step 532: Tokens Processed = 2183168\n",
      "Epoch 1, Step 533: Tokens Processed = 2187264\n",
      "Epoch 1, Step 533: Tokens Processed = 2187264\n",
      "Epoch 1, Step 534: Tokens Processed = 2191360\n",
      "Epoch 1, Step 534: Tokens Processed = 2191360\n",
      "Epoch 1, Step 535: Tokens Processed = 2195456\n",
      "Epoch 1, Step 535: Tokens Processed = 2195456\n",
      "Epoch 1, Step 536: Tokens Processed = 2199552\n",
      "Epoch 1, Step 536: Tokens Processed = 2199552\n",
      "Epoch 1, Step 537: Tokens Processed = 2203648\n",
      "Epoch 1, Step 537: Tokens Processed = 2203648\n",
      "Epoch 1, Step 538: Tokens Processed = 2207744\n",
      "Epoch 1, Step 538: Tokens Processed = 2207744\n",
      "Epoch 1, Step 539: Tokens Processed = 2211840\n",
      "Epoch 1, Step 539: Tokens Processed = 2211840\n",
      "Epoch 1, Step 540: Tokens Processed = 2215936\n",
      "Epoch 1, Step 540: Tokens Processed = 2215936\n",
      "Epoch 1, Step 541: Tokens Processed = 2220032\n",
      "Epoch 1, Step 541: Tokens Processed = 2220032\n",
      "Epoch 1, Step 542: Tokens Processed = 2224128\n",
      "Epoch 1, Step 542: Tokens Processed = 2224128\n",
      "Epoch 1, Step 543: Tokens Processed = 2228224\n",
      "Epoch 1, Step 543: Tokens Processed = 2228224\n",
      "Epoch 1, Step 544: Tokens Processed = 2232320\n",
      "Epoch 1, Step 544: Tokens Processed = 2232320\n",
      "Epoch 1, Step 545: Tokens Processed = 2236416\n",
      "Epoch 1, Step 545: Tokens Processed = 2236416\n",
      "Epoch 1, Step 546: Tokens Processed = 2240512\n",
      "Epoch 1, Step 546: Tokens Processed = 2240512\n",
      "Epoch 1, Step 547: Tokens Processed = 2244608\n",
      "Epoch 1, Step 547: Tokens Processed = 2244608\n",
      "Epoch 1, Step 548: Tokens Processed = 2248704\n",
      "Epoch 1, Step 548: Tokens Processed = 2248704\n",
      "Epoch 1, Step 549: Tokens Processed = 2252800\n",
      "Epoch 1, Step 549: Tokens Processed = 2252800\n",
      "Epoch 1, Step 550: Tokens Processed = 2256896\n",
      "Epoch 1, Step 550: Tokens Processed = 2256896\n",
      "Epoch 1, Step 551: Tokens Processed = 2260992\n",
      "Epoch 1, Step 551: Tokens Processed = 2260992\n",
      "Epoch 1, Step 552: Tokens Processed = 2265088\n",
      "Epoch 1, Step 552: Tokens Processed = 2265088\n",
      "Epoch 1, Step 553: Tokens Processed = 2269184\n",
      "Epoch 1, Step 553: Tokens Processed = 2269184\n",
      "Epoch 1, Step 554: Tokens Processed = 2273280\n",
      "Epoch 1, Step 554: Tokens Processed = 2273280\n",
      "Epoch 1, Step 555: Tokens Processed = 2277376\n",
      "Epoch 1, Step 555: Tokens Processed = 2277376\n",
      "Epoch 1, Step 556: Tokens Processed = 2281472\n",
      "Epoch 1, Step 556: Tokens Processed = 2281472\n",
      "Epoch 1, Step 557: Tokens Processed = 2285568\n",
      "Epoch 1, Step 557: Tokens Processed = 2285568\n",
      "Epoch 1, Step 558: Tokens Processed = 2289664\n",
      "Epoch 1, Step 558: Tokens Processed = 2289664\n",
      "Epoch 1, Step 559: Tokens Processed = 2293760\n",
      "Epoch 1, Step 559: Tokens Processed = 2293760\n",
      "Epoch 1, Step 560: Tokens Processed = 2297856\n",
      "Epoch 1, Step 560: Tokens Processed = 2297856\n",
      "Epoch 1, Step 561: Tokens Processed = 2301952\n",
      "Epoch 1, Step 561: Tokens Processed = 2301952\n",
      "Epoch 1, Step 562: Tokens Processed = 2306048\n",
      "Epoch 1, Step 562: Tokens Processed = 2306048\n",
      "Epoch 1, Step 563: Tokens Processed = 2310144\n",
      "Epoch 1, Step 563: Tokens Processed = 2310144\n",
      "Epoch 1, Step 564: Tokens Processed = 2314240\n",
      "Epoch 1, Step 564: Tokens Processed = 2314240\n",
      "Epoch 1, Step 565: Tokens Processed = 2318336\n",
      "Epoch 1, Step 565: Tokens Processed = 2318336\n",
      "Epoch 1, Step 566: Tokens Processed = 2322432\n",
      "Epoch 1, Step 566: Tokens Processed = 2322432\n",
      "Epoch 1, Step 567: Tokens Processed = 2326528\n",
      "Epoch 1, Step 567: Tokens Processed = 2326528\n",
      "Epoch 1, Step 568: Tokens Processed = 2330624\n",
      "Epoch 1, Step 568: Tokens Processed = 2330624\n",
      "Epoch 1, Step 569: Tokens Processed = 2334720\n",
      "Epoch 1, Step 569: Tokens Processed = 2334720\n",
      "Epoch 1, Step 570: Tokens Processed = 2338816\n",
      "Epoch 1, Step 570: Tokens Processed = 2338816\n",
      "Epoch 1, Step 571: Tokens Processed = 2342912\n",
      "Epoch 1, Step 571: Tokens Processed = 2342912\n",
      "Epoch 1, Step 572: Tokens Processed = 2347008\n",
      "Epoch 1, Step 572: Tokens Processed = 2347008\n",
      "Epoch 1, Step 573: Tokens Processed = 2351104\n",
      "Epoch 1, Step 573: Tokens Processed = 2351104\n",
      "Epoch 1, Step 574: Tokens Processed = 2355200\n",
      "Epoch 1, Step 574: Tokens Processed = 2355200\n",
      "Epoch 1, Step 575: Tokens Processed = 2359296\n",
      "Epoch 1, Step 575: Tokens Processed = 2359296\n",
      "Epoch 1, Step 576: Tokens Processed = 2363392\n",
      "Epoch 1, Step 576: Tokens Processed = 2363392\n",
      "Epoch 1, Step 577: Tokens Processed = 2367488\n",
      "Epoch 1, Step 577: Tokens Processed = 2367488\n",
      "Epoch 1, Step 578: Tokens Processed = 2371584\n",
      "Epoch 1, Step 578: Tokens Processed = 2371584\n",
      "Epoch 1, Step 579: Tokens Processed = 2375680\n",
      "Epoch 1, Step 579: Tokens Processed = 2375680\n",
      "Epoch 1, Step 580: Tokens Processed = 2379776\n",
      "Epoch 1, Step 580: Tokens Processed = 2379776\n",
      "Epoch 1, Step 581: Tokens Processed = 2383872\n",
      "Epoch 1, Step 581: Tokens Processed = 2383872\n",
      "Epoch 1, Step 582: Tokens Processed = 2387968\n",
      "Epoch 1, Step 582: Tokens Processed = 2387968\n",
      "Epoch 1, Step 583: Tokens Processed = 2392064\n",
      "Epoch 1, Step 583: Tokens Processed = 2392064\n",
      "Epoch 1, Step 584: Tokens Processed = 2396160\n",
      "Epoch 1, Step 584: Tokens Processed = 2396160\n",
      "Epoch 1, Step 585: Tokens Processed = 2400256\n",
      "Epoch 1, Step 585: Tokens Processed = 2400256\n",
      "Epoch 1, Step 586: Tokens Processed = 2404352\n",
      "Epoch 1, Step 586: Tokens Processed = 2404352\n",
      "Epoch 1, Step 587: Tokens Processed = 2408448\n",
      "Epoch 1, Step 587: Tokens Processed = 2408448\n",
      "Epoch 1, Step 588: Tokens Processed = 2412544\n",
      "Epoch 1, Step 588: Tokens Processed = 2412544\n",
      "Epoch 1, Step 589: Tokens Processed = 2416640\n",
      "Epoch 1, Step 589: Tokens Processed = 2416640\n",
      "Epoch 1, Step 590: Tokens Processed = 2420736\n",
      "Epoch 1, Step 590: Tokens Processed = 2420736\n",
      "Epoch 1, Step 591: Tokens Processed = 2424832\n",
      "Epoch 1, Step 591: Tokens Processed = 2424832\n",
      "Epoch 1, Step 592: Tokens Processed = 2428928\n",
      "Epoch 1, Step 592: Tokens Processed = 2428928\n",
      "Epoch 1, Step 593: Tokens Processed = 2433024\n",
      "Epoch 1, Step 593: Tokens Processed = 2433024\n",
      "Epoch 1, Step 594: Tokens Processed = 2437120\n",
      "Epoch 1, Step 594: Tokens Processed = 2437120\n",
      "Epoch 1, Step 595: Tokens Processed = 2441216\n",
      "Epoch 1, Step 595: Tokens Processed = 2441216\n",
      "Epoch 1, Step 596: Tokens Processed = 2445312\n",
      "Epoch 1, Step 596: Tokens Processed = 2445312\n",
      "Epoch 1, Step 597: Tokens Processed = 2449408\n",
      "Epoch 1, Step 597: Tokens Processed = 2449408\n",
      "Epoch 1, Step 598: Tokens Processed = 2453504\n",
      "Epoch 1, Step 598: Tokens Processed = 2453504\n",
      "Epoch 1, Step 599: Tokens Processed = 2457600\n",
      "Epoch 1, Step 599: Tokens Processed = 2457600\n",
      "Epoch 1, Step 600: Tokens Processed = 2461696\n",
      "Epoch 1, Step 600: Tokens Processed = 2461696\n",
      "Epoch 1, Step 600: Training Loss = 2.29552161693573, Validation Loss = 5.386713981628418, Tokens Processed = 2461696\n",
      "Epoch 1, Step 600: Training Loss = 2.29552161693573, Validation Loss = 5.386713981628418, Tokens Processed = 2461696\n",
      "Epoch 1, Step 601: Tokens Processed = 2465792\n",
      "Epoch 1, Step 601: Tokens Processed = 2465792\n",
      "Epoch 1, Step 602: Tokens Processed = 2469888\n",
      "Epoch 1, Step 602: Tokens Processed = 2469888\n",
      "Epoch 1, Step 603: Tokens Processed = 2473984\n",
      "Epoch 1, Step 603: Tokens Processed = 2473984\n",
      "Epoch 1, Step 604: Tokens Processed = 2478080\n",
      "Epoch 1, Step 604: Tokens Processed = 2478080\n",
      "Epoch 1, Step 605: Tokens Processed = 2482176\n",
      "Epoch 1, Step 605: Tokens Processed = 2482176\n",
      "Epoch 1, Step 606: Tokens Processed = 2486272\n",
      "Epoch 1, Step 606: Tokens Processed = 2486272\n",
      "Epoch 1, Step 607: Tokens Processed = 2490368\n",
      "Epoch 1, Step 607: Tokens Processed = 2490368\n",
      "Epoch 1, Step 608: Tokens Processed = 2494464\n",
      "Epoch 1, Step 608: Tokens Processed = 2494464\n",
      "Epoch 1, Step 609: Tokens Processed = 2498560\n",
      "Epoch 1, Step 609: Tokens Processed = 2498560\n",
      "Epoch 1, Step 610: Tokens Processed = 2502656\n",
      "Epoch 1, Step 610: Tokens Processed = 2502656\n",
      "Epoch 1, Step 611: Tokens Processed = 2506752\n",
      "Epoch 1, Step 611: Tokens Processed = 2506752\n",
      "Epoch 1, Step 612: Tokens Processed = 2510848\n",
      "Epoch 1, Step 612: Tokens Processed = 2510848\n",
      "Epoch 1, Step 613: Tokens Processed = 2514944\n",
      "Epoch 1, Step 613: Tokens Processed = 2514944\n",
      "Epoch 1, Step 614: Tokens Processed = 2519040\n",
      "Epoch 1, Step 614: Tokens Processed = 2519040\n",
      "Epoch 1, Step 615: Tokens Processed = 2523136\n",
      "Epoch 1, Step 615: Tokens Processed = 2523136\n",
      "Epoch 1, Step 616: Tokens Processed = 2527232\n",
      "Epoch 1, Step 616: Tokens Processed = 2527232\n",
      "Epoch 1, Step 617: Tokens Processed = 2531328\n",
      "Epoch 1, Step 617: Tokens Processed = 2531328\n",
      "Epoch 1, Step 618: Tokens Processed = 2535424\n",
      "Epoch 1, Step 618: Tokens Processed = 2535424\n",
      "Epoch 1, Step 619: Tokens Processed = 2539520\n",
      "Epoch 1, Step 619: Tokens Processed = 2539520\n",
      "Epoch 1, Step 620: Tokens Processed = 2543616\n",
      "Epoch 1, Step 620: Tokens Processed = 2543616\n",
      "Epoch 1, Step 621: Tokens Processed = 2547712\n",
      "Epoch 1, Step 621: Tokens Processed = 2547712\n",
      "Epoch 1, Step 622: Tokens Processed = 2551808\n",
      "Epoch 1, Step 622: Tokens Processed = 2551808\n",
      "Epoch 1, Step 623: Tokens Processed = 2555904\n",
      "Epoch 1, Step 623: Tokens Processed = 2555904\n",
      "Epoch 1, Step 624: Tokens Processed = 2560000\n",
      "Epoch 1, Step 624: Tokens Processed = 2560000\n",
      "Epoch 1, Step 625: Tokens Processed = 2564096\n",
      "Epoch 1, Step 625: Tokens Processed = 2564096\n",
      "Epoch 1, Step 626: Tokens Processed = 2568192\n",
      "Epoch 1, Step 626: Tokens Processed = 2568192\n",
      "Epoch 1, Step 627: Tokens Processed = 2572288\n",
      "Epoch 1, Step 627: Tokens Processed = 2572288\n",
      "Epoch 1, Step 628: Tokens Processed = 2576384\n",
      "Epoch 1, Step 628: Tokens Processed = 2576384\n",
      "Epoch 1, Step 629: Tokens Processed = 2580480\n",
      "Epoch 1, Step 629: Tokens Processed = 2580480\n",
      "Epoch 1, Step 630: Tokens Processed = 2584576\n",
      "Epoch 1, Step 630: Tokens Processed = 2584576\n",
      "Epoch 1, Step 631: Tokens Processed = 2588672\n",
      "Epoch 1, Step 631: Tokens Processed = 2588672\n",
      "Epoch 1, Step 632: Tokens Processed = 2592768\n",
      "Epoch 1, Step 632: Tokens Processed = 2592768\n",
      "Epoch 1, Step 633: Tokens Processed = 2596864\n",
      "Epoch 1, Step 633: Tokens Processed = 2596864\n",
      "Epoch 1, Step 634: Tokens Processed = 2600960\n",
      "Epoch 1, Step 634: Tokens Processed = 2600960\n",
      "Epoch 1, Step 635: Tokens Processed = 2605056\n",
      "Epoch 1, Step 635: Tokens Processed = 2605056\n",
      "Epoch 1, Step 636: Tokens Processed = 2609152\n",
      "Epoch 1, Step 636: Tokens Processed = 2609152\n",
      "Epoch 1, Step 637: Tokens Processed = 2613248\n",
      "Epoch 1, Step 637: Tokens Processed = 2613248\n",
      "Epoch 1, Step 638: Tokens Processed = 2617344\n",
      "Epoch 1, Step 638: Tokens Processed = 2617344\n",
      "Epoch 1, Step 639: Tokens Processed = 2621440\n",
      "Epoch 1, Step 639: Tokens Processed = 2621440\n",
      "Epoch 1, Step 640: Tokens Processed = 2625536\n",
      "Epoch 1, Step 640: Tokens Processed = 2625536\n",
      "Epoch 1, Step 641: Tokens Processed = 2629632\n",
      "Epoch 1, Step 641: Tokens Processed = 2629632\n",
      "Epoch 1, Step 642: Tokens Processed = 2633728\n",
      "Epoch 1, Step 642: Tokens Processed = 2633728\n",
      "Epoch 1, Step 643: Tokens Processed = 2637824\n",
      "Epoch 1, Step 643: Tokens Processed = 2637824\n",
      "Epoch 1, Step 644: Tokens Processed = 2641920\n",
      "Epoch 1, Step 644: Tokens Processed = 2641920\n",
      "Epoch 1, Step 645: Tokens Processed = 2646016\n",
      "Epoch 1, Step 645: Tokens Processed = 2646016\n",
      "Epoch 1, Step 646: Tokens Processed = 2650112\n",
      "Epoch 1, Step 646: Tokens Processed = 2650112\n",
      "Epoch 1, Step 647: Tokens Processed = 2654208\n",
      "Epoch 1, Step 647: Tokens Processed = 2654208\n",
      "Epoch 1, Step 648: Tokens Processed = 2658304\n",
      "Epoch 1, Step 648: Tokens Processed = 2658304\n",
      "Epoch 1, Step 649: Tokens Processed = 2662400\n",
      "Epoch 1, Step 649: Tokens Processed = 2662400\n",
      "Epoch 1, Step 650: Tokens Processed = 2666496\n",
      "Epoch 1, Step 650: Tokens Processed = 2666496\n",
      "Epoch 1, Step 651: Tokens Processed = 2670592\n",
      "Epoch 1, Step 651: Tokens Processed = 2670592\n",
      "Epoch 1, Step 652: Tokens Processed = 2674688\n",
      "Epoch 1, Step 652: Tokens Processed = 2674688\n",
      "Epoch 1, Step 653: Tokens Processed = 2678784\n",
      "Epoch 1, Step 653: Tokens Processed = 2678784\n",
      "Epoch 1, Step 654: Tokens Processed = 2682880\n",
      "Epoch 1, Step 654: Tokens Processed = 2682880\n",
      "Epoch 1, Step 655: Tokens Processed = 2686976\n",
      "Epoch 1, Step 655: Tokens Processed = 2686976\n",
      "Epoch 1, Step 656: Tokens Processed = 2691072\n",
      "Epoch 1, Step 656: Tokens Processed = 2691072\n",
      "Epoch 1, Step 657: Tokens Processed = 2695168\n",
      "Epoch 1, Step 657: Tokens Processed = 2695168\n",
      "Epoch 1, Step 658: Tokens Processed = 2699264\n",
      "Epoch 1, Step 658: Tokens Processed = 2699264\n",
      "Epoch 1, Step 659: Tokens Processed = 2703360\n",
      "Epoch 1, Step 659: Tokens Processed = 2703360\n",
      "Epoch 1, Step 660: Tokens Processed = 2707456\n",
      "Epoch 1, Step 660: Tokens Processed = 2707456\n",
      "Epoch 1, Step 661: Tokens Processed = 2711552\n",
      "Epoch 1, Step 661: Tokens Processed = 2711552\n",
      "Epoch 1, Step 662: Tokens Processed = 2715648\n",
      "Epoch 1, Step 662: Tokens Processed = 2715648\n",
      "Epoch 1, Step 663: Tokens Processed = 2719744\n",
      "Epoch 1, Step 663: Tokens Processed = 2719744\n",
      "Epoch 1, Step 664: Tokens Processed = 2723840\n",
      "Epoch 1, Step 664: Tokens Processed = 2723840\n",
      "Epoch 1, Step 665: Tokens Processed = 2727936\n",
      "Epoch 1, Step 665: Tokens Processed = 2727936\n",
      "Epoch 1, Step 666: Tokens Processed = 2732032\n",
      "Epoch 1, Step 666: Tokens Processed = 2732032\n",
      "Epoch 1, Step 667: Tokens Processed = 2736128\n",
      "Epoch 1, Step 667: Tokens Processed = 2736128\n",
      "Epoch 1, Step 668: Tokens Processed = 2740224\n",
      "Epoch 1, Step 668: Tokens Processed = 2740224\n",
      "Epoch 1, Step 669: Tokens Processed = 2744320\n",
      "Epoch 1, Step 669: Tokens Processed = 2744320\n",
      "Epoch 1, Step 670: Tokens Processed = 2748416\n",
      "Epoch 1, Step 670: Tokens Processed = 2748416\n",
      "Epoch 1, Step 671: Tokens Processed = 2752512\n",
      "Epoch 1, Step 671: Tokens Processed = 2752512\n",
      "Epoch 1, Step 672: Tokens Processed = 2756608\n",
      "Epoch 1, Step 672: Tokens Processed = 2756608\n",
      "Epoch 1, Step 673: Tokens Processed = 2760704\n",
      "Epoch 1, Step 673: Tokens Processed = 2760704\n",
      "Epoch 1, Step 674: Tokens Processed = 2764800\n",
      "Epoch 1, Step 674: Tokens Processed = 2764800\n",
      "Epoch 1, Step 675: Tokens Processed = 2768896\n",
      "Epoch 1, Step 675: Tokens Processed = 2768896\n",
      "Epoch 1, Step 676: Tokens Processed = 2772992\n",
      "Epoch 1, Step 676: Tokens Processed = 2772992\n",
      "Epoch 1, Step 677: Tokens Processed = 2777088\n",
      "Epoch 1, Step 677: Tokens Processed = 2777088\n",
      "Epoch 1, Step 678: Tokens Processed = 2781184\n",
      "Epoch 1, Step 678: Tokens Processed = 2781184\n",
      "Epoch 1, Step 679: Tokens Processed = 2785280\n",
      "Epoch 1, Step 679: Tokens Processed = 2785280\n",
      "Epoch 1, Step 680: Tokens Processed = 2789376\n",
      "Epoch 1, Step 680: Tokens Processed = 2789376\n",
      "Epoch 1, Step 681: Tokens Processed = 2793472\n",
      "Epoch 1, Step 681: Tokens Processed = 2793472\n",
      "Epoch 1, Step 682: Tokens Processed = 2797568\n",
      "Epoch 1, Step 682: Tokens Processed = 2797568\n",
      "Epoch 1, Step 683: Tokens Processed = 2801664\n",
      "Epoch 1, Step 683: Tokens Processed = 2801664\n",
      "Epoch 1, Step 684: Tokens Processed = 2805760\n",
      "Epoch 1, Step 684: Tokens Processed = 2805760\n",
      "Epoch 1, Step 685: Tokens Processed = 2809856\n",
      "Epoch 1, Step 685: Tokens Processed = 2809856\n",
      "Epoch 1, Step 686: Tokens Processed = 2813952\n",
      "Epoch 1, Step 686: Tokens Processed = 2813952\n",
      "Epoch 1, Step 687: Tokens Processed = 2818048\n",
      "Epoch 1, Step 687: Tokens Processed = 2818048\n",
      "Epoch 1, Step 688: Tokens Processed = 2822144\n",
      "Epoch 1, Step 688: Tokens Processed = 2822144\n",
      "Epoch 1, Step 689: Tokens Processed = 2826240\n",
      "Epoch 1, Step 689: Tokens Processed = 2826240\n",
      "Epoch 1, Step 690: Tokens Processed = 2830336\n",
      "Epoch 1, Step 690: Tokens Processed = 2830336\n",
      "Epoch 1, Step 691: Tokens Processed = 2834432\n",
      "Epoch 1, Step 691: Tokens Processed = 2834432\n",
      "Epoch 1, Step 692: Tokens Processed = 2838528\n",
      "Epoch 1, Step 692: Tokens Processed = 2838528\n",
      "Epoch 1, Step 693: Tokens Processed = 2842624\n",
      "Epoch 1, Step 693: Tokens Processed = 2842624\n",
      "Epoch 1, Step 694: Tokens Processed = 2846720\n",
      "Epoch 1, Step 694: Tokens Processed = 2846720\n",
      "Epoch 1, Step 695: Tokens Processed = 2850816\n",
      "Epoch 1, Step 695: Tokens Processed = 2850816\n",
      "Epoch 1, Step 696: Tokens Processed = 2854912\n",
      "Epoch 1, Step 696: Tokens Processed = 2854912\n",
      "Epoch 1, Step 697: Tokens Processed = 2859008\n",
      "Epoch 1, Step 697: Tokens Processed = 2859008\n",
      "Epoch 1, Step 698: Tokens Processed = 2863104\n",
      "Epoch 1, Step 698: Tokens Processed = 2863104\n",
      "Epoch 1, Step 699: Tokens Processed = 2867200\n",
      "Epoch 1, Step 699: Tokens Processed = 2867200\n",
      "Epoch 1, Step 700: Tokens Processed = 2871296\n",
      "Epoch 1, Step 700: Tokens Processed = 2871296\n",
      "Epoch 1, Step 700: Training Loss = 2.2460044622421265, Validation Loss = 5.335391521453857, Tokens Processed = 2871296\n",
      "Epoch 1, Step 700: Training Loss = 2.2460044622421265, Validation Loss = 5.335391521453857, Tokens Processed = 2871296\n",
      "Epoch 1, Step 701: Tokens Processed = 2875392\n",
      "Epoch 1, Step 701: Tokens Processed = 2875392\n",
      "Epoch 1, Step 702: Tokens Processed = 2879488\n",
      "Epoch 1, Step 702: Tokens Processed = 2879488\n",
      "Epoch 1, Step 703: Tokens Processed = 2883584\n",
      "Epoch 1, Step 703: Tokens Processed = 2883584\n",
      "Epoch 1, Step 704: Tokens Processed = 2887680\n",
      "Epoch 1, Step 704: Tokens Processed = 2887680\n",
      "Epoch 1, Step 705: Tokens Processed = 2891776\n",
      "Epoch 1, Step 705: Tokens Processed = 2891776\n",
      "Epoch 1, Step 706: Tokens Processed = 2895872\n",
      "Epoch 1, Step 706: Tokens Processed = 2895872\n",
      "Epoch 1, Step 707: Tokens Processed = 2899968\n",
      "Epoch 1, Step 707: Tokens Processed = 2899968\n",
      "Epoch 1, Step 708: Tokens Processed = 2904064\n",
      "Epoch 1, Step 708: Tokens Processed = 2904064\n",
      "Epoch 1, Step 709: Tokens Processed = 2908160\n",
      "Epoch 1, Step 709: Tokens Processed = 2908160\n",
      "Epoch 1, Step 710: Tokens Processed = 2912256\n",
      "Epoch 1, Step 710: Tokens Processed = 2912256\n",
      "Epoch 1, Step 711: Tokens Processed = 2916352\n",
      "Epoch 1, Step 711: Tokens Processed = 2916352\n",
      "Epoch 1, Step 712: Tokens Processed = 2920448\n",
      "Epoch 1, Step 712: Tokens Processed = 2920448\n",
      "Epoch 1, Step 713: Tokens Processed = 2924544\n",
      "Epoch 1, Step 713: Tokens Processed = 2924544\n",
      "Epoch 1, Step 714: Tokens Processed = 2928640\n",
      "Epoch 1, Step 714: Tokens Processed = 2928640\n",
      "Epoch 1, Step 715: Tokens Processed = 2932736\n",
      "Epoch 1, Step 715: Tokens Processed = 2932736\n",
      "Epoch 1, Step 716: Tokens Processed = 2936832\n",
      "Epoch 1, Step 716: Tokens Processed = 2936832\n",
      "Epoch 1, Step 717: Tokens Processed = 2940928\n",
      "Epoch 1, Step 717: Tokens Processed = 2940928\n",
      "Epoch 1, Step 718: Tokens Processed = 2945024\n",
      "Epoch 1, Step 718: Tokens Processed = 2945024\n",
      "Epoch 1, Step 719: Tokens Processed = 2949120\n",
      "Epoch 1, Step 719: Tokens Processed = 2949120\n",
      "Epoch 1, Step 720: Tokens Processed = 2953216\n",
      "Epoch 1, Step 720: Tokens Processed = 2953216\n",
      "Epoch 1, Step 721: Tokens Processed = 2957312\n",
      "Epoch 1, Step 721: Tokens Processed = 2957312\n",
      "Epoch 1, Step 722: Tokens Processed = 2961408\n",
      "Epoch 1, Step 722: Tokens Processed = 2961408\n",
      "Epoch 1, Step 723: Tokens Processed = 2965504\n",
      "Epoch 1, Step 723: Tokens Processed = 2965504\n",
      "Epoch 1, Step 724: Tokens Processed = 2969600\n",
      "Epoch 1, Step 724: Tokens Processed = 2969600\n",
      "Epoch 1, Step 725: Tokens Processed = 2973696\n",
      "Epoch 1, Step 725: Tokens Processed = 2973696\n",
      "Epoch 1, Step 726: Tokens Processed = 2977792\n",
      "Epoch 1, Step 726: Tokens Processed = 2977792\n",
      "Epoch 1, Step 727: Tokens Processed = 2981888\n",
      "Epoch 1, Step 727: Tokens Processed = 2981888\n",
      "Epoch 1, Step 728: Tokens Processed = 2985984\n",
      "Epoch 1, Step 728: Tokens Processed = 2985984\n",
      "Epoch 1, Step 729: Tokens Processed = 2990080\n",
      "Epoch 1, Step 729: Tokens Processed = 2990080\n",
      "Epoch 1, Step 730: Tokens Processed = 2994176\n",
      "Epoch 1, Step 730: Tokens Processed = 2994176\n",
      "Epoch 1, Step 731: Tokens Processed = 2998272\n",
      "Epoch 1, Step 731: Tokens Processed = 2998272\n",
      "Epoch 1, Step 732: Tokens Processed = 3002368\n",
      "Epoch 1, Step 732: Tokens Processed = 3002368\n",
      "Epoch 1, Step 733: Tokens Processed = 3006464\n",
      "Epoch 1, Step 733: Tokens Processed = 3006464\n",
      "Epoch 1, Step 734: Tokens Processed = 3010560\n",
      "Epoch 1, Step 734: Tokens Processed = 3010560\n",
      "Epoch 1, Step 735: Tokens Processed = 3014656\n",
      "Epoch 1, Step 735: Tokens Processed = 3014656\n",
      "Epoch 1, Step 736: Tokens Processed = 3018752\n",
      "Epoch 1, Step 736: Tokens Processed = 3018752\n",
      "Epoch 1, Step 737: Tokens Processed = 3022848\n",
      "Epoch 1, Step 737: Tokens Processed = 3022848\n",
      "Epoch 1, Step 738: Tokens Processed = 3026944\n",
      "Epoch 1, Step 738: Tokens Processed = 3026944\n",
      "Epoch 1, Step 739: Tokens Processed = 3031040\n",
      "Epoch 1, Step 739: Tokens Processed = 3031040\n",
      "Epoch 1, Step 740: Tokens Processed = 3035136\n",
      "Epoch 1, Step 740: Tokens Processed = 3035136\n",
      "Epoch 1, Step 741: Tokens Processed = 3039232\n",
      "Epoch 1, Step 741: Tokens Processed = 3039232\n",
      "Epoch 1, Step 742: Tokens Processed = 3043328\n",
      "Epoch 1, Step 742: Tokens Processed = 3043328\n",
      "Epoch 1, Step 743: Tokens Processed = 3047424\n",
      "Epoch 1, Step 743: Tokens Processed = 3047424\n",
      "Epoch 1, Step 744: Tokens Processed = 3051520\n",
      "Epoch 1, Step 744: Tokens Processed = 3051520\n",
      "Epoch 1, Step 745: Tokens Processed = 3055616\n",
      "Epoch 1, Step 745: Tokens Processed = 3055616\n",
      "Epoch 1, Step 746: Tokens Processed = 3059712\n",
      "Epoch 1, Step 746: Tokens Processed = 3059712\n",
      "Epoch 1, Step 747: Tokens Processed = 3063808\n",
      "Epoch 1, Step 747: Tokens Processed = 3063808\n",
      "Epoch 1, Step 748: Tokens Processed = 3067904\n",
      "Epoch 1, Step 748: Tokens Processed = 3067904\n",
      "Epoch 1, Step 749: Tokens Processed = 3072000\n",
      "Epoch 1, Step 749: Tokens Processed = 3072000\n",
      "Epoch 1, Step 750: Tokens Processed = 3076096\n",
      "Epoch 1, Step 750: Tokens Processed = 3076096\n",
      "Epoch 1, Step 751: Tokens Processed = 3080192\n",
      "Epoch 1, Step 751: Tokens Processed = 3080192\n",
      "Epoch 1, Step 752: Tokens Processed = 3084288\n",
      "Epoch 1, Step 752: Tokens Processed = 3084288\n",
      "Epoch 1, Step 753: Tokens Processed = 3088384\n",
      "Epoch 1, Step 753: Tokens Processed = 3088384\n",
      "Epoch 1, Step 754: Tokens Processed = 3092480\n",
      "Epoch 1, Step 754: Tokens Processed = 3092480\n",
      "Epoch 1, Step 755: Tokens Processed = 3096576\n",
      "Epoch 1, Step 755: Tokens Processed = 3096576\n",
      "Epoch 1, Step 756: Tokens Processed = 3100672\n",
      "Epoch 1, Step 756: Tokens Processed = 3100672\n",
      "Epoch 1, Step 757: Tokens Processed = 3104768\n",
      "Epoch 1, Step 757: Tokens Processed = 3104768\n",
      "Epoch 1, Step 758: Tokens Processed = 3108864\n",
      "Epoch 1, Step 758: Tokens Processed = 3108864\n",
      "Epoch 1, Step 759: Tokens Processed = 3112960\n",
      "Epoch 1, Step 759: Tokens Processed = 3112960\n",
      "Epoch 1, Step 760: Tokens Processed = 3117056\n",
      "Epoch 1, Step 760: Tokens Processed = 3117056\n",
      "Epoch 1, Step 761: Tokens Processed = 3121152\n",
      "Epoch 1, Step 761: Tokens Processed = 3121152\n",
      "Epoch 1, Step 762: Tokens Processed = 3125248\n",
      "Epoch 1, Step 762: Tokens Processed = 3125248\n",
      "Epoch 1, Step 763: Tokens Processed = 3129344\n",
      "Epoch 1, Step 763: Tokens Processed = 3129344\n",
      "Epoch 1, Step 764: Tokens Processed = 3133440\n",
      "Epoch 1, Step 764: Tokens Processed = 3133440\n",
      "Epoch 1, Step 765: Tokens Processed = 3137536\n",
      "Epoch 1, Step 765: Tokens Processed = 3137536\n",
      "Epoch 1, Step 766: Tokens Processed = 3141632\n",
      "Epoch 1, Step 766: Tokens Processed = 3141632\n",
      "Epoch 1, Step 767: Tokens Processed = 3145728\n",
      "Epoch 1, Step 767: Tokens Processed = 3145728\n",
      "Epoch 1, Step 768: Tokens Processed = 3149824\n",
      "Epoch 1, Step 768: Tokens Processed = 3149824\n",
      "Epoch 1, Step 769: Tokens Processed = 3153920\n",
      "Epoch 1, Step 769: Tokens Processed = 3153920\n",
      "Epoch 1, Step 770: Tokens Processed = 3158016\n",
      "Epoch 1, Step 770: Tokens Processed = 3158016\n",
      "Epoch 1, Step 771: Tokens Processed = 3162112\n",
      "Epoch 1, Step 771: Tokens Processed = 3162112\n",
      "Epoch 1, Step 772: Tokens Processed = 3166208\n",
      "Epoch 1, Step 772: Tokens Processed = 3166208\n",
      "Epoch 1, Step 773: Tokens Processed = 3170304\n",
      "Epoch 1, Step 773: Tokens Processed = 3170304\n",
      "Epoch 1, Step 774: Tokens Processed = 3174400\n",
      "Epoch 1, Step 774: Tokens Processed = 3174400\n",
      "Epoch 1, Step 775: Tokens Processed = 3178496\n",
      "Epoch 1, Step 775: Tokens Processed = 3178496\n",
      "Epoch 1, Step 776: Tokens Processed = 3182592\n",
      "Epoch 1, Step 776: Tokens Processed = 3182592\n",
      "Epoch 1, Step 777: Tokens Processed = 3186688\n",
      "Epoch 1, Step 777: Tokens Processed = 3186688\n",
      "Epoch 1, Step 778: Tokens Processed = 3190784\n",
      "Epoch 1, Step 778: Tokens Processed = 3190784\n",
      "Epoch 1, Step 779: Tokens Processed = 3194880\n",
      "Epoch 1, Step 779: Tokens Processed = 3194880\n",
      "Epoch 1, Step 780: Tokens Processed = 3198976\n",
      "Epoch 1, Step 780: Tokens Processed = 3198976\n",
      "Epoch 1, Step 781: Tokens Processed = 3203072\n",
      "Epoch 1, Step 781: Tokens Processed = 3203072\n",
      "Epoch 1, Step 782: Tokens Processed = 3207168\n",
      "Epoch 1, Step 782: Tokens Processed = 3207168\n",
      "Epoch 1, Step 783: Tokens Processed = 3211264\n",
      "Epoch 1, Step 783: Tokens Processed = 3211264\n",
      "Epoch 1, Step 784: Tokens Processed = 3215360\n",
      "Epoch 1, Step 784: Tokens Processed = 3215360\n",
      "Epoch 1, Step 785: Tokens Processed = 3219456\n",
      "Epoch 1, Step 785: Tokens Processed = 3219456\n",
      "Epoch 1, Step 786: Tokens Processed = 3223552\n",
      "Epoch 1, Step 786: Tokens Processed = 3223552\n",
      "Epoch 1, Step 787: Tokens Processed = 3227648\n",
      "Epoch 1, Step 787: Tokens Processed = 3227648\n",
      "Epoch 1, Step 788: Tokens Processed = 3231744\n",
      "Epoch 1, Step 788: Tokens Processed = 3231744\n",
      "Epoch 1, Step 789: Tokens Processed = 3235840\n",
      "Epoch 1, Step 789: Tokens Processed = 3235840\n",
      "Epoch 1, Step 790: Tokens Processed = 3239936\n",
      "Epoch 1, Step 790: Tokens Processed = 3239936\n",
      "Epoch 1, Step 791: Tokens Processed = 3244032\n",
      "Epoch 1, Step 791: Tokens Processed = 3244032\n",
      "Epoch 1, Step 792: Tokens Processed = 3248128\n",
      "Epoch 1, Step 792: Tokens Processed = 3248128\n",
      "Epoch 1, Step 793: Tokens Processed = 3252224\n",
      "Epoch 1, Step 793: Tokens Processed = 3252224\n",
      "Epoch 1, Step 794: Tokens Processed = 3256320\n",
      "Epoch 1, Step 794: Tokens Processed = 3256320\n",
      "Epoch 1, Step 795: Tokens Processed = 3260416\n",
      "Epoch 1, Step 795: Tokens Processed = 3260416\n",
      "Epoch 1, Step 796: Tokens Processed = 3264512\n",
      "Epoch 1, Step 796: Tokens Processed = 3264512\n",
      "Epoch 1, Step 797: Tokens Processed = 3268608\n",
      "Epoch 1, Step 797: Tokens Processed = 3268608\n",
      "Epoch 1, Step 798: Tokens Processed = 3272704\n",
      "Epoch 1, Step 798: Tokens Processed = 3272704\n",
      "Epoch 1, Step 799: Tokens Processed = 3276800\n",
      "Epoch 1, Step 799: Tokens Processed = 3276800\n",
      "Epoch 1, Step 800: Tokens Processed = 3280896\n",
      "Epoch 1, Step 800: Tokens Processed = 3280896\n",
      "Epoch 1, Step 800: Training Loss = 2.053513765335083, Validation Loss = 5.409467697143555, Tokens Processed = 3280896\n",
      "Epoch 1, Step 800: Training Loss = 2.053513765335083, Validation Loss = 5.409467697143555, Tokens Processed = 3280896\n",
      "Epoch 1, Step 801: Tokens Processed = 3284992\n",
      "Epoch 1, Step 801: Tokens Processed = 3284992\n",
      "Epoch 1, Step 802: Tokens Processed = 3289088\n",
      "Epoch 1, Step 802: Tokens Processed = 3289088\n",
      "Epoch 1, Step 803: Tokens Processed = 3293184\n",
      "Epoch 1, Step 803: Tokens Processed = 3293184\n",
      "Epoch 1, Step 804: Tokens Processed = 3297280\n",
      "Epoch 1, Step 804: Tokens Processed = 3297280\n",
      "Epoch 1, Step 805: Tokens Processed = 3301376\n",
      "Epoch 1, Step 805: Tokens Processed = 3301376\n",
      "Epoch 1, Step 806: Tokens Processed = 3305472\n",
      "Epoch 1, Step 806: Tokens Processed = 3305472\n",
      "Epoch 1, Step 807: Tokens Processed = 3309568\n",
      "Epoch 1, Step 807: Tokens Processed = 3309568\n",
      "Epoch 1, Step 808: Tokens Processed = 3313664\n",
      "Epoch 1, Step 808: Tokens Processed = 3313664\n",
      "Epoch 1, Step 809: Tokens Processed = 3317760\n",
      "Epoch 1, Step 809: Tokens Processed = 3317760\n",
      "Epoch 1, Step 810: Tokens Processed = 3321856\n",
      "Epoch 1, Step 810: Tokens Processed = 3321856\n",
      "Epoch 1, Step 811: Tokens Processed = 3325952\n",
      "Epoch 1, Step 811: Tokens Processed = 3325952\n",
      "Epoch 1, Step 812: Tokens Processed = 3330048\n",
      "Epoch 1, Step 812: Tokens Processed = 3330048\n",
      "Epoch 1, Step 813: Tokens Processed = 3334144\n",
      "Epoch 1, Step 813: Tokens Processed = 3334144\n",
      "Epoch 1, Step 814: Tokens Processed = 3338240\n",
      "Epoch 1, Step 814: Tokens Processed = 3338240\n",
      "Epoch 1, Step 815: Tokens Processed = 3342336\n",
      "Epoch 1, Step 815: Tokens Processed = 3342336\n",
      "Epoch 1, Step 816: Tokens Processed = 3346432\n",
      "Epoch 1, Step 816: Tokens Processed = 3346432\n",
      "Epoch 1, Step 817: Tokens Processed = 3350528\n",
      "Epoch 1, Step 817: Tokens Processed = 3350528\n",
      "Epoch 1, Step 818: Tokens Processed = 3354624\n",
      "Epoch 1, Step 818: Tokens Processed = 3354624\n",
      "Epoch 1, Step 819: Tokens Processed = 3358720\n",
      "Epoch 1, Step 819: Tokens Processed = 3358720\n",
      "Epoch 1, Step 820: Tokens Processed = 3362816\n",
      "Epoch 1, Step 820: Tokens Processed = 3362816\n",
      "Epoch 1, Step 821: Tokens Processed = 3366912\n",
      "Epoch 1, Step 821: Tokens Processed = 3366912\n",
      "Epoch 1, Step 822: Tokens Processed = 3371008\n",
      "Epoch 1, Step 822: Tokens Processed = 3371008\n",
      "Epoch 1, Step 823: Tokens Processed = 3375104\n",
      "Epoch 1, Step 823: Tokens Processed = 3375104\n",
      "Epoch 1, Step 824: Tokens Processed = 3379200\n",
      "Epoch 1, Step 824: Tokens Processed = 3379200\n",
      "Epoch 1, Step 825: Tokens Processed = 3383296\n",
      "Epoch 1, Step 825: Tokens Processed = 3383296\n",
      "Epoch 1, Step 826: Tokens Processed = 3387392\n",
      "Epoch 1, Step 826: Tokens Processed = 3387392\n",
      "Epoch 1, Step 827: Tokens Processed = 3391488\n",
      "Epoch 1, Step 827: Tokens Processed = 3391488\n",
      "Epoch 1, Step 828: Tokens Processed = 3395584\n",
      "Epoch 1, Step 828: Tokens Processed = 3395584\n",
      "Epoch 1, Step 829: Tokens Processed = 3399680\n",
      "Epoch 1, Step 829: Tokens Processed = 3399680\n",
      "Epoch 1, Step 830: Tokens Processed = 3403776\n",
      "Epoch 1, Step 830: Tokens Processed = 3403776\n",
      "Epoch 1, Step 831: Tokens Processed = 3407872\n",
      "Epoch 1, Step 831: Tokens Processed = 3407872\n",
      "Epoch 1, Step 832: Tokens Processed = 3411968\n",
      "Epoch 1, Step 832: Tokens Processed = 3411968\n",
      "Epoch 1, Step 833: Tokens Processed = 3416064\n",
      "Epoch 1, Step 833: Tokens Processed = 3416064\n",
      "Epoch 1, Step 834: Tokens Processed = 3420160\n",
      "Epoch 1, Step 834: Tokens Processed = 3420160\n",
      "Epoch 1, Step 835: Tokens Processed = 3424256\n",
      "Epoch 1, Step 835: Tokens Processed = 3424256\n",
      "Epoch 1, Step 836: Tokens Processed = 3428352\n",
      "Epoch 1, Step 836: Tokens Processed = 3428352\n",
      "Epoch 1, Step 837: Tokens Processed = 3432448\n",
      "Epoch 1, Step 837: Tokens Processed = 3432448\n",
      "Epoch 1, Step 838: Tokens Processed = 3436544\n",
      "Epoch 1, Step 838: Tokens Processed = 3436544\n",
      "Epoch 1, Step 839: Tokens Processed = 3440640\n",
      "Epoch 1, Step 839: Tokens Processed = 3440640\n",
      "Epoch 1, Step 840: Tokens Processed = 3444736\n",
      "Epoch 1, Step 840: Tokens Processed = 3444736\n",
      "Epoch 1, Step 841: Tokens Processed = 3448832\n",
      "Epoch 1, Step 841: Tokens Processed = 3448832\n",
      "Epoch 1, Step 842: Tokens Processed = 3452928\n",
      "Epoch 1, Step 842: Tokens Processed = 3452928\n",
      "Epoch 1, Step 843: Tokens Processed = 3457024\n",
      "Epoch 1, Step 843: Tokens Processed = 3457024\n",
      "Epoch 1, Step 844: Tokens Processed = 3461120\n",
      "Epoch 1, Step 844: Tokens Processed = 3461120\n",
      "Epoch 1, Step 845: Tokens Processed = 3465216\n",
      "Epoch 1, Step 845: Tokens Processed = 3465216\n",
      "Epoch 1, Step 846: Tokens Processed = 3469312\n",
      "Epoch 1, Step 846: Tokens Processed = 3469312\n",
      "Epoch 1, Step 847: Tokens Processed = 3473408\n",
      "Epoch 1, Step 847: Tokens Processed = 3473408\n",
      "Epoch 1, Step 848: Tokens Processed = 3477504\n",
      "Epoch 1, Step 848: Tokens Processed = 3477504\n",
      "Epoch 1, Step 849: Tokens Processed = 3481600\n",
      "Epoch 1, Step 849: Tokens Processed = 3481600\n",
      "Epoch 1, Step 850: Tokens Processed = 3485696\n",
      "Epoch 1, Step 850: Tokens Processed = 3485696\n",
      "Epoch 1, Step 851: Tokens Processed = 3489792\n",
      "Epoch 1, Step 851: Tokens Processed = 3489792\n",
      "Epoch 1, Step 852: Tokens Processed = 3493888\n",
      "Epoch 1, Step 852: Tokens Processed = 3493888\n",
      "Epoch 1, Step 853: Tokens Processed = 3497984\n",
      "Epoch 1, Step 853: Tokens Processed = 3497984\n",
      "Epoch 1, Step 854: Tokens Processed = 3502080\n",
      "Epoch 1, Step 854: Tokens Processed = 3502080\n",
      "Epoch 1, Step 855: Tokens Processed = 3506176\n",
      "Epoch 1, Step 855: Tokens Processed = 3506176\n",
      "Epoch 1, Step 856: Tokens Processed = 3510272\n",
      "Epoch 1, Step 856: Tokens Processed = 3510272\n",
      "Epoch 1, Step 857: Tokens Processed = 3514368\n",
      "Epoch 1, Step 857: Tokens Processed = 3514368\n",
      "Epoch 1, Step 858: Tokens Processed = 3518464\n",
      "Epoch 1, Step 858: Tokens Processed = 3518464\n",
      "Epoch 1, Step 859: Tokens Processed = 3522560\n",
      "Epoch 1, Step 859: Tokens Processed = 3522560\n",
      "Epoch 1, Step 860: Tokens Processed = 3526656\n",
      "Epoch 1, Step 860: Tokens Processed = 3526656\n",
      "Epoch 1, Step 861: Tokens Processed = 3530752\n",
      "Epoch 1, Step 861: Tokens Processed = 3530752\n",
      "Epoch 1, Step 862: Tokens Processed = 3534848\n",
      "Epoch 1, Step 862: Tokens Processed = 3534848\n",
      "Epoch 1, Step 863: Tokens Processed = 3538944\n",
      "Epoch 1, Step 863: Tokens Processed = 3538944\n",
      "Epoch 1, Step 864: Tokens Processed = 3543040\n",
      "Epoch 1, Step 864: Tokens Processed = 3543040\n",
      "Epoch 1, Step 865: Tokens Processed = 3547136\n",
      "Epoch 1, Step 865: Tokens Processed = 3547136\n",
      "Epoch 1, Step 866: Tokens Processed = 3551232\n",
      "Epoch 1, Step 866: Tokens Processed = 3551232\n",
      "Epoch 1, Step 867: Tokens Processed = 3555328\n",
      "Epoch 1, Step 867: Tokens Processed = 3555328\n",
      "Epoch 1, Step 868: Tokens Processed = 3559424\n",
      "Epoch 1, Step 868: Tokens Processed = 3559424\n",
      "Epoch 1, Step 869: Tokens Processed = 3563520\n",
      "Epoch 1, Step 869: Tokens Processed = 3563520\n",
      "Epoch 1, Step 870: Tokens Processed = 3567616\n",
      "Epoch 1, Step 870: Tokens Processed = 3567616\n",
      "Epoch 1, Step 871: Tokens Processed = 3571712\n",
      "Epoch 1, Step 871: Tokens Processed = 3571712\n",
      "Epoch 1, Step 872: Tokens Processed = 3575808\n",
      "Epoch 1, Step 872: Tokens Processed = 3575808\n",
      "Epoch 1, Step 873: Tokens Processed = 3579904\n",
      "Epoch 1, Step 873: Tokens Processed = 3579904\n",
      "Epoch 1, Step 874: Tokens Processed = 3584000\n",
      "Epoch 1, Step 874: Tokens Processed = 3584000\n",
      "Epoch 1, Step 875: Tokens Processed = 3588096\n",
      "Epoch 1, Step 875: Tokens Processed = 3588096\n",
      "Epoch 1, Step 876: Tokens Processed = 3592192\n",
      "Epoch 1, Step 876: Tokens Processed = 3592192\n",
      "Epoch 1, Step 877: Tokens Processed = 3596288\n",
      "Epoch 1, Step 877: Tokens Processed = 3596288\n",
      "Epoch 1, Step 878: Tokens Processed = 3600384\n",
      "Epoch 1, Step 878: Tokens Processed = 3600384\n",
      "Epoch 1, Step 879: Tokens Processed = 3604480\n",
      "Epoch 1, Step 879: Tokens Processed = 3604480\n",
      "Epoch 1, Step 880: Tokens Processed = 3608576\n",
      "Epoch 1, Step 880: Tokens Processed = 3608576\n",
      "Epoch 1, Step 881: Tokens Processed = 3612672\n",
      "Epoch 1, Step 881: Tokens Processed = 3612672\n",
      "Epoch 1, Step 882: Tokens Processed = 3616768\n",
      "Epoch 1, Step 882: Tokens Processed = 3616768\n",
      "Epoch 1, Step 883: Tokens Processed = 3620864\n",
      "Epoch 1, Step 883: Tokens Processed = 3620864\n",
      "Epoch 1, Step 884: Tokens Processed = 3624960\n",
      "Epoch 1, Step 884: Tokens Processed = 3624960\n",
      "Epoch 1, Step 885: Tokens Processed = 3629056\n",
      "Epoch 1, Step 885: Tokens Processed = 3629056\n",
      "Epoch 1, Step 886: Tokens Processed = 3633152\n",
      "Epoch 1, Step 886: Tokens Processed = 3633152\n",
      "Epoch 1, Step 887: Tokens Processed = 3637248\n",
      "Epoch 1, Step 887: Tokens Processed = 3637248\n",
      "Epoch 1, Step 888: Tokens Processed = 3641344\n",
      "Epoch 1, Step 888: Tokens Processed = 3641344\n",
      "Epoch 1, Step 889: Tokens Processed = 3645440\n",
      "Epoch 1, Step 889: Tokens Processed = 3645440\n",
      "Epoch 1, Step 890: Tokens Processed = 3649536\n",
      "Epoch 1, Step 890: Tokens Processed = 3649536\n",
      "Epoch 1, Step 891: Tokens Processed = 3653632\n",
      "Epoch 1, Step 891: Tokens Processed = 3653632\n",
      "Epoch 1, Step 892: Tokens Processed = 3657728\n",
      "Epoch 1, Step 892: Tokens Processed = 3657728\n",
      "Epoch 1, Step 893: Tokens Processed = 3661824\n",
      "Epoch 1, Step 893: Tokens Processed = 3661824\n",
      "Epoch 1, Step 894: Tokens Processed = 3665920\n",
      "Epoch 1, Step 894: Tokens Processed = 3665920\n",
      "Epoch 1, Step 895: Tokens Processed = 3670016\n",
      "Epoch 1, Step 895: Tokens Processed = 3670016\n",
      "Epoch 1, Step 896: Tokens Processed = 3674112\n",
      "Epoch 1, Step 896: Tokens Processed = 3674112\n",
      "Epoch 1, Step 897: Tokens Processed = 3678208\n",
      "Epoch 1, Step 897: Tokens Processed = 3678208\n",
      "Epoch 1, Step 898: Tokens Processed = 3682304\n",
      "Epoch 1, Step 898: Tokens Processed = 3682304\n",
      "Epoch 1, Step 899: Tokens Processed = 3686400\n",
      "Epoch 1, Step 899: Tokens Processed = 3686400\n",
      "Epoch 1, Step 900: Tokens Processed = 3690496\n",
      "Epoch 1, Step 900: Tokens Processed = 3690496\n",
      "Epoch 1, Step 900: Training Loss = 1.8110554218292236, Validation Loss = 5.229981899261475, Tokens Processed = 3690496\n",
      "Epoch 1, Step 900: Training Loss = 1.8110554218292236, Validation Loss = 5.229981899261475, Tokens Processed = 3690496\n",
      "Epoch 1, Step 901: Tokens Processed = 3694592\n",
      "Epoch 1, Step 901: Tokens Processed = 3694592\n",
      "Epoch 1, Step 902: Tokens Processed = 3698688\n",
      "Epoch 1, Step 902: Tokens Processed = 3698688\n",
      "Epoch 1, Step 903: Tokens Processed = 3702784\n",
      "Epoch 1, Step 903: Tokens Processed = 3702784\n",
      "Epoch 1, Step 904: Tokens Processed = 3706880\n",
      "Epoch 1, Step 904: Tokens Processed = 3706880\n",
      "Epoch 1, Step 905: Tokens Processed = 3710976\n",
      "Epoch 1, Step 905: Tokens Processed = 3710976\n",
      "Epoch 1, Step 906: Tokens Processed = 3715072\n",
      "Epoch 1, Step 906: Tokens Processed = 3715072\n",
      "Epoch 1, Step 907: Tokens Processed = 3719168\n",
      "Epoch 1, Step 907: Tokens Processed = 3719168\n",
      "Epoch 1, Step 908: Tokens Processed = 3723264\n",
      "Epoch 1, Step 908: Tokens Processed = 3723264\n",
      "Epoch 1, Step 909: Tokens Processed = 3727360\n",
      "Epoch 1, Step 909: Tokens Processed = 3727360\n",
      "Epoch 1, Step 910: Tokens Processed = 3731456\n",
      "Epoch 1, Step 910: Tokens Processed = 3731456\n",
      "Epoch 1, Step 911: Tokens Processed = 3735552\n",
      "Epoch 1, Step 911: Tokens Processed = 3735552\n",
      "Epoch 1, Step 912: Tokens Processed = 3739648\n",
      "Epoch 1, Step 912: Tokens Processed = 3739648\n",
      "Epoch 1, Step 913: Tokens Processed = 3743744\n",
      "Epoch 1, Step 913: Tokens Processed = 3743744\n",
      "Epoch 1, Step 914: Tokens Processed = 3747840\n",
      "Epoch 1, Step 914: Tokens Processed = 3747840\n",
      "Epoch 1, Step 915: Tokens Processed = 3751936\n",
      "Epoch 1, Step 915: Tokens Processed = 3751936\n",
      "Epoch 1, Step 916: Tokens Processed = 3756032\n",
      "Epoch 1, Step 916: Tokens Processed = 3756032\n",
      "Epoch 1, Step 917: Tokens Processed = 3760128\n",
      "Epoch 1, Step 917: Tokens Processed = 3760128\n",
      "Epoch 1, Step 918: Tokens Processed = 3764224\n",
      "Epoch 1, Step 918: Tokens Processed = 3764224\n",
      "Epoch 1, Step 919: Tokens Processed = 3768320\n",
      "Epoch 1, Step 919: Tokens Processed = 3768320\n",
      "Epoch 1, Step 920: Tokens Processed = 3772416\n",
      "Epoch 1, Step 920: Tokens Processed = 3772416\n",
      "Epoch 1, Step 921: Tokens Processed = 3776512\n",
      "Epoch 1, Step 921: Tokens Processed = 3776512\n",
      "Epoch 1, Step 922: Tokens Processed = 3780608\n",
      "Epoch 1, Step 922: Tokens Processed = 3780608\n",
      "Epoch 1, Step 923: Tokens Processed = 3784704\n",
      "Epoch 1, Step 923: Tokens Processed = 3784704\n",
      "Epoch 1, Step 924: Tokens Processed = 3788800\n",
      "Epoch 1, Step 924: Tokens Processed = 3788800\n",
      "Epoch 1, Step 925: Tokens Processed = 3792896\n",
      "Epoch 1, Step 925: Tokens Processed = 3792896\n",
      "Epoch 1, Step 926: Tokens Processed = 3796992\n",
      "Epoch 1, Step 926: Tokens Processed = 3796992\n",
      "Epoch 1, Step 927: Tokens Processed = 3801088\n",
      "Epoch 1, Step 927: Tokens Processed = 3801088\n",
      "Epoch 1, Step 928: Tokens Processed = 3805184\n",
      "Epoch 1, Step 928: Tokens Processed = 3805184\n",
      "Epoch 1, Step 929: Tokens Processed = 3809280\n",
      "Epoch 1, Step 929: Tokens Processed = 3809280\n",
      "Epoch 1, Step 930: Tokens Processed = 3813376\n",
      "Epoch 1, Step 930: Tokens Processed = 3813376\n",
      "Epoch 1, Step 931: Tokens Processed = 3817472\n",
      "Epoch 1, Step 931: Tokens Processed = 3817472\n",
      "Epoch 1, Step 932: Tokens Processed = 3821568\n",
      "Epoch 1, Step 932: Tokens Processed = 3821568\n",
      "Epoch 1, Step 933: Tokens Processed = 3825664\n",
      "Epoch 1, Step 933: Tokens Processed = 3825664\n",
      "Epoch 1, Step 934: Tokens Processed = 3829760\n",
      "Epoch 1, Step 934: Tokens Processed = 3829760\n",
      "Epoch 1, Step 935: Tokens Processed = 3833856\n",
      "Epoch 1, Step 935: Tokens Processed = 3833856\n",
      "Epoch 1, Step 936: Tokens Processed = 3837952\n",
      "Epoch 1, Step 936: Tokens Processed = 3837952\n",
      "Epoch 1, Step 937: Tokens Processed = 3842048\n",
      "Epoch 1, Step 937: Tokens Processed = 3842048\n",
      "Epoch 1, Step 938: Tokens Processed = 3846144\n",
      "Epoch 1, Step 938: Tokens Processed = 3846144\n",
      "Epoch 1, Step 939: Tokens Processed = 3850240\n",
      "Epoch 1, Step 939: Tokens Processed = 3850240\n",
      "Epoch 1, Step 940: Tokens Processed = 3854336\n",
      "Epoch 1, Step 940: Tokens Processed = 3854336\n",
      "Epoch 1, Step 941: Tokens Processed = 3858432\n",
      "Epoch 1, Step 941: Tokens Processed = 3858432\n",
      "Epoch 1, Step 942: Tokens Processed = 3862528\n",
      "Epoch 1, Step 942: Tokens Processed = 3862528\n",
      "Epoch 1, Step 943: Tokens Processed = 3866624\n",
      "Epoch 1, Step 943: Tokens Processed = 3866624\n",
      "Epoch 1, Step 944: Tokens Processed = 3870720\n",
      "Epoch 1, Step 944: Tokens Processed = 3870720\n",
      "Epoch 1, Step 945: Tokens Processed = 3874816\n",
      "Epoch 1, Step 945: Tokens Processed = 3874816\n",
      "Epoch 1, Step 946: Tokens Processed = 3878912\n",
      "Epoch 1, Step 946: Tokens Processed = 3878912\n",
      "Epoch 1, Step 947: Tokens Processed = 3883008\n",
      "Epoch 1, Step 947: Tokens Processed = 3883008\n",
      "Epoch 1, Step 948: Tokens Processed = 3887104\n",
      "Epoch 1, Step 948: Tokens Processed = 3887104\n",
      "Epoch 1, Step 949: Tokens Processed = 3891200\n",
      "Epoch 1, Step 949: Tokens Processed = 3891200\n",
      "Epoch 1, Step 950: Tokens Processed = 3895296\n",
      "Epoch 1, Step 950: Tokens Processed = 3895296\n",
      "Epoch 1, Step 951: Tokens Processed = 3899392\n",
      "Epoch 1, Step 951: Tokens Processed = 3899392\n",
      "Epoch 1, Step 952: Tokens Processed = 3903488\n",
      "Epoch 1, Step 952: Tokens Processed = 3903488\n",
      "Epoch 1, Step 953: Tokens Processed = 3907584\n",
      "Epoch 1, Step 953: Tokens Processed = 3907584\n",
      "Epoch 1, Step 954: Tokens Processed = 3911680\n",
      "Epoch 1, Step 954: Tokens Processed = 3911680\n",
      "Epoch 1, Step 955: Tokens Processed = 3915776\n",
      "Epoch 1, Step 955: Tokens Processed = 3915776\n",
      "Epoch 1, Step 956: Tokens Processed = 3919872\n",
      "Epoch 1, Step 956: Tokens Processed = 3919872\n",
      "Epoch 1, Step 957: Tokens Processed = 3923968\n",
      "Epoch 1, Step 957: Tokens Processed = 3923968\n",
      "Epoch 1, Step 958: Tokens Processed = 3928064\n",
      "Epoch 1, Step 958: Tokens Processed = 3928064\n",
      "Epoch 1, Step 959: Tokens Processed = 3932160\n",
      "Epoch 1, Step 959: Tokens Processed = 3932160\n",
      "Epoch 1, Step 960: Tokens Processed = 3936256\n",
      "Epoch 1, Step 960: Tokens Processed = 3936256\n",
      "Epoch 1, Step 961: Tokens Processed = 3940352\n",
      "Epoch 1, Step 961: Tokens Processed = 3940352\n",
      "Epoch 1, Step 962: Tokens Processed = 3944448\n",
      "Epoch 1, Step 962: Tokens Processed = 3944448\n",
      "Epoch 1, Step 963: Tokens Processed = 3948544\n",
      "Epoch 1, Step 963: Tokens Processed = 3948544\n",
      "Epoch 1, Step 964: Tokens Processed = 3952640\n",
      "Epoch 1, Step 964: Tokens Processed = 3952640\n",
      "Epoch 1, Step 965: Tokens Processed = 3956736\n",
      "Epoch 1, Step 965: Tokens Processed = 3956736\n",
      "Epoch 1, Step 966: Tokens Processed = 3960832\n",
      "Epoch 1, Step 966: Tokens Processed = 3960832\n",
      "Epoch 1, Step 967: Tokens Processed = 3964928\n",
      "Epoch 1, Step 967: Tokens Processed = 3964928\n",
      "Epoch 1, Step 968: Tokens Processed = 3969024\n",
      "Epoch 1, Step 968: Tokens Processed = 3969024\n",
      "Epoch 1, Step 969: Tokens Processed = 3973120\n",
      "Epoch 1, Step 969: Tokens Processed = 3973120\n",
      "Epoch 1, Step 970: Tokens Processed = 3977216\n",
      "Epoch 1, Step 970: Tokens Processed = 3977216\n",
      "Epoch 1, Step 971: Tokens Processed = 3981312\n",
      "Epoch 1, Step 971: Tokens Processed = 3981312\n",
      "Epoch 1, Step 972: Tokens Processed = 3985408\n",
      "Epoch 1, Step 972: Tokens Processed = 3985408\n",
      "Epoch 1, Step 973: Tokens Processed = 3989504\n",
      "Epoch 1, Step 973: Tokens Processed = 3989504\n",
      "Epoch 1, Step 974: Tokens Processed = 3993600\n",
      "Epoch 1, Step 974: Tokens Processed = 3993600\n",
      "Epoch 1, Step 975: Tokens Processed = 3997696\n",
      "Epoch 1, Step 975: Tokens Processed = 3997696\n",
      "Epoch 1, Step 976: Tokens Processed = 4001792\n",
      "Epoch 1, Step 976: Tokens Processed = 4001792\n",
      "Epoch 1, Step 977: Tokens Processed = 4005888\n",
      "Epoch 1, Step 977: Tokens Processed = 4005888\n",
      "Epoch 1, Step 978: Tokens Processed = 4009984\n",
      "Epoch 1, Step 978: Tokens Processed = 4009984\n",
      "Epoch 1, Step 979: Tokens Processed = 4014080\n",
      "Epoch 1, Step 979: Tokens Processed = 4014080\n",
      "Epoch 1, Step 980: Tokens Processed = 4018176\n",
      "Epoch 1, Step 980: Tokens Processed = 4018176\n",
      "Epoch 1, Step 981: Tokens Processed = 4022272\n",
      "Epoch 1, Step 981: Tokens Processed = 4022272\n",
      "Epoch 1, Step 982: Tokens Processed = 4026368\n",
      "Epoch 1, Step 982: Tokens Processed = 4026368\n",
      "Epoch 1, Step 983: Tokens Processed = 4030464\n",
      "Epoch 1, Step 983: Tokens Processed = 4030464\n",
      "Epoch 1, Step 984: Tokens Processed = 4034560\n",
      "Epoch 1, Step 984: Tokens Processed = 4034560\n",
      "Epoch 1, Step 985: Tokens Processed = 4038656\n",
      "Epoch 1, Step 985: Tokens Processed = 4038656\n",
      "Epoch 1, Step 986: Tokens Processed = 4042752\n",
      "Epoch 1, Step 986: Tokens Processed = 4042752\n",
      "Epoch 1, Step 987: Tokens Processed = 4046848\n",
      "Epoch 1, Step 987: Tokens Processed = 4046848\n",
      "Epoch 1, Step 988: Tokens Processed = 4050944\n",
      "Epoch 1, Step 988: Tokens Processed = 4050944\n",
      "Epoch 1, Step 989: Tokens Processed = 4055040\n",
      "Epoch 1, Step 989: Tokens Processed = 4055040\n",
      "Epoch 1, Step 990: Tokens Processed = 4059136\n",
      "Epoch 1, Step 990: Tokens Processed = 4059136\n",
      "Epoch 1, Step 991: Tokens Processed = 4063232\n",
      "Epoch 1, Step 991: Tokens Processed = 4063232\n",
      "Epoch 1, Step 992: Tokens Processed = 4067328\n",
      "Epoch 1, Step 992: Tokens Processed = 4067328\n",
      "Epoch 1, Step 993: Tokens Processed = 4071424\n",
      "Epoch 1, Step 993: Tokens Processed = 4071424\n",
      "Epoch 1, Step 994: Tokens Processed = 4075520\n",
      "Epoch 1, Step 994: Tokens Processed = 4075520\n",
      "Epoch 1, Step 995: Tokens Processed = 4079616\n",
      "Epoch 1, Step 995: Tokens Processed = 4079616\n",
      "Epoch 1, Step 996: Tokens Processed = 4083712\n",
      "Epoch 1, Step 996: Tokens Processed = 4083712\n",
      "Epoch 1, Step 997: Tokens Processed = 4087808\n",
      "Epoch 1, Step 997: Tokens Processed = 4087808\n",
      "Epoch 1, Step 998: Tokens Processed = 4091904\n",
      "Epoch 1, Step 998: Tokens Processed = 4091904\n",
      "Epoch 1, Step 999: Tokens Processed = 4096000\n",
      "Epoch 1, Step 999: Tokens Processed = 4096000\n",
      "Epoch 1, Step 1000: Tokens Processed = 4100096\n",
      "Epoch 1, Step 1000: Tokens Processed = 4100096\n",
      "Epoch 1, Step 1000: Training Loss = 1.8279298543930054, Validation Loss = 4.667431831359863, Tokens Processed = 4100096\n",
      "Epoch 1, Step 1000: Training Loss = 1.8279298543930054, Validation Loss = 4.667431831359863, Tokens Processed = 4100096\n",
      "Epoch 1, Step 1001: Tokens Processed = 4104192\n",
      "Epoch 1, Step 1001: Tokens Processed = 4104192\n",
      "Epoch 1, Step 1002: Tokens Processed = 4108288\n",
      "Epoch 1, Step 1002: Tokens Processed = 4108288\n",
      "Epoch 1, Step 1003: Tokens Processed = 4112384\n",
      "Epoch 1, Step 1003: Tokens Processed = 4112384\n",
      "Epoch 1, Step 1004: Tokens Processed = 4116480\n",
      "Epoch 1, Step 1004: Tokens Processed = 4116480\n",
      "Epoch 1, Step 1005: Tokens Processed = 4120576\n",
      "Epoch 1, Step 1005: Tokens Processed = 4120576\n",
      "Epoch 1, Step 1006: Tokens Processed = 4124672\n",
      "Epoch 1, Step 1006: Tokens Processed = 4124672\n",
      "Epoch 1, Step 1007: Tokens Processed = 4128768\n",
      "Epoch 1, Step 1007: Tokens Processed = 4128768\n",
      "Epoch 1, Step 1008: Tokens Processed = 4132864\n",
      "Epoch 1, Step 1008: Tokens Processed = 4132864\n",
      "Epoch 1, Step 1009: Tokens Processed = 4136960\n",
      "Epoch 1, Step 1009: Tokens Processed = 4136960\n",
      "Epoch 1, Step 1010: Tokens Processed = 4141056\n",
      "Epoch 1, Step 1010: Tokens Processed = 4141056\n",
      "Epoch 1, Step 1011: Tokens Processed = 4145152\n",
      "Epoch 1, Step 1011: Tokens Processed = 4145152\n",
      "Epoch 1, Step 1012: Tokens Processed = 4149248\n",
      "Epoch 1, Step 1012: Tokens Processed = 4149248\n",
      "Epoch 1, Step 1013: Tokens Processed = 4153344\n",
      "Epoch 1, Step 1013: Tokens Processed = 4153344\n",
      "Epoch 1, Step 1014: Tokens Processed = 4157440\n",
      "Epoch 1, Step 1014: Tokens Processed = 4157440\n",
      "Epoch 1, Step 1015: Tokens Processed = 4161536\n",
      "Epoch 1, Step 1015: Tokens Processed = 4161536\n",
      "Epoch 1, Step 1016: Tokens Processed = 4165632\n",
      "Epoch 1, Step 1016: Tokens Processed = 4165632\n",
      "Epoch 1, Step 1017: Tokens Processed = 4169728\n",
      "Epoch 1, Step 1017: Tokens Processed = 4169728\n",
      "Epoch 1, Step 1018: Tokens Processed = 4173824\n",
      "Epoch 1, Step 1018: Tokens Processed = 4173824\n",
      "Epoch 1, Step 1019: Tokens Processed = 4177920\n",
      "Epoch 1, Step 1019: Tokens Processed = 4177920\n",
      "Epoch 1, Step 1020: Tokens Processed = 4182016\n",
      "Epoch 1, Step 1020: Tokens Processed = 4182016\n",
      "Epoch 1, Step 1021: Tokens Processed = 4186112\n",
      "Epoch 1, Step 1021: Tokens Processed = 4186112\n",
      "Epoch 1, Step 1022: Tokens Processed = 4190208\n",
      "Epoch 1, Step 1022: Tokens Processed = 4190208\n",
      "Epoch 1, Step 1023: Tokens Processed = 4194304\n",
      "Epoch 1, Step 1023: Tokens Processed = 4194304\n",
      "Epoch 1, Step 1024: Tokens Processed = 4198400\n",
      "Epoch 1, Step 1024: Tokens Processed = 4198400\n",
      "Epoch 1, Step 1025: Tokens Processed = 4202496\n",
      "Epoch 1, Step 1025: Tokens Processed = 4202496\n",
      "Epoch 1, Step 1026: Tokens Processed = 4206592\n",
      "Epoch 1, Step 1026: Tokens Processed = 4206592\n",
      "Epoch 1, Step 1027: Tokens Processed = 4210688\n",
      "Epoch 1, Step 1027: Tokens Processed = 4210688\n",
      "Epoch 1, Step 1028: Tokens Processed = 4214784\n",
      "Epoch 1, Step 1028: Tokens Processed = 4214784\n",
      "Epoch 1, Step 1029: Tokens Processed = 4218880\n",
      "Epoch 1, Step 1029: Tokens Processed = 4218880\n",
      "Epoch 1, Step 1030: Tokens Processed = 4222976\n",
      "Epoch 1, Step 1030: Tokens Processed = 4222976\n",
      "Epoch 1, Step 1031: Tokens Processed = 4227072\n",
      "Epoch 1, Step 1031: Tokens Processed = 4227072\n",
      "Epoch 1, Step 1032: Tokens Processed = 4231168\n",
      "Epoch 1, Step 1032: Tokens Processed = 4231168\n",
      "Epoch 1, Step 1033: Tokens Processed = 4235264\n",
      "Epoch 1, Step 1033: Tokens Processed = 4235264\n",
      "Epoch 1, Step 1034: Tokens Processed = 4239360\n",
      "Epoch 1, Step 1034: Tokens Processed = 4239360\n",
      "Epoch 1, Step 1035: Tokens Processed = 4243456\n",
      "Epoch 1, Step 1035: Tokens Processed = 4243456\n",
      "Epoch 1, Step 1036: Tokens Processed = 4247552\n",
      "Epoch 1, Step 1036: Tokens Processed = 4247552\n",
      "Epoch 1, Step 1037: Tokens Processed = 4251648\n",
      "Epoch 1, Step 1037: Tokens Processed = 4251648\n",
      "Epoch 1, Step 1038: Tokens Processed = 4255744\n",
      "Epoch 1, Step 1038: Tokens Processed = 4255744\n",
      "Epoch 1, Step 1039: Tokens Processed = 4259840\n",
      "Epoch 1, Step 1039: Tokens Processed = 4259840\n",
      "Epoch 1, Step 1040: Tokens Processed = 4263936\n",
      "Epoch 1, Step 1040: Tokens Processed = 4263936\n",
      "Epoch 1, Step 1041: Tokens Processed = 4268032\n",
      "Epoch 1, Step 1041: Tokens Processed = 4268032\n",
      "Epoch 1, Step 1042: Tokens Processed = 4272128\n",
      "Epoch 1, Step 1042: Tokens Processed = 4272128\n",
      "Epoch 1, Step 1043: Tokens Processed = 4276224\n",
      "Epoch 1, Step 1043: Tokens Processed = 4276224\n",
      "Epoch 1, Step 1044: Tokens Processed = 4280320\n",
      "Epoch 1, Step 1044: Tokens Processed = 4280320\n",
      "Epoch 1, Step 1045: Tokens Processed = 4284416\n",
      "Epoch 1, Step 1045: Tokens Processed = 4284416\n",
      "Epoch 1, Step 1046: Tokens Processed = 4288512\n",
      "Epoch 1, Step 1046: Tokens Processed = 4288512\n",
      "Epoch 1, Step 1047: Tokens Processed = 4292608\n",
      "Epoch 1, Step 1047: Tokens Processed = 4292608\n",
      "Epoch 1, Step 1048: Tokens Processed = 4296704\n",
      "Epoch 1, Step 1048: Tokens Processed = 4296704\n",
      "Epoch 1, Step 1049: Tokens Processed = 4300800\n",
      "Epoch 1, Step 1049: Tokens Processed = 4300800\n",
      "Epoch 1, Step 1050: Tokens Processed = 4304896\n",
      "Epoch 1, Step 1050: Tokens Processed = 4304896\n",
      "Epoch 1, Step 1051: Tokens Processed = 4308992\n",
      "Epoch 1, Step 1051: Tokens Processed = 4308992\n",
      "Epoch 1, Step 1052: Tokens Processed = 4313088\n",
      "Epoch 1, Step 1052: Tokens Processed = 4313088\n",
      "Epoch 1, Step 1053: Tokens Processed = 4317184\n",
      "Epoch 1, Step 1053: Tokens Processed = 4317184\n",
      "Epoch 1, Step 1054: Tokens Processed = 4321280\n",
      "Epoch 1, Step 1054: Tokens Processed = 4321280\n",
      "Epoch 1, Step 1055: Tokens Processed = 4325376\n",
      "Epoch 1, Step 1055: Tokens Processed = 4325376\n",
      "Epoch 1, Step 1056: Tokens Processed = 4329472\n",
      "Epoch 1, Step 1056: Tokens Processed = 4329472\n",
      "Epoch 1, Step 1057: Tokens Processed = 4333568\n",
      "Epoch 1, Step 1057: Tokens Processed = 4333568\n",
      "Epoch 1, Step 1058: Tokens Processed = 4337664\n",
      "Epoch 1, Step 1058: Tokens Processed = 4337664\n",
      "Epoch 1, Step 1059: Tokens Processed = 4341760\n",
      "Epoch 1, Step 1059: Tokens Processed = 4341760\n",
      "Epoch 1, Step 1060: Tokens Processed = 4345856\n",
      "Epoch 1, Step 1060: Tokens Processed = 4345856\n",
      "Epoch 1, Step 1061: Tokens Processed = 4349952\n",
      "Epoch 1, Step 1061: Tokens Processed = 4349952\n",
      "Epoch 1, Step 1062: Tokens Processed = 4354048\n",
      "Epoch 1, Step 1062: Tokens Processed = 4354048\n",
      "Epoch 1, Step 1063: Tokens Processed = 4358144\n",
      "Epoch 1, Step 1063: Tokens Processed = 4358144\n",
      "Epoch 1, Step 1064: Tokens Processed = 4362240\n",
      "Epoch 1, Step 1064: Tokens Processed = 4362240\n",
      "Epoch 1, Step 1065: Tokens Processed = 4366336\n",
      "Epoch 1, Step 1065: Tokens Processed = 4366336\n",
      "Epoch 1, Step 1066: Tokens Processed = 4370432\n",
      "Epoch 1, Step 1066: Tokens Processed = 4370432\n",
      "Epoch 1, Step 1067: Tokens Processed = 4374528\n",
      "Epoch 1, Step 1067: Tokens Processed = 4374528\n",
      "Epoch 1, Step 1068: Tokens Processed = 4378624\n",
      "Epoch 1, Step 1068: Tokens Processed = 4378624\n",
      "Epoch 1, Step 1069: Tokens Processed = 4382720\n",
      "Epoch 1, Step 1069: Tokens Processed = 4382720\n",
      "Epoch 1, Step 1070: Tokens Processed = 4386816\n",
      "Epoch 1, Step 1070: Tokens Processed = 4386816\n",
      "Epoch 1, Step 1071: Tokens Processed = 4390912\n",
      "Epoch 1, Step 1071: Tokens Processed = 4390912\n",
      "Epoch 1, Step 1072: Tokens Processed = 4395008\n",
      "Epoch 1, Step 1072: Tokens Processed = 4395008\n",
      "Epoch 1, Step 1073: Tokens Processed = 4399104\n",
      "Epoch 1, Step 1073: Tokens Processed = 4399104\n",
      "Epoch 1, Step 1074: Tokens Processed = 4403200\n",
      "Epoch 1, Step 1074: Tokens Processed = 4403200\n",
      "Epoch 1, Step 1075: Tokens Processed = 4407296\n",
      "Epoch 1, Step 1075: Tokens Processed = 4407296\n",
      "Epoch 1, Step 1076: Tokens Processed = 4411392\n",
      "Epoch 1, Step 1076: Tokens Processed = 4411392\n",
      "Epoch 1, Step 1077: Tokens Processed = 4415488\n",
      "Epoch 1, Step 1077: Tokens Processed = 4415488\n",
      "Epoch 1, Step 1078: Tokens Processed = 4419584\n",
      "Epoch 1, Step 1078: Tokens Processed = 4419584\n",
      "Epoch 1, Step 1079: Tokens Processed = 4423680\n",
      "Epoch 1, Step 1079: Tokens Processed = 4423680\n",
      "Epoch 1, Step 1080: Tokens Processed = 4427776\n",
      "Epoch 1, Step 1080: Tokens Processed = 4427776\n",
      "Epoch 1, Step 1081: Tokens Processed = 4431872\n",
      "Epoch 1, Step 1081: Tokens Processed = 4431872\n",
      "Epoch 1, Step 1082: Tokens Processed = 4435968\n",
      "Epoch 1, Step 1082: Tokens Processed = 4435968\n",
      "Epoch 1, Step 1083: Tokens Processed = 4440064\n",
      "Epoch 1, Step 1083: Tokens Processed = 4440064\n",
      "Epoch 1, Step 1084: Tokens Processed = 4444160\n",
      "Epoch 1, Step 1084: Tokens Processed = 4444160\n",
      "Epoch 1, Step 1085: Tokens Processed = 4448256\n",
      "Epoch 1, Step 1085: Tokens Processed = 4448256\n",
      "Epoch 1, Step 1086: Tokens Processed = 4452352\n",
      "Epoch 1, Step 1086: Tokens Processed = 4452352\n",
      "Epoch 1, Step 1087: Tokens Processed = 4456448\n",
      "Epoch 1, Step 1087: Tokens Processed = 4456448\n",
      "Epoch 1, Step 1088: Tokens Processed = 4460544\n",
      "Epoch 1, Step 1088: Tokens Processed = 4460544\n",
      "Epoch 1, Step 1089: Tokens Processed = 4464640\n",
      "Epoch 1, Step 1089: Tokens Processed = 4464640\n",
      "Epoch 1, Step 1090: Tokens Processed = 4468736\n",
      "Epoch 1, Step 1090: Tokens Processed = 4468736\n",
      "Epoch 1, Step 1091: Tokens Processed = 4472832\n",
      "Epoch 1, Step 1091: Tokens Processed = 4472832\n",
      "Epoch 1, Step 1092: Tokens Processed = 4476928\n",
      "Epoch 1, Step 1092: Tokens Processed = 4476928\n",
      "Epoch 1, Step 1093: Tokens Processed = 4481024\n",
      "Epoch 1, Step 1093: Tokens Processed = 4481024\n",
      "Epoch 1, Step 1094: Tokens Processed = 4485120\n",
      "Epoch 1, Step 1094: Tokens Processed = 4485120\n",
      "Epoch 1, Step 1095: Tokens Processed = 4489216\n",
      "Epoch 1, Step 1095: Tokens Processed = 4489216\n",
      "Epoch 1, Step 1096: Tokens Processed = 4493312\n",
      "Epoch 1, Step 1096: Tokens Processed = 4493312\n",
      "Epoch 1, Step 1097: Tokens Processed = 4497408\n",
      "Epoch 1, Step 1097: Tokens Processed = 4497408\n",
      "Epoch 1, Step 1098: Tokens Processed = 4501504\n",
      "Epoch 1, Step 1098: Tokens Processed = 4501504\n",
      "Epoch 1, Step 1099: Tokens Processed = 4505600\n",
      "Epoch 1, Step 1099: Tokens Processed = 4505600\n",
      "Epoch 1, Step 1100: Tokens Processed = 4509696\n",
      "Epoch 1, Step 1100: Tokens Processed = 4509696\n",
      "Epoch 1, Step 1100: Training Loss = 2.0907214879989624, Validation Loss = 5.661266326904297, Tokens Processed = 4509696\n",
      "Epoch 1, Step 1100: Training Loss = 2.0907214879989624, Validation Loss = 5.661266326904297, Tokens Processed = 4509696\n",
      "Epoch 1, Step 1101: Tokens Processed = 4513792\n",
      "Epoch 1, Step 1101: Tokens Processed = 4513792\n",
      "Epoch 1, Step 1102: Tokens Processed = 4517888\n",
      "Epoch 1, Step 1102: Tokens Processed = 4517888\n",
      "Epoch 1, Step 1103: Tokens Processed = 4521984\n",
      "Epoch 1, Step 1103: Tokens Processed = 4521984\n",
      "Epoch 1, Step 1104: Tokens Processed = 4526080\n",
      "Epoch 1, Step 1104: Tokens Processed = 4526080\n",
      "Epoch 1, Step 1105: Tokens Processed = 4530176\n",
      "Epoch 1, Step 1105: Tokens Processed = 4530176\n",
      "Epoch 1, Step 1106: Tokens Processed = 4534272\n",
      "Epoch 1, Step 1106: Tokens Processed = 4534272\n",
      "Epoch 1, Step 1107: Tokens Processed = 4538368\n",
      "Epoch 1, Step 1107: Tokens Processed = 4538368\n",
      "Epoch 1, Step 1108: Tokens Processed = 4542464\n",
      "Epoch 1, Step 1108: Tokens Processed = 4542464\n",
      "Epoch 1, Step 1109: Tokens Processed = 4546560\n",
      "Epoch 1, Step 1109: Tokens Processed = 4546560\n",
      "Epoch 1, Step 1110: Tokens Processed = 4550656\n",
      "Epoch 1, Step 1110: Tokens Processed = 4550656\n",
      "Epoch 1, Step 1111: Tokens Processed = 4554752\n",
      "Epoch 1, Step 1111: Tokens Processed = 4554752\n",
      "Epoch 1, Step 1112: Tokens Processed = 4558848\n",
      "Epoch 1, Step 1112: Tokens Processed = 4558848\n",
      "Epoch 1, Step 1113: Tokens Processed = 4562944\n",
      "Epoch 1, Step 1113: Tokens Processed = 4562944\n",
      "Epoch 1, Step 1114: Tokens Processed = 4567040\n",
      "Epoch 1, Step 1114: Tokens Processed = 4567040\n",
      "Epoch 1, Step 1115: Tokens Processed = 4571136\n",
      "Epoch 1, Step 1115: Tokens Processed = 4571136\n",
      "Epoch 1, Step 1116: Tokens Processed = 4575232\n",
      "Epoch 1, Step 1116: Tokens Processed = 4575232\n",
      "Epoch 1, Step 1117: Tokens Processed = 4579328\n",
      "Epoch 1, Step 1117: Tokens Processed = 4579328\n",
      "Epoch 1, Step 1118: Tokens Processed = 4583424\n",
      "Epoch 1, Step 1118: Tokens Processed = 4583424\n",
      "Epoch 1, Step 1119: Tokens Processed = 4587520\n",
      "Epoch 1, Step 1119: Tokens Processed = 4587520\n",
      "Epoch 1, Step 1120: Tokens Processed = 4591616\n",
      "Epoch 1, Step 1120: Tokens Processed = 4591616\n",
      "Epoch 1, Step 1121: Tokens Processed = 4595712\n",
      "Epoch 1, Step 1121: Tokens Processed = 4595712\n",
      "Epoch 1, Step 1122: Tokens Processed = 4599808\n",
      "Epoch 1, Step 1122: Tokens Processed = 4599808\n",
      "Epoch 1, Step 1123: Tokens Processed = 4603904\n",
      "Epoch 1, Step 1123: Tokens Processed = 4603904\n",
      "Epoch 1, Step 1124: Tokens Processed = 4608000\n",
      "Epoch 1, Step 1124: Tokens Processed = 4608000\n",
      "Epoch 1, Step 1125: Tokens Processed = 4612096\n",
      "Epoch 1, Step 1125: Tokens Processed = 4612096\n",
      "Epoch 1, Step 1126: Tokens Processed = 4616192\n",
      "Epoch 1, Step 1126: Tokens Processed = 4616192\n",
      "Epoch 1, Step 1127: Tokens Processed = 4620288\n",
      "Epoch 1, Step 1127: Tokens Processed = 4620288\n",
      "Epoch 1, Step 1128: Tokens Processed = 4624384\n",
      "Epoch 1, Step 1128: Tokens Processed = 4624384\n",
      "Epoch 1, Step 1129: Tokens Processed = 4628480\n",
      "Epoch 1, Step 1129: Tokens Processed = 4628480\n",
      "Epoch 1, Step 1130: Tokens Processed = 4632576\n",
      "Epoch 1, Step 1130: Tokens Processed = 4632576\n",
      "Epoch 1, Step 1131: Tokens Processed = 4636672\n",
      "Epoch 1, Step 1131: Tokens Processed = 4636672\n",
      "Epoch 1, Step 1132: Tokens Processed = 4640768\n",
      "Epoch 1, Step 1132: Tokens Processed = 4640768\n",
      "Epoch 1, Step 1133: Tokens Processed = 4644864\n",
      "Epoch 1, Step 1133: Tokens Processed = 4644864\n",
      "Epoch 1, Step 1134: Tokens Processed = 4648960\n",
      "Epoch 1, Step 1134: Tokens Processed = 4648960\n",
      "Epoch 1, Step 1135: Tokens Processed = 4653056\n",
      "Epoch 1, Step 1135: Tokens Processed = 4653056\n",
      "Epoch 1, Step 1136: Tokens Processed = 4657152\n",
      "Epoch 1, Step 1136: Tokens Processed = 4657152\n",
      "Epoch 1, Step 1137: Tokens Processed = 4661248\n",
      "Epoch 1, Step 1137: Tokens Processed = 4661248\n",
      "Epoch 1, Step 1138: Tokens Processed = 4665344\n",
      "Epoch 1, Step 1138: Tokens Processed = 4665344\n",
      "Epoch 1, Step 1139: Tokens Processed = 4669440\n",
      "Epoch 1, Step 1139: Tokens Processed = 4669440\n",
      "Epoch 1, Step 1140: Tokens Processed = 4673536\n",
      "Epoch 1, Step 1140: Tokens Processed = 4673536\n",
      "Epoch 1, Step 1141: Tokens Processed = 4677632\n",
      "Epoch 1, Step 1141: Tokens Processed = 4677632\n",
      "Epoch 1, Step 1142: Tokens Processed = 4681728\n",
      "Epoch 1, Step 1142: Tokens Processed = 4681728\n",
      "Epoch 1, Step 1143: Tokens Processed = 4685824\n",
      "Epoch 1, Step 1143: Tokens Processed = 4685824\n",
      "Epoch 1, Step 1144: Tokens Processed = 4689920\n",
      "Epoch 1, Step 1144: Tokens Processed = 4689920\n",
      "Epoch 1, Step 1145: Tokens Processed = 4694016\n",
      "Epoch 1, Step 1145: Tokens Processed = 4694016\n",
      "Epoch 1, Step 1146: Tokens Processed = 4698112\n",
      "Epoch 1, Step 1146: Tokens Processed = 4698112\n",
      "Epoch 1, Step 1147: Tokens Processed = 4702208\n",
      "Epoch 1, Step 1147: Tokens Processed = 4702208\n",
      "Epoch 1, Step 1148: Tokens Processed = 4706304\n",
      "Epoch 1, Step 1148: Tokens Processed = 4706304\n",
      "Epoch 1, Step 1149: Tokens Processed = 4710400\n",
      "Epoch 1, Step 1149: Tokens Processed = 4710400\n",
      "Epoch 1, Step 1150: Tokens Processed = 4714496\n",
      "Epoch 1, Step 1150: Tokens Processed = 4714496\n",
      "Epoch 1, Step 1151: Tokens Processed = 4718592\n",
      "Epoch 1, Step 1151: Tokens Processed = 4718592\n",
      "Epoch 1, Step 1152: Tokens Processed = 4722688\n",
      "Epoch 1, Step 1152: Tokens Processed = 4722688\n",
      "Epoch 1, Step 1153: Tokens Processed = 4726784\n",
      "Epoch 1, Step 1153: Tokens Processed = 4726784\n",
      "Epoch 1, Step 1154: Tokens Processed = 4730880\n",
      "Epoch 1, Step 1154: Tokens Processed = 4730880\n",
      "Epoch 1, Step 1155: Tokens Processed = 4734976\n",
      "Epoch 1, Step 1155: Tokens Processed = 4734976\n",
      "Epoch 1, Step 1156: Tokens Processed = 4739072\n",
      "Epoch 1, Step 1156: Tokens Processed = 4739072\n",
      "Epoch 1, Step 1157: Tokens Processed = 4743168\n",
      "Epoch 1, Step 1157: Tokens Processed = 4743168\n",
      "Epoch 1, Step 1158: Tokens Processed = 4747264\n",
      "Epoch 1, Step 1158: Tokens Processed = 4747264\n",
      "Epoch 1, Step 1159: Tokens Processed = 4751360\n",
      "Epoch 1, Step 1159: Tokens Processed = 4751360\n",
      "Epoch 1, Step 1160: Tokens Processed = 4755456\n",
      "Epoch 1, Step 1160: Tokens Processed = 4755456\n",
      "Epoch 1, Step 1161: Tokens Processed = 4759552\n",
      "Epoch 1, Step 1161: Tokens Processed = 4759552\n",
      "Epoch 1, Step 1162: Tokens Processed = 4763648\n",
      "Epoch 1, Step 1162: Tokens Processed = 4763648\n",
      "Epoch 1, Step 1163: Tokens Processed = 4767744\n",
      "Epoch 1, Step 1163: Tokens Processed = 4767744\n",
      "Epoch 1, Step 1164: Tokens Processed = 4771840\n",
      "Epoch 1, Step 1164: Tokens Processed = 4771840\n",
      "Epoch 1, Step 1165: Tokens Processed = 4775936\n",
      "Epoch 1, Step 1165: Tokens Processed = 4775936\n",
      "Epoch 1, Step 1166: Tokens Processed = 4780032\n",
      "Epoch 1, Step 1166: Tokens Processed = 4780032\n",
      "Epoch 1, Step 1167: Tokens Processed = 4784128\n",
      "Epoch 1, Step 1167: Tokens Processed = 4784128\n",
      "Epoch 1, Step 1168: Tokens Processed = 4788224\n",
      "Epoch 1, Step 1168: Tokens Processed = 4788224\n",
      "Epoch 1, Step 1169: Tokens Processed = 4792320\n",
      "Epoch 1, Step 1169: Tokens Processed = 4792320\n",
      "Epoch 1, Step 1170: Tokens Processed = 4796416\n",
      "Epoch 1, Step 1170: Tokens Processed = 4796416\n",
      "Epoch 1, Step 1171: Tokens Processed = 4800512\n",
      "Epoch 1, Step 1171: Tokens Processed = 4800512\n",
      "Epoch 1, Step 1172: Tokens Processed = 4804608\n",
      "Epoch 1, Step 1172: Tokens Processed = 4804608\n",
      "Epoch 1, Step 1173: Tokens Processed = 4808704\n",
      "Epoch 1, Step 1173: Tokens Processed = 4808704\n",
      "Epoch 1, Step 1174: Tokens Processed = 4812800\n",
      "Epoch 1, Step 1174: Tokens Processed = 4812800\n",
      "Epoch 1, Step 1175: Tokens Processed = 4816896\n",
      "Epoch 1, Step 1175: Tokens Processed = 4816896\n",
      "Epoch 1, Step 1176: Tokens Processed = 4820992\n",
      "Epoch 1, Step 1176: Tokens Processed = 4820992\n",
      "Epoch 1, Step 1177: Tokens Processed = 4825088\n",
      "Epoch 1, Step 1177: Tokens Processed = 4825088\n",
      "Epoch 1, Step 1178: Tokens Processed = 4829184\n",
      "Epoch 1, Step 1178: Tokens Processed = 4829184\n",
      "Epoch 1, Step 1179: Tokens Processed = 4833280\n",
      "Epoch 1, Step 1179: Tokens Processed = 4833280\n",
      "Epoch 1, Step 1180: Tokens Processed = 4837376\n",
      "Epoch 1, Step 1180: Tokens Processed = 4837376\n",
      "Epoch 1, Step 1181: Tokens Processed = 4841472\n",
      "Epoch 1, Step 1181: Tokens Processed = 4841472\n",
      "Epoch 1, Step 1182: Tokens Processed = 4845568\n",
      "Epoch 1, Step 1182: Tokens Processed = 4845568\n",
      "Epoch 1, Step 1183: Tokens Processed = 4849664\n",
      "Epoch 1, Step 1183: Tokens Processed = 4849664\n",
      "Epoch 1, Step 1184: Tokens Processed = 4853760\n",
      "Epoch 1, Step 1184: Tokens Processed = 4853760\n",
      "Epoch 1, Step 1185: Tokens Processed = 4857856\n",
      "Epoch 1, Step 1185: Tokens Processed = 4857856\n",
      "Epoch 1, Step 1186: Tokens Processed = 4861952\n",
      "Epoch 1, Step 1186: Tokens Processed = 4861952\n",
      "Epoch 1, Step 1187: Tokens Processed = 4866048\n",
      "Epoch 1, Step 1187: Tokens Processed = 4866048\n",
      "Epoch 1, Step 1188: Tokens Processed = 4870144\n",
      "Epoch 1, Step 1188: Tokens Processed = 4870144\n",
      "Epoch 1, Step 1189: Tokens Processed = 4874240\n",
      "Epoch 1, Step 1189: Tokens Processed = 4874240\n",
      "Epoch 1, Step 1190: Tokens Processed = 4878336\n",
      "Epoch 1, Step 1190: Tokens Processed = 4878336\n",
      "Epoch 1, Step 1191: Tokens Processed = 4882432\n",
      "Epoch 1, Step 1191: Tokens Processed = 4882432\n",
      "Epoch 1, Step 1192: Tokens Processed = 4886528\n",
      "Epoch 1, Step 1192: Tokens Processed = 4886528\n",
      "Epoch 1, Step 1193: Tokens Processed = 4890624\n",
      "Epoch 1, Step 1193: Tokens Processed = 4890624\n",
      "Epoch 1, Step 1194: Tokens Processed = 4894720\n",
      "Epoch 1, Step 1194: Tokens Processed = 4894720\n",
      "Epoch 1, Step 1195: Tokens Processed = 4898816\n",
      "Epoch 1, Step 1195: Tokens Processed = 4898816\n",
      "Epoch 1, Step 1196: Tokens Processed = 4902912\n",
      "Epoch 1, Step 1196: Tokens Processed = 4902912\n",
      "Epoch 1, Step 1197: Tokens Processed = 4907008\n",
      "Epoch 1, Step 1197: Tokens Processed = 4907008\n",
      "Epoch 1, Step 1198: Tokens Processed = 4911104\n",
      "Epoch 1, Step 1198: Tokens Processed = 4911104\n",
      "Epoch 1, Step 1199: Tokens Processed = 4915200\n",
      "Epoch 1, Step 1199: Tokens Processed = 4915200\n",
      "Epoch 1, Step 1200: Tokens Processed = 4919296\n",
      "Epoch 1, Step 1200: Tokens Processed = 4919296\n",
      "Epoch 1, Step 1200: Training Loss = 1.9370502829551697, Validation Loss = 5.083054065704346, Tokens Processed = 4919296\n",
      "Epoch 1, Step 1200: Training Loss = 1.9370502829551697, Validation Loss = 5.083054065704346, Tokens Processed = 4919296\n",
      "Epoch 1, Step 1201: Tokens Processed = 4923392\n",
      "Epoch 1, Step 1201: Tokens Processed = 4923392\n",
      "Epoch 1, Step 1202: Tokens Processed = 4927488\n",
      "Epoch 1, Step 1202: Tokens Processed = 4927488\n",
      "Epoch 1, Step 1203: Tokens Processed = 4931584\n",
      "Epoch 1, Step 1203: Tokens Processed = 4931584\n",
      "Epoch 1, Step 1204: Tokens Processed = 4935680\n",
      "Epoch 1, Step 1204: Tokens Processed = 4935680\n",
      "Epoch 1, Step 1205: Tokens Processed = 4939776\n",
      "Epoch 1, Step 1205: Tokens Processed = 4939776\n",
      "Epoch 1, Step 1206: Tokens Processed = 4943872\n",
      "Epoch 1, Step 1206: Tokens Processed = 4943872\n",
      "Epoch 1, Step 1207: Tokens Processed = 4947968\n",
      "Epoch 1, Step 1207: Tokens Processed = 4947968\n",
      "Epoch 1, Step 1208: Tokens Processed = 4952064\n",
      "Epoch 1, Step 1208: Tokens Processed = 4952064\n",
      "Epoch 1, Step 1209: Tokens Processed = 4956160\n",
      "Epoch 1, Step 1209: Tokens Processed = 4956160\n",
      "Epoch 1, Step 1210: Tokens Processed = 4960256\n",
      "Epoch 1, Step 1210: Tokens Processed = 4960256\n",
      "Epoch 1, Step 1211: Tokens Processed = 4964352\n",
      "Epoch 1, Step 1211: Tokens Processed = 4964352\n",
      "Epoch 1, Step 1212: Tokens Processed = 4968448\n",
      "Epoch 1, Step 1212: Tokens Processed = 4968448\n",
      "Epoch 1, Step 1213: Tokens Processed = 4972544\n",
      "Epoch 1, Step 1213: Tokens Processed = 4972544\n",
      "Epoch 1, Step 1214: Tokens Processed = 4976640\n",
      "Epoch 1, Step 1214: Tokens Processed = 4976640\n",
      "Epoch 1, Step 1215: Tokens Processed = 4980736\n",
      "Epoch 1, Step 1215: Tokens Processed = 4980736\n",
      "Epoch 1, Step 1216: Tokens Processed = 4984832\n",
      "Epoch 1, Step 1216: Tokens Processed = 4984832\n",
      "Epoch 1, Step 1217: Tokens Processed = 4988928\n",
      "Epoch 1, Step 1217: Tokens Processed = 4988928\n",
      "Epoch 1, Step 1218: Tokens Processed = 4993024\n",
      "Epoch 1, Step 1218: Tokens Processed = 4993024\n",
      "Epoch 1, Step 1219: Tokens Processed = 4997120\n",
      "Epoch 1, Step 1219: Tokens Processed = 4997120\n",
      "Epoch 1, Step 1220: Tokens Processed = 5001216\n",
      "Epoch 1, Step 1220: Tokens Processed = 5001216\n",
      "Epoch 1, Step 1221: Tokens Processed = 5005312\n",
      "Epoch 1, Step 1221: Tokens Processed = 5005312\n",
      "Epoch 1, Step 1222: Tokens Processed = 5009408\n",
      "Epoch 1, Step 1222: Tokens Processed = 5009408\n",
      "Epoch 1, Step 1223: Tokens Processed = 5013504\n",
      "Epoch 1, Step 1223: Tokens Processed = 5013504\n",
      "Epoch 1, Step 1224: Tokens Processed = 5017600\n",
      "Epoch 1, Step 1224: Tokens Processed = 5017600\n",
      "Epoch 1, Step 1225: Tokens Processed = 5021696\n",
      "Epoch 1, Step 1225: Tokens Processed = 5021696\n",
      "Epoch 1, Step 1226: Tokens Processed = 5025792\n",
      "Epoch 1, Step 1226: Tokens Processed = 5025792\n",
      "Epoch 1, Step 1227: Tokens Processed = 5029888\n",
      "Epoch 1, Step 1227: Tokens Processed = 5029888\n",
      "Epoch 1, Step 1228: Tokens Processed = 5033984\n",
      "Epoch 1, Step 1228: Tokens Processed = 5033984\n",
      "Epoch 1, Step 1229: Tokens Processed = 5038080\n",
      "Epoch 1, Step 1229: Tokens Processed = 5038080\n",
      "Epoch 1, Step 1230: Tokens Processed = 5042176\n",
      "Epoch 1, Step 1230: Tokens Processed = 5042176\n",
      "Epoch 1, Step 1231: Tokens Processed = 5046272\n",
      "Epoch 1, Step 1231: Tokens Processed = 5046272\n",
      "Epoch 1, Step 1232: Tokens Processed = 5050368\n",
      "Epoch 1, Step 1232: Tokens Processed = 5050368\n",
      "Epoch 1, Step 1233: Tokens Processed = 5054464\n",
      "Epoch 1, Step 1233: Tokens Processed = 5054464\n",
      "Epoch 1, Step 1234: Tokens Processed = 5058560\n",
      "Epoch 1, Step 1234: Tokens Processed = 5058560\n",
      "Epoch 1, Step 1235: Tokens Processed = 5062656\n",
      "Epoch 1, Step 1235: Tokens Processed = 5062656\n",
      "Epoch 1, Step 1236: Tokens Processed = 5066752\n",
      "Epoch 1, Step 1236: Tokens Processed = 5066752\n",
      "Epoch 1, Step 1237: Tokens Processed = 5070848\n",
      "Epoch 1, Step 1237: Tokens Processed = 5070848\n",
      "Epoch 1, Step 1238: Tokens Processed = 5074944\n",
      "Epoch 1, Step 1238: Tokens Processed = 5074944\n",
      "Epoch 1, Step 1239: Tokens Processed = 5079040\n",
      "Epoch 1, Step 1239: Tokens Processed = 5079040\n",
      "Epoch 1, Step 1240: Tokens Processed = 5083136\n",
      "Epoch 1, Step 1240: Tokens Processed = 5083136\n",
      "Epoch 1, Step 1241: Tokens Processed = 5087232\n",
      "Epoch 1, Step 1241: Tokens Processed = 5087232\n",
      "Epoch 1, Step 1242: Tokens Processed = 5091328\n",
      "Epoch 1, Step 1242: Tokens Processed = 5091328\n",
      "Epoch 1, Step 1243: Tokens Processed = 5095424\n",
      "Epoch 1, Step 1243: Tokens Processed = 5095424\n",
      "Epoch 1, Step 1244: Tokens Processed = 5099520\n",
      "Epoch 1, Step 1244: Tokens Processed = 5099520\n",
      "Epoch 1, Step 1245: Tokens Processed = 5103616\n",
      "Epoch 1, Step 1245: Tokens Processed = 5103616\n",
      "Epoch 1, Step 1246: Tokens Processed = 5107712\n",
      "Epoch 1, Step 1246: Tokens Processed = 5107712\n",
      "Epoch 1, Step 1247: Tokens Processed = 5111808\n",
      "Epoch 1, Step 1247: Tokens Processed = 5111808\n",
      "Epoch 1, Step 1248: Tokens Processed = 5115904\n",
      "Epoch 1, Step 1248: Tokens Processed = 5115904\n",
      "Generated Text: Once upon a time,        and a tax of apprenticeship, a tax of three or four shillings and a-half,       imposed upon the wages of the land tax, which, in the time of time,       is said to be the most unequal. The tythe, or       of the most other taxes, is not, in the highest degree,       of the most\n",
      "Generated Text: Once upon a time,        and a tax of apprenticeship, a tax of three or four shillings and a-half,       imposed upon the wages of the land tax, which, in the time of time,       is said to be the most unequal. The tythe, or       of the most other taxes, is not, in the highest degree,       of the most\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SydsGPT(SYDSGPT_CONFIG_345M)\n",
    "checkpoint = torch.load(\"sydsgpt_345m_trained_model_optimizer.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.0002, weight_decay = 0.05)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 2\n",
    "training_losses, validation_losses, total_tokens_processed = train_model_v1(\n",
    "    model,\n",
    "    training_dataloader,\n",
    "    validation_dataloader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    evaluation_frequency = 100,\n",
    "    evaluation_iterations = 2,\n",
    "    start_context = \"Once upon a time\",\n",
    "    tokenizer = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3516ce72",
   "metadata": {},
   "source": [
    "# Manual generation cell (direct greedy sampling after training)\n",
    "\n",
    "This cell performs a one‑off text generation directly with the trained model—outside the wrapped helper—to give a longer qualitative sample (200 new tokens) from the prompt \"once upon a time\".\n",
    "\n",
    "---\n",
    "## What the code does (step by step)\n",
    "1. `model.eval()` switches the model to evaluation mode:\n",
    "   - Disables dropout.\n",
    "   - Ensures deterministic output given fixed weights and input.\n",
    "2. `generate_simple(...)` is called with:\n",
    "   - The current `model`.\n",
    "   - A tokenized prompt (conversion handled by `text_to_tokens`).\n",
    "   - `max_new_tokens=200` for a longer continuation.\n",
    "   - The configured `context_length` (prevents exceeding the model's positional window).\n",
    "3. The returned tensor of token IDs is decoded back to human‑readable text via `tokens_to_text`.\n",
    "4. The final string is printed for inspection.\n",
    "\n",
    "---\n",
    "## Purpose\n",
    "- Obtain a longer, standalone sample than the shorter (100-token) helper output.\n",
    "- Compare stylistic/coherence evolution after training or fine‑tuning phases.\n",
    "- Serve as a quick qualitative regression test if later modifications are made to generation logic.\n",
    "\n",
    "---\n",
    "## Inputs and outputs\n",
    "| Item | Description |\n",
    "|------|-------------|\n",
    "| Prompt | \"once upon a time\" (lowercase variant) |\n",
    "| `max_new_tokens` | 200 (total output length ≈ prompt length + 200, capped by `context_length`) |\n",
    "| Output tokens | 1D sequence of token IDs after greedy extension |\n",
    "| Printed text | Decoded English continuation for human evaluation |\n",
    "\n",
    "---\n",
    "## Greedy decoding characteristics\n",
    "- Always selects argmax token; produces deterministic, lower‑entropy completions.\n",
    "- May become repetitive over very long stretches (lack of sampling diversity). For more natural variation consider top‑k, nucleus (top‑p), or temperature sampling.\n",
    "\n",
    "---\n",
    "## Adjustments you can make\n",
    "| Change | Effect |\n",
    "|--------|--------|\n",
    "| Increase `max_new_tokens` | Longer stories / more context, higher runtime, more memory usage |\n",
    "| Shorter `max_new_tokens` (e.g. 64) | Faster iteration / quick spot checks |\n",
    "| Different prompt casing / content | Alters stylistic bias of continuation |\n",
    "\n",
    "---\n",
    "## After running\n",
    "You can proceed to additional evaluation, start continuation training, or experiment with alternative decoding strategies for more creative outputs.\n",
    "\n",
    "---\n",
    "## Summary\n",
    "A direct, transparent greedy generation producing a longer sample from a fixed prompt—ideal for quick qualitative assessment of model fluency and coherence after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9598b4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Text: once upon a time before she\n",
      "was so busy, that I felt quite sure that I felt quite sure that I was\n",
      "not quite sure that I felt it was.\n",
      "\n",
      "“You are a child,” said I, “that you are a beautiful woman, and\n",
      "you are a beautiful woman.”\n",
      "\n",
      "“Yes,” said Ada, “that there is nothing in it.”\n",
      "\n",
      "“That is,” said my guardian, “that there is nothing else that\n",
      "makes it so.”\n",
      "\n",
      "“That is not,” said my guardian, “that there is such a time as\n",
      "you are.”\n",
      "\n",
      "“You are not to be removed,” said my guardian, “that there is no\n",
      "considerable answer.”\n",
      "\n",
      "“You are not to be always happy,” said my guardian, “that there is\n",
      "something of the kind\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "output_tokens = generate_simple(model, text_to_tokens(\"once upon a time\", tokenizer).to(device), 200, SYDSGPT_CONFIG_345M['context_length'])\n",
    "output_text = tokens_to_text(output_tokens, tokenizer)\n",
    "print(f\"Output Text: {output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f5208",
   "metadata": {},
   "source": [
    "# Token sampling demo: greedy vs. multinomial and empirical frequency\n",
    "\n",
    "This cell illustrates fundamental next‑token selection strategies on a tiny handcrafted vocabulary. It shows how raw logits become probabilities, how greedy decoding picks the argmax, how stochastic sampling (`torch.multinomial`) introduces diversity, and how repeated sampling approximates the underlying probability distribution.\n",
    "\n",
    "---\n",
    "## What the code builds\n",
    "1. `example_vocab`: A small mapping from token strings to integer IDs (10 tokens). Acts like a miniature language model vocabulary.\n",
    "2. `inverse_example_vocab`: Reverse lookup so we can convert sampled IDs back to readable tokens.\n",
    "3. `example_next_token_logits`: A 1‑D tensor of unnormalized scores (logits) for each vocabulary entry.\n",
    "4. `example_next_token_probs = torch.softmax(logits, dim=0)`: Converts logits to a valid probability distribution (non‑negative, sums to 1).\n",
    "5. `example_greedy_next_token = torch.argmax(...)`: Deterministic choice of the highest‑probability token.\n",
    "6. `torch.multinomial(example_next_token_probs, num_samples=1)`: Draws one token according to the categorical distribution—probabilistic sampling.\n",
    "7. `get_sampled_tokens(...)`: Repeats multinomial sampling 1000 times to empirically estimate how often each token is chosen; prints frequencies.\n",
    "\n",
    "---\n",
    "## Why this matters\n",
    "Language models typically output logits over a large vocabulary each step. Generation quality depends heavily on how you transform these logits into a selected next token:\n",
    "- Greedy decoding maximizes immediate probability but can produce repetitive or bland sequences.\n",
    "- Stochastic sampling (optionally with temperature/top‑k/top‑p) encourages diversity and can yield more creative or natural continuations.\n",
    "- Empirical sampling frequency (many draws) converges toward the theoretical probability distribution—helpful for intuition.\n",
    "\n",
    "---\n",
    "## Key concepts demonstrated\n",
    "| Concept | Shown By | Notes |\n",
    "|---------|----------|-------|\n",
    "| Logits vs. probabilities | `softmax` call | Softmax rescales differences; large positive logits dominate. |\n",
    "| Greedy selection | `torch.argmax` | Always same token given identical logits. |\n",
    "| Random sampling | `torch.multinomial` | Draw proportional to probability mass; needs a proper distribution (no negative values, sums to 1). |\n",
    "| Empirical distribution | 1000 repeats + `bincount` | Frequencies approximate probabilities; more samples → tighter convergence (Law of Large Numbers). |\n",
    "\n",
    "---\n",
    "## Interpreting results\n",
    "- The printed “Greedy Next Token” is the single most likely token (highest probability after softmax). In this example it should correspond to the largest logit (here `before` with logit 3.63).\n",
    "- “Random Next Token” might or might not match the greedy token—depends on one draw from the distribution.\n",
    "- Frequency table: Tokens with higher true probabilities appear more often; rare tokens may still show up occasionally, demonstrating stochastic exploration.\n",
    "\n",
    "---\n",
    "## Quick experiment ideas\n",
    "Try changing one logit dramatically (e.g., raise `after` from -5.38 to 2.5) and re-run: watch probability mass shift and empirical frequencies respond proportionally.\n",
    "\n",
    "---\n",
    "## Summary\n",
    "This micro‑example demystifies next‑token selection: logits → probabilities → deterministic (greedy) vs. stochastic (multinomial) choice, with repeated sampling revealing the underlying distribution. It’s a conceptual foundation for more advanced decoding strategies (top‑k, top‑p, temperature) used in full language model generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60325e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Next Token: before\n",
      "Random Next Token: once\n",
      "Token: once: 68 times\n",
      "Token: upon: 103 times\n",
      "Token: a: 83 times\n",
      "Token: time: 16 times\n",
      "Token: before: 723 times\n",
      "Token: she: 4 times\n",
      "Token: lived: 3 times\n"
     ]
    }
   ],
   "source": [
    "example_vocab = {\n",
    "    \"once\" : 0,\n",
    "    \"upon\" : 1,\n",
    "    \"a\" : 2,\n",
    "    \"time\" : 3,\n",
    "    \"before\" : 4,\n",
    "    \"she\" : 5,\n",
    "    \"lived\" : 6,\n",
    "    \"happily\" : 7,\n",
    "    \"ever\" : 8,\n",
    "    \"after\" : 9\n",
    "}\n",
    "inverse_example_vocab = {v: k for k, v in example_vocab.items()}\n",
    "\n",
    "example_next_token_logits = torch.tensor([1.35, 1.86, 1.53, 0.17, 3.63, -1.82, -2.17, -3.90, -4.85, -5.38])\n",
    "example_next_token_probs = torch.softmax(example_next_token_logits, dim = 0)\n",
    "example_greedy_next_token = torch.argmax(example_next_token_probs).item()\n",
    "print(f\"Greedy Next Token: {inverse_example_vocab[example_greedy_next_token]}\")\n",
    "\n",
    "torch.manual_seed(246)\n",
    "example_random_next_token = torch.multinomial(example_next_token_probs, num_samples = 1).item()\n",
    "print(f\"Random Next Token: {inverse_example_vocab[example_random_next_token]}\")\n",
    "\n",
    "def get_sampled_tokens(probs):\n",
    "    sampled_token = [torch.multinomial(probs, num_samples = 1).item() for i in range(1000)]\n",
    "    sampled_tokens = torch.bincount(torch.tensor(sampled_token))\n",
    "    for i, frequency in enumerate(sampled_tokens):\n",
    "        print(f\"Token: {inverse_example_vocab[i]}: {frequency.item()} times\")\n",
    "\n",
    "get_sampled_tokens(example_next_token_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fd9750",
   "metadata": {},
   "source": [
    "# Temperature scaling demo: controlling randomness in sampling\n",
    "\n",
    "This cell explores how the softmax temperature T reshapes a probability distribution and affects stochastic next‑token sampling. Using the same logits as the previous mini‑vocabulary example, it prints empirical frequencies at multiple temperatures to show how generation becomes more/less diverse.\n",
    "\n",
    "---\n",
    "## What the code does\n",
    "1. Defines `softmax_with_temperature(logits, temperature)`:\n",
    "   - Scales logits by 1/T and applies softmax.\n",
    "   - For a vector of logits z, probabilities are:  \n",
    "     p_i(T) = exp(z_i / T) / Σ_j exp(z_j / T)\n",
    "2. Iterates over `temperatures = [0.1, 0.5, 1.0, 2.0]`:\n",
    "   - Computes `temperature_scaled_probs` for each T.\n",
    "   - Calls `get_sampled_tokens(...)` (from the previous cell) to draw 1000 samples and print a frequency table for each T.\n",
    "\n",
    "---\n",
    "## How temperature changes behavior\n",
    "- T < 1.0 (e.g., 0.5, 0.1): sharpens the distribution\n",
    "  - Increases contrast between high and low‑probability tokens.\n",
    "  - Sampling becomes more deterministic; top tokens dominate frequency counts.\n",
    "- T = 1.0: baseline distribution (no scaling)\n",
    "  - Frequencies reflect the original softmax over `example_next_token_logits`.\n",
    "- T > 1.0 (e.g., 2.0): flattens the distribution\n",
    "  - Reduces differences between tokens.\n",
    "  - Increases diversity; lower‑probability tokens appear more often.\n",
    "\n",
    "---\n",
    "## Interpreting the output\n",
    "- For each T, you’ll see counts for each token over 1000 draws.\n",
    "- As T decreases, the most likely token (greedy token) should dominate the histogram.\n",
    "- As T increases, frequencies spread toward a more uniform distribution.\n",
    "- Because sampling is stochastic, exact numbers vary between runs; larger sample sizes (e.g., 10k) reduce variance and better reveal trends.\n",
    "\n",
    "---\n",
    "## Practical guidance for generation\n",
    "- Start with T in the 0.7–1.2 range; adjust based on desired creativity vs. factuality.\n",
    "- Keep T consistent across steps within a single generation unless experimenting with annealing strategies.\n",
    "\n",
    "---\n",
    "## Summary\n",
    "Temperature scaling is a simple, powerful knob for controlling randomness in language model sampling: low T → precise and repetitive; high T → diverse and creative. Use it alongside top‑k/top‑p for fine‑grained control over generation quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab60656c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Temperature: 0.1\n",
      "Token: once: 0 times\n",
      "Token: upon: 0 times\n",
      "Token: a: 0 times\n",
      "Token: time: 0 times\n",
      "Token: before: 1000 times\n",
      "\n",
      " Temperature: 0.5\n",
      "Token: once: 15 times\n",
      "Token: upon: 34 times\n",
      "Token: a: 18 times\n",
      "Token: time: 3 times\n",
      "Token: before: 930 times\n",
      "\n",
      " Temperature: 1.0\n",
      "Token: once: 75 times\n",
      "Token: upon: 123 times\n",
      "Token: a: 82 times\n",
      "Token: time: 24 times\n",
      "Token: before: 690 times\n",
      "Token: she: 4 times\n",
      "Token: lived: 1 times\n",
      "Token: happily: 1 times\n",
      "\n",
      " Temperature: 2.0\n",
      "Token: once: 113 times\n",
      "Token: upon: 159 times\n",
      "Token: a: 131 times\n",
      "Token: time: 78 times\n",
      "Token: before: 447 times\n",
      "Token: she: 25 times\n",
      "Token: lived: 22 times\n",
      "Token: happily: 13 times\n",
      "Token: ever: 8 times\n",
      "Token: after: 4 times\n"
     ]
    }
   ],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    probs = torch.softmax(scaled_logits, dim = 0)\n",
    "    return probs\n",
    "\n",
    "temperatures = [0.1, 0.5, 1.0, 2.0]\n",
    "for temp in temperatures:\n",
    "    temperature_scaled_probs = softmax_with_temperature(example_next_token_logits, temp)\n",
    "    print(f\"\\n Temperature: {temp}\")\n",
    "    get_sampled_tokens(temperature_scaled_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cc7f7b",
   "metadata": {},
   "source": [
    "# Top-k filtering demo: restricting the candidate set before sampling\n",
    "\n",
    "This cell demonstrates a classic decoding refinement: **top-k sampling**. Instead of sampling from the full vocabulary distribution, we keep only the k highest‑logit tokens, mask the rest to negative infinity (so their post‑softmax probability becomes zero), then sample within that reduced set. This balances diversity and relevance.\n",
    "\n",
    "---\n",
    "## What the code does (step by step)\n",
    "1. `top_k = 4`: Choose how many highest‑scoring tokens to retain.\n",
    "2. `torch.topk(example_next_token_logits, top_k)` returns:\n",
    "   - `top_k_logits`: The 4 largest logits in descending order.\n",
    "   - `top_k_indices`: Their original token indices.\n",
    "3. `torch.where(example_next_token_logits < top_k_logits[-1], -inf, example_next_token_logits)`:\n",
    "   - Finds the cutoff logit (the smallest logit among the top-k set: `top_k_logits[-1]`).\n",
    "   - Replaces any logit below that cutoff with `-inf`, effectively zeroing its probability after softmax.\n",
    "4. `top_k_probs = torch.softmax(new_logits, dim=0)`: Computes normalized probabilities over only the surviving top-k logits (others become exactly 0 probability).\n",
    "5. `get_sampled_tokens(top_k_probs)`: Samples repeatedly (1000 draws) and prints frequencies among the retained tokens.\n",
    "\n",
    "---\n",
    "## Why top-k filtering\n",
    "| Motivation | Benefit |\n",
    "|-----------|---------|\n",
    "| Remove low-probability tail | Reduces chance of bizarre / out-of-context tokens |\n",
    "| Retain diversity among strong candidates | Allows exploration beyond pure greedy argmax |\n",
    "| Computational simplicity | Easy to implement; single `topk` + masking step |\n",
    "\n",
    "Compared to pure greedy decoding, top-k can produce more varied yet still on-topic continuations. Compared to temperature-only scaling, it imposes a hard boundary on candidate tokens, preventing very low-probability choices even at higher temperatures.\n",
    "\n",
    "---\n",
    "## Alternative masking approaches\n",
    "- **Direct index filtering**: Gather only top-k indices and sample from that subset array; equivalent probability outcome.\n",
    "- **Top-p (nucleus) sampling**: Instead of a fixed k, pick the smallest set of tokens whose cumulative probability ≥ p (e.g., 0.9). Adapts dynamically to distribution shape.\n",
    "- **Temperature + top-k**: Sharpen or flatten within the retained set for fine-grained control.\n",
    "\n",
    "---\n",
    "## Numerical details\n",
    "- Using `-inf` (negative infinity) ensures `exp(-inf) = 0` in softmax, producing exact zeros without manual renormalization.\n",
    "- If implementing with large tensors, broadcasting and in-place operations can reduce memory pressure.\n",
    "- Always compute softmax after masking; masking post-softmax requires renormalization manually.\n",
    "\n",
    "---\n",
    "## Practical tips\n",
    "- Typical values: k=20…50 for medium vocabularies (LLM generation often uses k≈40).\n",
    "- Smaller k increases focus but risks repetition.\n",
    "\n",
    "---\n",
    "## Summary\n",
    "Top-k filtering discards the tail of the probability distribution, limiting sampling to the k most promising tokens. It offers a simple, deterministic way to balance diversity and coherence, and serves as a building block for more advanced decoding strategies used in modern language model generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b697235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-4 Indices: tensor([4, 1, 2, 0])\n",
      "Top-4 Logits: tensor([3.6300, 1.8600, 1.5300, 1.3500])\n",
      "New Logits after Top-4 filtering: tensor([1.3500, 1.8600, 1.5300,   -inf, 3.6300,   -inf,   -inf,   -inf,   -inf,\n",
      "          -inf])\n",
      "Top-4 Probabilities: tensor([0.0733, 0.1221, 0.0878, 0.0000, 0.7168, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000])\n",
      "Token: once: 62 times\n",
      "Token: upon: 120 times\n",
      "Token: a: 92 times\n",
      "Token: time: 0 times\n",
      "Token: before: 726 times\n"
     ]
    }
   ],
   "source": [
    "top_k = 4\n",
    "top_k_logits, top_k_indices = torch.topk(example_next_token_logits, top_k)\n",
    "print(f\"Top-{top_k} Indices: {top_k_indices}\")\n",
    "print(f\"Top-{top_k} Logits: {top_k_logits}\")\n",
    "\n",
    "new_logits = torch.where(\n",
    "    condition = example_next_token_logits < top_k_logits[-1],\n",
    "    input = torch.tensor(float('-inf')),\n",
    "    other = example_next_token_logits\n",
    ")\n",
    "\n",
    "print(f\"New Logits after Top-{top_k} filtering: {new_logits}\")\n",
    "\n",
    "top_k_probs = torch.softmax(new_logits, dim = 0)\n",
    "print(f\"Top-{top_k} Probabilities: {top_k_probs}\")\n",
    "get_sampled_tokens(top_k_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95b1b4a",
   "metadata": {},
   "source": [
    "# Advanced `generate` function: temperature, top‑k, and EOS handling\n",
    "\n",
    "This cell defines a more flexible text generation routine that supports temperature scaling, optional top‑k filtering, greedy fall‑back, and early stopping on an end‑of‑sequence (EOS) token. It also enforces the model’s `context_size` window by truncating the input to the most recent tokens at each step.\n",
    "\n",
    "---\n",
    "## Function signature\n",
    "```python\n",
    "def generate(model, input_tokens, max_new_tokens, context_size,\n",
    "            temperature=1.0, top_k=None, eos_id=None):\n",
    "    ...\n",
    "```\n",
    "\n",
    "### Parameters\n",
    "- `model` (`nn.Module`): Autoregressive language model producing logits over the vocab.\n",
    "- `input_tokens` (`LongTensor`, shape `(1, T0)`): Seed prompt; function appends new tokens to this tensor in place.\n",
    "- `max_new_tokens` (`int`): Maximum number of tokens to generate.\n",
    "- `context_size` (`int`): Maximum sequence length the model attends to; older tokens are truncated beyond this window.\n",
    "- `temperature` (`float`, default `1.0`):\n",
    "  - `> 0`: Scale logits by `1/temperature` and sample via multinomial.\n",
    "  - `== 0`: Use greedy argmax (deterministic).\n",
    "- `top_k` (`int | None`): If set, keep only the top‑k logits per step (mask others to `-inf`), then apply softmax.\n",
    "- `eos_id` (`int | None`): If set, stop generation early when this token is selected.\n",
    "\n",
    "### Returns\n",
    "- `LongTensor` of shape `(1, T0 + N)` where `0 ≤ N ≤ max_new_tokens` (may stop early on `eos_id`).\n",
    "\n",
    "---\n",
    "## Step‑by‑step flow\n",
    "1. Loop `max_new_tokens` times.\n",
    "2. Slice the active context: `input_context = input_tokens[:, -context_size:]` to obey the model’s positional limit.\n",
    "3. Forward pass with `torch.no_grad()`; extract last‑step logits: `logits = logits[:, -1, :]`.\n",
    "4. Optional top‑k filtering:\n",
    "   - `top_k_logits, _ = torch.topk(logits, top_k)` → get per‑batch cutoffs.\n",
    "   - `min_top_k_logit = top_k_logits[:, -1]` → the k‑th largest logit per example.\n",
    "   - Mask all logits below the cutoff to `-inf` using `torch.where` (on the same device/dtype as `logits`).\n",
    "5. Temperature / decoding mode:\n",
    "   - If `temperature > 0`:\n",
    "     - Scale logits by `1/temperature` and compute `probs = softmax(logits, dim=-1)`.\n",
    "     - Sample `next_token = torch.multinomial(probs, num_samples=1)`.\n",
    "   - Else (temperature == 0):\n",
    "     - Greedy: `next_token = torch.argmax(logits, dim=-1, keepdim=True)`.\n",
    "6. Early stop: If `eos_id` is provided and the sampled token equals `eos_id`, `break`.\n",
    "7. Append: `input_tokens = torch.cat((input_tokens, next_token), dim=1)`.\n",
    "8. Return the extended token sequence.\n",
    "\n",
    "---\n",
    "## Design choices and rationale\n",
    "- Context truncation ensures compute and memory scale with `context_size`, not total generated length.\n",
    "- Top‑k pruning removes the low‑probability tail for safer sampling, especially at higher temperatures.\n",
    "- Temperature provides a single, intuitive knob for diversity: lower → more deterministic; higher → more varied.\n",
    "- Greedy fallback via `temperature == 0` keeps the API simple without a separate mode switch.\n",
    "- `eos_id` allows clean termination when the model emits a special end token.\n",
    "\n",
    "---\n",
    "## Usage examples\n",
    "- Pure greedy (deterministic): `generate(model, x, 128, context_size, temperature=0.0)`\n",
    "- Temperature‑only sampling: `generate(model, x, 128, context_size, temperature=0.8)`\n",
    "- Top‑k sampling: `generate(model, x, 128, context_size, temperature=0.8, top_k=40)`\n",
    "- Early stop on EOS: `generate(model, x, 256, context_size, temperature=0.7, eos_id=tokenizer.eot_token)`\n",
    "\n",
    "---\n",
    "## Summary\n",
    "This `generate` function brings together practical decoding controls—context management, temperature, top‑k, and EOS—into a compact loop suitable for qualitative sampling and quick experiments. It’s a solid baseline to plug into training checkpoints and prompt‑driven evaluations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c75221c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, input_tokens, max_new_tokens, context_size, temperature = 1.0, top_k = None, eos_id = None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        input_context = input_tokens[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_context)\n",
    "            logits = logits[:, -1, :]\n",
    "            if top_k is not None:\n",
    "                top_k_logits, _ = torch.topk(logits, top_k)\n",
    "                min_top_k_logit = top_k_logits[:, -1]\n",
    "                logits = torch.where(logits < min_top_k_logit, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "            if temperature > 0.0:\n",
    "                logits = logits / temperature\n",
    "                probs = torch.softmax(logits, dim = -1)\n",
    "                next_token = torch.multinomial(probs, num_samples = 1)\n",
    "            else:\n",
    "                next_token = torch.argmax(logits, dim = -1, keepdim = True)\n",
    "            if next_token == eos_id:\n",
    "                break\n",
    "            input_tokens = torch.cat((input_tokens, next_token), dim = 1)\n",
    "    return input_tokens\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3843f991",
   "metadata": {},
   "source": [
    "# Example: temperature + top‑k generation (invocation)\n",
    "\n",
    "This cell runs the advanced `generate` function on a natural prompt using both temperature scaling and top‑k filtering to produce a diverse yet controlled continuation.\n",
    "\n",
    "---\n",
    "## What the code does\n",
    "1. Sets a fixed RNG seed (`torch.manual_seed(246)`) to make sampling reproducible.\n",
    "2. Defines a human‑readable prompt: `\"Once upon a time there was a\"`.\n",
    "3. Encodes it to token IDs and moves them to `device`.\n",
    "4. Calls `generate(...)` with:\n",
    "   - `max_new_tokens=200`: up to 200 tokens of continuation.\n",
    "   - `context_size=SYDSGPT_CONFIG_345M['context_length']`: enforces model’s max window.\n",
    "   - `temperature=1.5`: flattens probabilities to encourage variety.\n",
    "   - `top_k=30`: restricts sampling to the 30 most likely tokens each step.\n",
    "5. Decodes tokens back to text and prints the result.\n",
    "\n",
    "---\n",
    "## Why combine temperature and top‑k\n",
    "- Temperature > 1.0 increases diversity, avoiding overly‑confident loops.\n",
    "- Top‑k caps the candidate set, preventing extremely unlikely tokens from appearing even when temperature is high.\n",
    "- Together they provide a practical balance: varied but not nonsensical.\n",
    "\n",
    "---\n",
    "## Tuning tips\n",
    "- If output feels chaotic: lower `temperature` (e.g., 0.8–1.0) or reduce `top_k`.\n",
    "- If output is dull/repetitive: raise `temperature` (1.2–1.8) or increase `top_k` (e.g., 50).\n",
    "- Keep the prompt specific to steer the model; generic prompts amplify variance.\n",
    "\n",
    "---\n",
    "## After running\n",
    "Skim the output for coherence, repetition, and topic adherence. Adjust `temperature` and `top_k` to your preferences, then reuse this pattern for different prompts or integrate it into a qualitative evaluation loop across checkpoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "447b15ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Text:\n",
      " once upon a time after his arrival. But again\n",
      "had that time too already settled in the possibility of talking in\n",
      "the existing histories given personal opportunity of reproachfulness? Hath it not\n",
      "not been brought together simply that one who had not always tried? And the\n",
      "most wonderful man, in a sort of unbension with which he had\n",
      "been capable of using the money by a man who had done so intimatelyision\n",
      "and must not think about as a politician be better in his physical\n",
      "conversation, for whose knowledge there must give a reference to\n",
      "the facts (a lady, especially on purpose, placed\n",
      "upright in their hands) of the unhappy man. The victim might receive\n",
      "her reason to be as much as a hypocrite as possible, but of having\n",
      "supposed she to do as, as it came upon him, as a mode of their being a woman.\n",
      "\n",
      "The latter part of his respect took place to him as much as much as possible\n",
      "to give it him\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(246)\n",
    "input_text = \"once upon a time\"\n",
    "input_tokens = text_to_tokens(input_text, tokenizer).to(device)\n",
    "output_tokens = generate(model, input_tokens, 200, SYDSGPT_CONFIG_345M['context_length'], temperature = 1.5, top_k = 40)\n",
    "output_text = tokens_to_text(output_tokens, tokenizer)\n",
    "print(f\"Output Text:\\n {output_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
